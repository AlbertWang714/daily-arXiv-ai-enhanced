<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [IG-Pruning: Input-Guided Block Pruning for Large Language Models](https://arxiv.org/abs/2511.02213)
*Kangyu Qiao,Shaolei Zhang,Yang Feng*

Main category: cs.CL

TL;DR: IG-Pruning是一种新型的输入感知块剪枝方法，通过动态选择层掩码优化大型语言模型的计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型计算需求的增长，高效推理成为实际部署的关键，现有固定块掩码方法在不同任务和输入中表现不佳。

Method: 提出两阶段方法：1）通过语义聚类和L0优化发现多种掩码候选；2）实现无需大量训练的高效动态剪枝。

Result: 实验结果表明，IG-Pruning在性能上优于现有静态深度剪枝方法。

Conclusion: IG-Pruning尤其适用于资源受限的部署场景。

Abstract: With the growing computational demands of large language models (LLMs),
efficient inference has become increasingly critical for practical deployment.
Depth pruning has emerged as a promising approach for reducing the
computational costs of large language models by removing transformer layers.
However, existing methods typically rely on fixed block masks, which can lead
to suboptimal performance across different tasks and inputs. In this paper, we
propose IG-Pruning, a novel input-aware block-wise pruning method that
dynamically selects layer masks at inference time. Our approach consists of two
stages: (1) Discovering diverse mask candidates through semantic clustering and
L0 optimization, and (2) Implementing efficient dynamic pruning without the
need for extensive training. Experimental results demonstrate that our method
consistently outperforms state-of-the-art static depth pruning methods, making
it particularly suitable for resource-constrained deployment scenarios.

</details>


### [2] [Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results](https://arxiv.org/abs/2511.02246)
*Jonathan Liu,Haoling Qiu,Jonathan Lasko,Damianos Karakos,Mahsa Yarmohammadi,Mark Dredze*

Main category: cs.CL

TL;DR: 摘要讨论了医学聊天机器人在涉及非医疗因素（如人口统计信息）时的表现问题，并提出了自动生成查询和多LLM评估的方法。研究发现LLM评估者间一致性低，建议使用多个LLM评估以保证结果的可推广性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了理解医学聊天机器人在非医疗因素（如人口统计信息）影响下的表现问题，以确保其在医疗环境中提供一致的建议。

Method: 研究方法包括：1）自动生成查询以探测LLM；2）使用多LLM评估设置和提示来评估答案，包括幻觉和遗漏检测。

Result: 研究发现LLM标注者间一致性较低（Cohen's Kappa κ=0.118），且仅特定LLM配对在写作风格、性别和种族上表现出显著差异。

Conclusion: 结论建议使用多个LLM评估以避免非可推广的统计显著性结果，并建议公开LLM间一致性指标以提高透明度。

Abstract: Recent research has shown that hallucinations, omissions, and biases are
prevalent in everyday use-cases of LLMs. However, chatbots used in medical
contexts must provide consistent advice in situations where non-medical factors
are involved, such as when demographic information is present. In order to
understand the conditions under which medical chatbots fail to perform as
expected, we develop an infrastructure that 1) automatically generates queries
to probe LLMs and 2) evaluates answers to these queries using multiple
LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples
the space of patient demographics, histories, disorders, and writing styles to
create realistic questions that we subsequently use to prompt LLMs. In 2), our
evaluation pipeline provides hallucination and omission detection using
LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge
treatment category detectors. As a baseline study, we perform two case studies
on inter-LLM agreement and the impact of varying the answering and evaluation
LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's
Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs
yield statistically significant differences across writing styles, genders, and
races. We recommend that studies using LLM evaluation use multiple LLMs as
evaluators in order to avoid arriving at statistically significant but
non-generalizable results, particularly in the absence of ground-truth data. We
also suggest publishing inter-LLM agreement metrics for transparency. Our code
and dataset are available here:
https://github.com/BBN-E/medic-neurips-2025-demo.

</details>


### [3] [LTD-Bench: Evaluating Large Language Models by Letting Them Draw](https://arxiv.org/abs/2511.02347)
*Liuhao Lin,Ke Li,Zihan Xu,Yuchen Shi,Yulei Qin,Yan Zhang,Xing Sun,Rongrong Ji*

Main category: cs.CL

TL;DR: LTD-Bench是一个创新的基准测试，通过可视化输出和可执行代码评估大语言模型的空间推理能力，揭示其在实际应用中的局限性。


<details>
  <summary>Details</summary>
Motivation: 目前对大语言模型的评估依赖于不透明的数值指标，掩盖了其在空间推理方面的局限性。LTD-Bench旨在通过可视化方法填补这一空白。

Method: LTD-Bench采用互补的生成任务（测试空间想象力）和识别任务（评估空间感知），覆盖三个难度级别，系统评估语言与空间的映射能力。

Result: 实验表明，即使在一些传统基准测试中表现优异的模型，在语言和空间概念的双向映射上也存在严重缺陷。

Conclusion: LTD-Bench不仅揭示了模型的局限性，还提供了诊断分析的新方法，有助于进一步研究模型的相似性和改进方向。

Abstract: Current evaluation paradigms for large language models (LLMs) represent a
critical blind spot in AI research--relying on opaque numerical metrics that
conceal fundamental limitations in spatial reasoning while providing no
intuitive understanding of model capabilities. This deficiency creates a
dangerous disconnect between reported performance and practical abilities,
particularly for applications requiring physical world understanding. We
introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation
from abstract scores to directly observable visual outputs by requiring models
to generate drawings through dot matrices or executable code. This approach
makes spatial reasoning limitations immediately apparent even to non-experts,
bridging the fundamental gap between statistical performance and intuitive
assessment. LTD-Bench implements a comprehensive methodology with complementary
generation tasks (testing spatial imagination) and recognition tasks (assessing
spatial perception) across three progressively challenging difficulty levels,
methodically evaluating both directions of the critical language-spatial
mapping. Our extensive experiments with state-of-the-art models expose an
alarming capability gap: even LLMs achieving impressive results on traditional
benchmarks demonstrate profound deficiencies in establishing bidirectional
mappings between language and spatial concept--a fundamental limitation that
undermines their potential as genuine world models. Furthermore, LTD-Bench's
visual outputs enable powerful diagnostic analysis, offering a potential
approach to investigate model similarity.

</details>


### [4] [Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation](https://arxiv.org/abs/2511.02358)
*Wongyu Kim,Hochang Lee,Sanghak Lee,Yoonsung Kim,Jaehyun Park*

Main category: cs.CL

TL;DR: 提出了M-Solomon方法，通过自适应判断何时增强查询，减少嵌入延迟并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的嵌入方法对所有查询进行增强，导致高延迟且对某些查询性能不利，且未探索多模态环境。

Method: M-Solomon通过将查询分组、合成增强内容及自适应增强步骤，仅在必要时增强查询。

Result: 实验表明，M-Solomon性能显著优于无增强和全增强基线，同时大幅降低延迟。

Conclusion: M-Solomon通过自适应查询增强在多模态环境下实现了高效且性能优越的嵌入。

Abstract: Query augmentation makes queries more meaningful by appending further
information to the queries to find relevant documents. Current studies have
proposed Large Language Model (LLM)-based embedders, which learn representation
for embedding and generation for query augmentation in a multi-task manner by
leveraging the generative capabilities of LLM. During inference, these jointly
trained embedders have conducted query augmentation followed by embedding,
showing effective results. However, augmenting every query leads to substantial
embedding latency and query augmentation can be detrimental to performance for
some queries. Also, previous methods have not been explored in multimodal
environments. To tackle these problems, we propose M-Solomon, a universal
multimodal embedder that can adaptively determine when to augment queries. Our
approach first divides the queries of the training datasets into two groups at
the dataset level. One includes queries that require augmentation and the other
includes queries that do not. Then, we introduces a synthesis process that
generates appropriate augmentations for queries that require them by leveraging
a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.
Through this step, M-Solomon can conduct query augmentation only when necessary
by learning to generate synthetic augmentations with the prefix /augment for
queries that demand them and to generate the simple string /embed for others.
Experimental results showed that M-Solomon not only surpassed the baseline
without augmentation by a large margin but also outperformed the baseline that
always used augmentation, providing much faster embedding latency.

</details>


### [5] [LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context](https://arxiv.org/abs/2511.02366)
*Yudong Li,Zhongliang Yang,Kejiang Chen,Wenxuan Wang,Tianxin Zhang,Sifang Wan,Kecheng Wang,Haitian Li,Xu Wang,Lefan Cheng,Youdan Yang,Baocheng Chen,Ziyu Liu,Yufei Sun,Liyan Wu,Wenya Wen,Xingchi Gu,Peiru Yang*

Main category: cs.CL

TL;DR: LiveSecBench是一个针对中文LLM应用场景的动态安全基准测试，涵盖合法性、伦理等六个维度，并计划更新以包括更多威胁向量。


<details>
  <summary>Details</summary>
Motivation: 旨在为中文语言的LLM应用提供动态更新的安全评估标准。

Method: 通过六个关键维度评估模型，并动态更新威胁向量。

Result: 已评估18个LLM，展示了中文AI安全现状。

Conclusion: LiveSecBench为中语言LLM的安全提供了一个实用且动态的评估工具。

Abstract: In this work, we propose LiveSecBench, a dynamic and continuously updated
safety benchmark specifically for Chinese-language LLM application scenarios.
LiveSecBench evaluates models across six critical dimensions (Legality, Ethics,
Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in
the Chinese legal and social frameworks. This benchmark maintains relevance
through a dynamic update schedule that incorporates new threat vectors, such as
the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in
the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,
providing a landscape of AI safety in the context of Chinese language. The
leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.

</details>


### [6] [AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda](https://arxiv.org/abs/2511.02374)
*Mohd Nauman,Sravan Gvm,Vijay Devane,Shyam Pawar,Viraj Thakur,Kundeshwar Pundalik,Piyush Sawarkar,Rohit Saluja,Maunendra Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: AyurParam-2.9B是一个专注于阿育吠陀领域的双语语言模型，通过精细调校和高质量数据集，超越了同规模开源模型，甚至能与更大规模的模型竞争。


<details>
  <summary>Details</summary>
Motivation: 现有通用大语言模型在需要深厚专业知识的领域表现不佳，尤其是传统医学系统如阿育吠陀。需要开发一个专业化的模型来准确解释和应用这些知识。

Method: 基于Param-1-2.9B模型，使用精心策划的阿育吠陀数据集进行微调，数据集包含英文和印地语的上下文感知、推理和问答内容，并经过严格标注。

Result: AyurParam在BhashaBench-Ayur基准测试中超越了同规模开源模型，并显示出与更大规模模型竞争或更优的性能。

Conclusion: 研究强调了在专业领域提供可靠、文化契合的AI时，领域适应和高质量监督的必要性。

Abstract: Current large language models excel at broad, general-purpose tasks, but
consistently underperform when exposed to highly specialized domains that
require deep cultural, linguistic, and subject-matter expertise. In particular,
traditional medical systems such as Ayurveda embody centuries of nuanced
textual and clinical knowledge that mainstream LLMs fail to accurately
interpret or apply. We introduce AyurParam-2.9B, a domain-specialized,
bilingual language model fine-tuned from Param-1-2.9B using an extensive,
expertly curated Ayurveda dataset spanning classical texts and clinical
guidance. AyurParam's dataset incorporates context-aware, reasoning, and
objective-style Q&A in both English and Hindi, with rigorous annotation
protocols for factual precision and instructional clarity. Benchmarked on
BhashaBench-Ayur, AyurParam not only surpasses all open-source
instruction-tuned models in its size class (1.5--3B parameters), but also
demonstrates competitive or superior performance compared to much larger
models. The results from AyurParam highlight the necessity for authentic domain
adaptation and high-quality supervision in delivering reliable, culturally
congruent AI for specialized medical knowledge.

</details>


### [7] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2511.02376)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CL

TL;DR: AutoAdv是一个无需训练的多轮越狱框架，通过自适应机制在多轮对话中实现高达95%的攻击成功率，揭示了当前单轮防御策略的不足。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭露大型语言模型在多轮对话中对抗攻击的脆弱性，现有评估多集中于单轮互动，而实际攻击往往是自适应的多轮对话。

Method: AutoAdv结合了三种自适应机制：模式管理器从成功攻击中学习以改进提示，温度管理器根据失败模式动态调整采样参数，以及两阶段重写策略来伪装和优化有害请求。

Result: AutoAdv在Llama-3.1-8B上六轮内达到95%的攻击成功率，比单轮基线提升了24%，并在多个商业和开源模型上验证了多轮攻击的持续性优势。

Conclusion: 研究结论指出，针对单轮互动的安全对齐策略在多轮对话中失效，亟需开发多轮感知的防御机制。

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where
adversarial prompts elicit harmful outputs, yet most evaluations focus on
single-turn interactions while real-world attacks unfold through adaptive
multi-turn conversations. We present AutoAdv, a training-free framework for
automated multi-turn jailbreaking that achieves up to 95% attack success rate
on Llama-3.1-8B within six turns a 24 percent improvement over single turn
baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern
manager that learns from successful attacks to enhance future prompts, a
temperature manager that dynamically adjusts sampling parameters based on
failure modes, and a two-phase rewriting strategy that disguises harmful
requests then iteratively refines them. Extensive evaluation across commercial
and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent
vulnerabilities in current safety mechanisms, with multi-turn attacks
consistently outperforming single-turn approaches. These findings demonstrate
that alignment strategies optimized for single-turn interactions fail to
maintain robustness across extended conversations, highlighting an urgent need
for multi-turn-aware defenses.

</details>


### [8] [Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance](https://arxiv.org/abs/2511.02451)
*Kentaro Ueda,François Portet,Hirohiko Suwa,Keiichi Yasumoto*

Main category: cs.CL

TL;DR: LLMs在专业领域（如金融）表现不足，需要多种技能。通过结合领域特定的连续预训练（CPT）专家模型，提出一种替代多技能训练的方法。研究探索了三种合并方法，发现合并专家模型能提升性能并可能产生跨领域技能。


<details>
  <summary>Details</summary>
Motivation: LLMs在通用任务中表现出色，但在专业领域（如金融）表现不佳，需要结合多种技能。目前对CPT模型合并的研究较少，而这是构建多技能LLMs的潜在高效方法。

Method: 提出三阶段评估方法（知识恢复、互补性和涌现性），并比较三种CPT模型合并方法（Task Arithmetic、TIES和DARE-TIES），使用来自18个任务的8个金融数据集进行测试。

Result: 合并专家模型能恢复CPT过程中丢失的通用知识，同时提升性能并可能产生跨领域技能。Task Arithmetic表现强但对超参数敏感，TIES更稳健。模型相似性与合并成功相关，但涌现技能受更复杂因素影响。

Conclusion: 这是对CPT模型合并的首个基础分析，为从现有资产构建多技能LLMs提供了原则性框架和实用指导。

Abstract: While LLMs excel at general tasks, they struggle in specialized domains like
finance, requiring diverse skills in domain knowledge, mathematical reasoning,
and multilingual processing. Merging domain-specific Continual Pre-training
(CPT) "experts" offers a practical alternative to costly and unstable
multi-skill training. However, unlike established Supervised Fine-Tuning (SFT)
model-based merging, CPT model merging remains largely unexplored. We address
this gap by creating financial LLMs from experts in finance, math, and
Japanese. We propose a three-stage evaluation focusing on knowledge recovery,
complementarity, and emergence, and assess three merging methods (Task
Arithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated
from 18 tasks across 8 established datasets. Results show that merging an
expert with its base model recovers general knowledge lost during CPT, while
merging experts improves performance and can yield emergent cross-domain
skills. Among the methods, Task Arithmetic performs strongly but is
hyperparameter-sensitive, whereas TIES is more robust. Our findings also
suggest that while model similarity correlates with merging success, emergent
skills depend on more complex factors. This work presents the first
foundational analysis of CPT model merging, establishing a principled framework
and providing clear guidance for building multi-skill LLMs from existing
assets.

</details>


### [9] [Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas](https://arxiv.org/abs/2511.02458)
*Giulia Iadisernia,Carolina Camassa*

Main category: cs.CL

TL;DR: 论文研究基于角色的提示是否能提升大语言模型（LLM）在宏观经济预测任务中的表现，发现GPT-4o与人专家预测准确性相近，但角色描述无显著优势。


<details>
  <summary>Details</summary>
Motivation: 探讨角色提示对LLM在宏观经济预测任务中的效果，比较模型与人类专家的预测准确性。

Method: 使用PersonaHub中的2,368个经济相关角色提示GPT-4o，进行50轮ECB专业预测调查（2013-2025），并与无角色描述的基线预测对比。

Result: GPT-4o与人类专家预测准确性相似且显著；角色描述无实际预测优势；模型在样本外数据上表现良好。

Conclusion: GPT-4o在宏观经济预测中可与人专家竞争，角色提示非必要，提示多样化未显著提升预测差异性。

Abstract: We evaluate whether persona-based prompting improves Large Language Model
(LLM) performance on macroeconomic forecasting tasks. Using 2,368
economics-related personas from the PersonaHub corpus, we prompt GPT-4o to
replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds
(2013-2025). We compare the persona-prompted forecasts against the human
experts panel, across four target variables (HICP, core HICP, GDP growth,
unemployment) and four forecast horizons. We also compare the results against
100 baseline forecasts without persona descriptions to isolate its effect. We
report two main findings. Firstly, GPT-4o and human forecasters achieve
remarkably similar accuracy levels, with differences that are statistically
significant yet practically modest. Our out-of-sample evaluation on 2024-2025
data demonstrates that GPT-4o can maintain competitive forecasting performance
on unseen events, though with notable differences compared to the in-sample
period. Secondly, our ablation experiment reveals no measurable forecasting
advantage from persona descriptions, suggesting these prompt components can be
omitted to reduce computational costs without sacrificing accuracy. Our results
provide evidence that GPT-4o can achieve competitive forecasting accuracy even
on out-of-sample macroeconomic events, if provided with relevant context data,
while revealing that diverse prompts produce remarkably homogeneous forecasts
compared to human panels.

</details>


### [10] [The Analysis of Lexical Errors in Machine Translation from English into Romanian](https://arxiv.org/abs/2511.02587)
*Angela Stamatie*

Main category: cs.CL

TL;DR: 研究分析了谷歌翻译在将WHO、Gavi组织和药物说明等官方英文文本译为罗马尼亚语时的词汇错误，旨在改进机器翻译的词汇选择和减少错误。


<details>
  <summary>Details</summary>
Motivation: 提升机器翻译的整体质量，特别是在处理与COVID-19相关的官方文本时，减少词汇错误。

Method: 通过对230篇英译罗马尼亚语的文本进行全面分析，重点关注词汇错误。

Result: 研究发现谷歌翻译在词汇选择上存在错误，需要改进。

Conclusion: 通过改进词汇选择和减少错误，可以显著提升机器翻译的质量，尤其是在处理专业和官方文本时。

Abstract: The research explores error analysis in the performance of translating by
Machine Translation from English into Romanian, and it focuses on lexical
errors found in texts which include official information, provided by the World
Health Organization (WHO), the Gavi Organization, by the patient information
leaflet (the information about the active ingredients of the vaccines or the
medication, the indications, the dosage instructions, the storage instructions,
the side effects and warning, etc.). All of these texts are related to Covid-19
and have been translated by Google Translate, a multilingual Machine
Translation that was created by Google. In the last decades, Google has
actively worked to develop a more accurate and fluent automatic translation
system. This research, specifically focused on improving Google Translate, aims
to enhance the overall quality of Machine Translation by achieving better
lexical selection and by reducing errors. The investigation involves a
comprehensive analysis of 230 texts that have been translated from English into
Romanian.

</details>


### [11] [Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour](https://arxiv.org/abs/2511.02599)
*Max Norris,Kobi Gal,Sahan Bulathwela*

Main category: cs.CL

TL;DR: NTKT是一种新颖的知识追踪方法，通过将问题文本与学生历史作为序列输入预训练语言模型，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型通常忽略问题文本的潜力，限制了预测性能。

Method: 提出Next Token Knowledge Tracing (NTKT)，将知识追踪任务重构为基于预训练大型语言模型的下一标记预测任务。

Result: NTKT在实验中显著优于现有神经知识追踪模型，对冷启动问题和用户表现更好。

Conclusion: 问题文本在知识追踪中至关重要，预训练语言模型能更有效地建模学生学习。

Abstract: Modelling student knowledge is a key challenge when leveraging AI in
education, with major implications for personalised learning. The Knowledge
Tracing (KT) task aims to predict how students will respond to educational
questions in learning environments, based on their prior interactions. Existing
KT models typically use response correctness along with metadata like skill
tags and timestamps, often overlooking the question text, which is an important
source of pedagogical insight. This omission poses a lost opportunity while
limiting predictive performance. We propose Next Token Knowledge Tracing
(NTKT), a novel approach that reframes KT as a next-token prediction task using
pretrained Large Language Models (LLMs). NTKT represents both student histories
and question content as sequences of text, allowing LLMs to learn patterns in
both behaviour and language. Our series of experiments significantly improves
performance over state-of-the-art neural KT models and generalises much better
to cold-start questions and users. These findings highlight the importance of
question content in KT and demonstrate the benefits of leveraging pretrained
representations of LLMs to model student learning more effectively.

</details>


### [12] [CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency](https://arxiv.org/abs/2511.02603)
*Ehsan Aghazadeh,Ahmad Ghasemi,Hedyeh Beyhaghi,Hossein Pishro-Nik*

Main category: cs.CL

TL;DR: CGES是一种基于贝叶斯框架的自适应停止采样方法，通过置信信号动态调整模型调用次数，显著减少调用次数同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决传统自一致性策略需要固定调用次数且无法处理正确答案稀少的情况。

Method: 引入Confidence-Guided Early Stopping (CGES)，利用标量置信信号形成候选答案的后验概率，动态停止采样。

Result: 在五个推理基准测试中，CGES平均减少69%的模型调用次数，准确率与自一致性策略相差仅0.06个百分点。

Conclusion: CGES是一种高效且准确的方法，适用于需要多次调用大语言模型的场景。

Abstract: Large language models (LLMs) are often queried multiple times at test time,
with predictions aggregated by majority vote. While effective, this
self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls
and can fail when the correct answer is rare. We introduce Confidence-Guided
Early Stopping (CGES), a Bayesian framework that forms posteriors over
candidate answers using scalar confidence signals derived from token
probabilities or reward models. CGES adaptively halts sampling once the
posterior mass of a candidate exceeds a threshold. We provide theoretical
guarantees for both perfectly calibrated confidences and realistic noisy
confidence signals. Across five reasoning benchmarks, CGES reduces the average
number of model calls by about 69 percent (for example, from 16.0 to 4.9) while
matching the accuracy of self-consistency within 0.06 percentage points.

</details>


### [13] [The Realignment Problem: When Right becomes Wrong in LLMs](https://arxiv.org/abs/2511.02623)
*Aakash Sen Sharma,Debdeep Sanyal,Vivek Srivastava,Shirish Karande,Murari Mandal*

Main category: cs.CL

TL;DR: TRACE框架通过程序化策略应用和混合优化，解决了大型语言模型（LLM）与现实政策对齐的难题，实现了动态、可扩展且成本效益高的重新对齐。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法静态且成本高昂，无法适应政策和规范的动态变化，导致对齐与现实之间的差距（Alignment-Reality Gap），亟需新的解决方案。

Method: TRACE框架通过以下步骤实现重新对齐：1）程序化筛选现有偏好数据与新政策的冲突；2）利用对齐影响评分识别高冲突点；3）通过混合优化（反转、丢弃或保留偏好）实现精确政策更新，同时保护模型性能。

Result: 实验显示，TRACE在多个模型家族（Qwen2.5-7B、Gemma-2-9B、Llama-3.1-8B）及复杂政策变化下（PKU-SafeRLHF数据集）均能稳健对齐新政策，且不损害通用能力。

Conclusion: TRACE为LLM的对齐提供了一种动态、可扩展且经济高效的新范式，为可持续和负责任的AI部署奠定了基础。

Abstract: The alignment of Large Language Models (LLMs) with human values is central to
their safe deployment, yet current practice produces static, brittle, and
costly-to-maintain models that fail to keep pace with evolving norms and
policies. This misalignment, which we term the Alignment-Reality Gap, poses a
growing challenge for reliable long-term use. Existing remedies are inadequate:
large-scale re-annotation is economically prohibitive, and standard unlearning
methods act as blunt instruments that erode utility rather than enable precise
policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict
Evaluation), a framework for principled unlearning that reconceives
re-alignment as a programmatic policy application problem. TRACE
programmatically triages existing preference data against a new policy,
identifies high-impact conflicts via a alignment impact score, and applies a
hybrid optimization that cleanly inverts, discards, or preserves preferences
while safeguarding model performance. Empirical results show that TRACE
achieves robust re-alignment across diverse model families (Qwen2.5-7B,
Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF
dataset under complex policy shift, TRACE enforces new principles without
degrading general capabilities. Our work establishes a scalable, dynamic, and
cost-effective paradigm for maintaining LLM alignment, providing a foundation
for sustainable and responsible AI deployment.

</details>


### [14] [Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation](https://arxiv.org/abs/2511.02626)
*Renfei Dang,Peng Hu,Changjiang Gao,Shujian Huang*

Main category: cs.CL

TL;DR: 研究发现，在LLMs微调时引入新知识会增加事实幻觉的倾向，尤其是在特定知识类型完全由新知识组成时。提出的KnownPatch方法通过后期引入少量已知知识样本有效缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs微调中新知识引入如何导致事实幻觉的具体表现和机制，并寻找缓解方法。

Method: 设计控制数据集Biography-Reasoning，分析多种知识类型和任务类型（QA和推理），提出KnownPatch方法。

Result: 特定知识类型的高陌生度是幻觉的主要驱动因素，KnownPatch能有效缓解幻觉并改善模型性能。

Conclusion: 高陌生知识类型易引发幻觉，KnownPatch通过调整注意力分配有效缓解这一问题。

Abstract: Previous studies show that introducing new knowledge during large language
models (LLMs) fine-tuning can lead to the generation of erroneous output when
tested on known information, thereby triggering factual hallucinations.
However, existing studies have not deeply investigated the specific
manifestations and underlying mechanisms of these hallucinations. Our work
addresses this gap by designing a controlled dataset Biography-Reasoning, and
conducting a fine-grained analysis across multiple knowledge types and two task
types, including knowledge question answering (QA) and knowledge reasoning
tasks. We find that when fine-tuned on a dataset in which a specific knowledge
type consists entirely of new knowledge, LLMs exhibit significantly increased
hallucination tendencies. This suggests that the high unfamiliarity of a
particular knowledge type, rather than the overall proportion of new knowledge,
is a stronger driver of hallucinations, and these tendencies can even affect
other knowledge types in QA tasks. To mitigate such factual hallucinations, we
propose KnownPatch, which patches a small number of known knowledge samples in
the later stages of training, effectively alleviating new-knowledge-induced
hallucinations. Through attention analysis, we find that learning new knowledge
reduces the model's attention to key entities in the question, thus causing
excessive focus on the surrounding context, which may increase the risk of
hallucination. Moreover, the attention pattern can propagate to similar
contexts, facilitating the spread of hallucinations to textually similar
questions. Our method effectively mitigates the disruption of new knowledge
learning to the model's attention on key entities, accompanied by improved
performance.

</details>


### [15] [Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes](https://arxiv.org/abs/2511.02681)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.CL

TL;DR: 该论文提出了一种高效存储预训练大语言模型（LLM）微调参数更新的方法，结合低秩和稀疏性特点，优化存储效率，同时保持模型表现力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型体积庞大且微调版本存储成本高，研究表明微调仅影响少量参数，亟需高效存储方法。

Method: 提出最优奇异值损伤法，选择性稀疏化低秩近似更新，保留关键奇异成分以提升存储效率。

Result: 实验表明，该方法在相同存储预算下显著优于单独使用低秩近似或稀疏化的方法。

Conclusion: 结合低秩和稀疏性的选择性处理可在高效存储的同时保持模型准确性。

Abstract: Large language models (LLMs) are increasingly prevalent across diverse
applications. However, their enormous size limits storage and processing
capabilities to a few well-resourced stakeholders. As a result, most
applications rely on pre-trained LLMs, fine-tuned for specific tasks. However,
even storing the fine-tuned versions of these models remains a significant
challenge due to the wide range of tasks they address. Recently, studies show
that fine-tuning these models primarily affects a small fraction of parameters,
highlighting the need for more efficient storage of fine-tuned models. This
paper focuses on efficient storage of parameter updates in pre-trained models
after fine-tuning. To address this challenge, we leverage the observation that
fine-tuning updates are both low-rank and sparse, which can be utilized for
storage efficiency. However, using only low-rank approximation or
sparsification may discard critical singular components that enhance model
expressivity. We first observe that given the same memory budget, sparsified
low-rank approximations with larger ranks outperform standard low-rank
approximations with smaller ranks. Building on this, we propose our method,
optimal singular damage, that selectively sparsifies low-rank approximated
updates by leveraging the interleaved importance of singular vectors, ensuring
that the most impactful components are retained. We demonstrate through
extensive experiments that our proposed methods lead to significant storage
efficiency and superior accuracy within the same memory budget compared to
employing the low-rank approximation or sparsification individually.

</details>


### [16] [PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation](https://arxiv.org/abs/2511.02721)
*Doreen Osmelak,Koel Dutta Chowdhury,Uliana Sentsova,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: PragExTra是一个多语言语料库和检测框架，用于研究翻译中的语用显化现象，覆盖八种语言对，并通过主动学习和人类标注提高分类器准确性。


<details>
  <summary>Details</summary>
Motivation: 翻译过程中常通过增加背景信息使隐含的文化意义显化，这种现象在翻译理论中广泛讨论但缺乏计算建模。

Method: 利用TED-Multi和Europarl构建多语言语料库，通过空对齐和主动学习结合人类标注识别显化案例。

Result: 实体和系统级显化最常见，主动学习使分类器准确性提升7-8个百分点，最高准确率达到0.88，F1分数为0.82。

Conclusion: PragExTra将语用显化定义为可测量的跨语言现象，并为构建文化感知的机器翻译奠定了基础。

Abstract: Translators often enrich texts with background details that make implicit
cultural meanings explicit for new audiences. This phenomenon, known as
pragmatic explicitation, has been widely discussed in translation theory but
rarely modeled computationally. We introduce PragExTra, the first multilingual
corpus and detection framework for pragmatic explicitation. The corpus covers
eight language pairs from TED-Multi and Europarl and includes additions such as
entity descriptions, measurement conversions, and translator remarks. We
identify candidate explicitation cases through null alignments and refined
using active learning with human annotation. Our results show that entity and
system-level explicitations are most frequent, and that active learning
improves classifier accuracy by 7-8 percentage points, achieving up to 0.88
accuracy and 0.82 F1 across languages. PragExTra establishes pragmatic
explicitation as a measurable, cross-linguistic phenomenon and takes a step
towards building culturally aware machine translation. Keywords: translation,
multilingualism, explicitation

</details>


### [17] [AI Diffusion in Low Resource Language Countries](https://arxiv.org/abs/2511.02752)
*Amit Misra,Syed Waqas Zamir,Wassim Hamidouche,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

TL;DR: 研究探讨了AI在全球推广中的语言障碍问题，发现低资源语言国家（LRLCs）的AI用户比例因语言性能差而降低约20%。


<details>
  <summary>Details</summary>
Motivation: 探索AI在低资源语言国家（LRLCs）中推广缓慢的原因，尤其是语言数据稀缺对AI性能的影响。

Method: 使用加权回归模型分离语言效应与社会经济和人口因素，分析LRLCs的AI用户比例差异。

Result: LRLCs的AI用户比例相比基线低约20%，语言可访问性是AI公平推广的重要障碍。

Conclusion: 语言性能不足显著阻碍了AI在LRLCs的公平推广，需解决语言数据稀缺问题。

Abstract: Artificial intelligence (AI) is diffusing globally at unprecedented speed,
but adoption remains uneven. Frontier Large Language Models (LLMs) are known to
perform poorly on low-resource languages due to data scarcity. We hypothesize
that this performance deficit reduces the utility of AI, thereby slowing
adoption in Low-Resource Language Countries (LRLCs). To test this, we use a
weighted regression model to isolate the language effect from socioeconomic and
demographic factors, finding that LRLCs have a share of AI users that is
approximately 20% lower relative to their baseline. These results indicate that
linguistic accessibility is a significant, independent barrier to equitable AI
diffusion.

</details>


### [18] [Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval](https://arxiv.org/abs/2511.02770)
*Hung-Ting Chen,Xiang Liu,Shauli Ravfogel,Eunsol Choi*

Main category: cs.CL

TL;DR: AMER是一种自回归多嵌入检索器，通过生成多个查询向量解决传统检索器在多模态文档检索中的局限性，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 传统检索器仅生成单一查询向量，无法有效处理文档条件分布的多模态性（如查询的不同解释）。

Method: 提出AMER模型，自回归生成多个查询向量，用于检索文档，并在合成数据及真实多答案数据集上微调。

Result: 在合成数据上性能提升4倍；两个真实数据集上分别相对提升4%和21%，尤其在目标文档嵌入相似度低的子集上效果更佳。

Conclusion: AMER展示了多查询向量检索器的潜力，为未来研究方向开辟了新路径。

Abstract: Most text retrievers generate \emph{one} query vector to retrieve relevant
documents. Yet, the conditional distribution of relevant documents for the
query may be multimodal, e.g., representing different interpretations of the
query. We first quantify the limitations of existing retrievers. All retrievers
we evaluate struggle more as the distance between target document embeddings
grows. To address this limitation, we develop a new retriever architecture,
\emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER).
Our model autoregressively generates multiple query vectors, and all the
predicted query vectors are used to retrieve documents from the corpus. We show
that on the synthetic vectorized data, the proposed method could capture
multiple target distributions perfectly, showing 4x better performance than
single embedding model. We also fine-tune our model on real-world multi-answer
retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative
gains over single-embedding baselines on two datasets we evaluate on.
Furthermore, we consistently observe larger gains on the subset of dataset
where the embeddings of the target documents are less similar to each other. We
demonstrate the potential of using a multi-query vector retriever and open up a
new direction for future work.

</details>


### [19] [Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities](https://arxiv.org/abs/2511.02817)
*Amanda Bertsch,Adithya Pratapa,Teruko Mitamura,Graham Neubig,Matthew R. Gormley*

Main category: cs.CL

TL;DR: Oolong 是一个新的长上下文推理基准，旨在测试模型在原子级别分析文本并聚合信息的能力，任务分为合成和现实两类，前沿模型在128K上下文下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着模型上下文长度的增加，需要验证模型是否能有效利用全部上下文。现有评测多依赖检索任务，忽略了对文本深层推理的需求。

Method: 提出 Oolong 基准，包含合成任务（Oolong-synth）和现实任务（Oolong-real），要求模型分析文本块并回答分布性问题。

Result: 前沿模型（如 GPT-5、Claude-Sonnet-4 和 Gemini-2.5-Pro）在 128K 上下文下准确率均低于 50%。

Conclusion: Oolong 为开发能处理大量文本推理的模型提供了数据和评测工具。

Abstract: As model context lengths continue to grow, concerns about whether models
effectively use the full context length have persisted. While several carefully
designed long-context evaluations have recently been released, these
evaluations tend to rely on retrieval from one or more sections of the context,
which allows nearly all of the context tokens to be disregarded as noise. This
represents only one type of task that might be performed with long context. We
introduce Oolong, a benchmark of long-context reasoning tasks that require
analyzing individual chunks of text on an atomic level, and then aggregating
these analyses to answer distributional questions. Oolong is separated into two
task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can
easily ablate components of the reasoning problem; and Oolong-real, a
downstream setting which requires reasoning over real-world conversational
data. Oolong requires models to reason over large quantities of examples, to
perform both classification and counting in-context, and to reason over
temporal and user relations. Even frontier models struggle on Oolong, with
GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy
on both splits at 128K. We release the data and evaluation harness for Oolong
to enable further development of models that can reason over large quantities
of text.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing](https://arxiv.org/abs/2511.02071)
*Xinyi Lin,Yuyang Zhang,Yuanhang Gan,Juntao Chen,Hao Shen,Yichun He,Lijun Li,Ze Yuan,Shuang Wang,Chaohao Wang,Rui Zhang,Na Li,Jia Liu*

Main category: cs.AI

TL;DR: 论文提出了一种结合人类、AI和可穿戴设备的新型智能系统，用于提升科学实验和制造的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和自动化技术无法完全替代人类在物理领域的专业知识和执行，导致科学实验和制造的可扩展性和可重复性受到限制。

Method: 提出了一种人类与AI协同的智能系统（APEX），通过可穿戴设备和混合现实技术，结合人类的精确执行和AI的上下文推理与实时反馈。

Result: 在柔性电子制造中，APEX系统实现了高精度的上下文推理、实时纠错，并能将专业知识传递给新手。

Conclusion: 该系统将AI推理能力扩展到物理领域，使科学研究和制造更加自主、可追溯、可解释和可扩展。

Abstract: Scientific experiment and manufacture rely on complex, multi-step procedures
that demand continuous human expertise for precise execution and
decision-making. Despite advances in machine learning and automation,
conventional models remain confined to virtual domains, while real-world
experiment and manufacture still rely on human supervision and expertise. This
gap between machine intelligence and physical execution limits reproducibility,
scalability, and accessibility across scientific and manufacture workflows.
Here, we introduce human-AI co-embodied intelligence, a new form of physical AI
that unites human users, agentic AI, and wearable hardware into an integrated
system for real-world experiment and intelligent manufacture. In this paradigm,
humans provide precise execution and control, while agentic AI contributes
memory, contextual reasoning, adaptive planning, and real-time feedback. The
wearable interface continuously captures the experimental and manufacture
processes, facilitates seamless communication between humans and AI for
corrective guidance and interpretable collaboration. As a demonstration, we
present Agentic-Physical Experimentation (APEX) system, coupling agentic
reasoning with physical execution through mixed-reality. APEX observes and
interprets human actions, aligns them with standard operating procedures,
provides 3D visual guidance, and analyzes every step. Implemented in a
cleanroom for flexible electronics fabrication, APEX system achieves
context-aware reasoning with accuracy exceeding general multimodal large
language models, corrects errors in real time, and transfers expertise to
beginners. These results establish a new class of agentic-physical-human
intelligence that extends agentic reasoning beyond computation into the
physical domain, transforming scientific research and manufacturing into
autonomous, traceable, interpretable, and scalable processes.

</details>


### [21] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: Deep Value Benchmark（DVB）是一个评估框架，用于测试大型语言模型（LLMs）是否学习到人类深层价值观还是仅模仿表层偏好，发现当前模型的价值观泛化能力低于随机水平。


<details>
  <summary>Details</summary>
Motivation: 区分AI系统是否学习到深层价值观（如道德原则）或仅捕捉表层偏好（如语言形式）对AI对齐至关重要。缺乏深层理解的模型可能导致行为失调。

Method: DVB通过实验设计控制深层价值观与表层特征的混淆，训练阶段暴露模型于相关数据，测试阶段打破相关性，测量模型的深层价值观泛化率（DVGR）。

Result: 9个模型的平均DVGR仅为0.30，均低于随机水平；更大模型的DVGR更低。

Conclusion: DVB为评估AI对齐核心特性提供了可解释的度量，现有模型在价值观泛化上表现不足。

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [22] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Dan M. Frangopol,Minghui Cheng*

Main category: cs.AI

TL;DR: 该研究提出了一种基于大语言模型（LLM）的工具InsurAgent，用于模拟洪水保险购买决策，结合检索增强生成（RAG）和记忆模块，弥补了LLM在定量概率估计上的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管洪水保险能有效减少灾害损失，但美国高风险人群的参保率仍然很低，因此需要研究保险决策的行为机制。

Method: 通过构建基准数据集评估LLM的能力，并提出InsurAgent工具，包括感知、检索、推理、行动和记忆五个模块，结合RAG和常识推理改进决策模拟。

Result: InsurAgent能够准确估计边际和二元概率，并通过记忆模块支持时间决策演化的模拟。

Conclusion: InsurAgent为行为建模和政策分析提供了有价值的工具。

Abstract: Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [23] [Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.02130)
*Renos Zabounidis,Aditya Golatkar,Michael Kleinman,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: Re-FORC是一种自适应奖励预测方法，通过训练轻量级适配器优化推理模型，显著提升计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 为解决推理模型中计算资源浪费和准确性不足的问题，提出动态调整推理长度和模型选择的方法。

Method: 训练轻量级适配器预测未来奖励，支持动态停止无希望的推理链，并优化模型和推理长度选择。

Result: 减少26%计算成本的同时保持准确性，优化模型选择提升4%准确性或节省55%计算成本，自适应调整在高/低计算环境下分别提升11%和7%准确性。

Conclusion: Re-FORC通过动态控制和预测，有效平衡计算成本与推理准确性，适用于多样化计算需求场景。

Abstract: We propose Re-FORC, an adaptive reward prediction method that, given a
context, enables prediction of the expected future rewards as a function of the
number of future thinking tokens. Re-FORC trains a lightweight adapter on
reasoning models, demonstrating improved prediction with longer reasoning and
larger models. Re-FORC enables: 1) early stopping of unpromising reasoning
chains, reducing compute by 26% while maintaining accuracy, 2) optimized model
and thinking length selection that achieves 4% higher accuracy at equal compute
and 55% less compute at equal accuracy compared to the largest model, 3)
adaptive test-time scaling, which increases accuracy by 11% in high compute
regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with
length control via cost-per-token thresholds while estimating computation time
upfront.

</details>


### [24] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: 论文提出了ATHENA框架，结合符号效用建模和语义适应，优化个性化决策建模，在疫苗选择和出行模式任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 个体决策与群体最优预测存在差异，需结合数值属性和语言影响，开发更精准的个性化决策模型。

Method: ATHENA框架分两步：1) 通过LLM增强发现群体级符号效用函数；2) 基于最优效用进行个体级语义适配。

Result: 在疫苗选择和出行模式任务中，ATHENA的F1分数比最优前沿模型提升至少6.5%。

Conclusion: ATHENA通过符号与语义的有机结合，为人本决策建模提供了新方案。

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [25] [Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration](https://arxiv.org/abs/2511.02200)
*Jingbo Wang,Sendong Zhao,Haochun Wang,Yuzheng Fan,Lizhe Zhang,Yan Liu,Ting Liu*

Main category: cs.AI

TL;DR: 论文提出了一种名为STRMAC的状态感知路由框架，用于提升多智能体系统中的协作效率，通过动态选择合适智能体和自我进化数据生成方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统在任务解决中虽然表现出潜力，但因为调度和协调策略不灵活，限制了其充分发挥。

Method: 提出了STRMAC框架，通过分别编码交互历史和智能体知识来动态选择最合适的智能体，并采用自我进化数据生成方法加速高质量执行路径的收集。

Result: 在协作推理基准测试中，STRMAC实现了23.8%的性能提升，并将数据收集开销减少90.1%。

Conclusion: STRMAC通过灵活的路由和高效的数据生成，显著提升了多智能体系统的协作效率和适应性。

Abstract: The emergence of multi-agent systems powered by large language models (LLMs)
has unlocked new frontiers in complex task-solving, enabling diverse agents to
integrate unique expertise, collaborate flexibly, and address challenges
unattainable for individual models. However, the full potential of such systems
is hindered by rigid agent scheduling and inefficient coordination strategies
that fail to adapt to evolving task requirements. In this paper, we propose
STRMAC, a state-aware routing framework designed for efficient collaboration in
multi-agent systems. Our method separately encodes interaction history and
agent knowledge to power the router, which adaptively selects the most suitable
single agent at each step for efficient and effective collaboration.
Furthermore, we introduce a self-evolving data generation approach that
accelerates the collection of high-quality execution paths for efficient system
training. Experiments on challenging collaborative reasoning benchmarks
demonstrate that our method achieves state-of-the-art performance, achieving up
to 23.8% improvement over baselines and reducing data collection overhead by up
to 90.1% compared to exhaustive search.

</details>


### [26] [TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data](https://arxiv.org/abs/2511.02219)
*Changjiang Jiang,Fengchang Yu,Haihua Chen,Wei Lu,Jin Zeng*

Main category: cs.AI

TL;DR: 论文提出了一种名为\method的框架，用于提升大语言模型在表格数据复杂推理中的性能，通过查询分解、表格净化和基于思维程序的推理器实现，并在新数据集CalTab151上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理复杂表格查询、噪声数据和数值推理时表现不佳，需要一种更有效的解决方案。

Method: 框架包含查询分解器、表格净化器和基于思维程序的推理器，生成可执行代码以从净化后的表格中得出答案。

Result: 实验表明，\method在多个数据集上优于现有方法，准确率提升显著，且能无缝集成主流大语言模型。

Conclusion: 该框架有效提升了大语言模型在表格数据复杂推理中的性能，为实际数据分析提供了可靠方案。

Abstract: Complex reasoning over tabular data is crucial in real-world data analysis,
yet large language models (LLMs) often underperform due to complex queries,
noisy data, and limited numerical capabilities. To address these issues, we
propose \method, a framework consisting of: (1) a query decomposer that breaks
down complex questions, (2) a table sanitizer that cleans and filters noisy
tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates
executable code to derive the final answer from the sanitized table. To ensure
unbiased evaluation and mitigate data leakage, we introduce a new dataset,
CalTab151, specifically designed for complex numerical reasoning over tables.
Experimental results demonstrate that \method consistently outperforms existing
methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and
19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively.
Moreover, our framework integrates seamlessly with mainstream LLMs, providing a
robust solution for complex tabular numerical reasoning. These findings
highlight the effectiveness of our framework in enhancing LLM performance for
complex tabular numerical reasoning. Data and code are available upon request.

</details>


### [27] [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)
*Zhuoran Zhang,Tengyue Wang,Xilin Gong,Yang Shi,Haotian Wang,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: 本文提出了一种新的框架，将多模态大语言模型（MLLMs）的模态跟随行为分解为两个关键因素：相对推理不确定性和固有模态偏好，并通过实验验证了这一框架的合理性。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型在解决不同模态提供的矛盾信息时的行为机制，弥补了之前研究仅通过粗粒度数据集统计的不足。

Method: 提出了一个新框架，分解模态跟随行为为相对推理不确定性和固有模态偏好；构建了一个可控数据集，系统化调节视觉和文本输入的推理难度；使用熵作为细粒度的不确定性度量。

Result: 发现了一个普遍规律：跟随某一模态的概率随其相对不确定性的增加而单调下降；揭示了模型在平衡点附近的内部振荡机制。

Conclusion: 相对不确定性和固有偏好是模态跟随的两个主导原则，为理解MLLMs如何解决矛盾信息提供了定量框架和机制性见解。

Abstract: Multimodal large language models (MLLMs) must resolve conflicts when
different modalities provide contradictory information, a process we term
modality following. Prior work measured this behavior only with coarse
dataset-level statistics, overlooking the influence of model's confidence in
unimodal reasoning. In this paper, we introduce a new framework that decomposes
modality following into two fundamental factors: relative reasoning uncertainty
(the case-specific confidence gap between unimodal predictions) and inherent
modality preference( a model's stable bias when uncertainties are balanced). To
validate this framework, we construct a controllable dataset that
systematically varies the reasoning difficulty of visual and textual inputs.
Using entropy as a fine-grained uncertainty metric, we uncover a universal law:
the probability of following a modality decreases monotonically as its relative
uncertainty increases. At the relative difficulty level where the model tends
to follow both modalities with comparable probability what we call the balance
point, a practical indicator of the model's inherent preference. Unlike
traditional macro-level ratios, this measure offers a more principled and less
confounded way to characterize modality bias, disentangling it from unimodal
capabilities and dataset artifacts. Further, by probing layer-wise predictions,
we reveal the internal mechanism of oscillation: in ambiguous regions near the
balance point, models vacillate between modalities across layers, explaining
externally observed indecision. Together, these findings establish relative
uncertainty and inherent preference as the two governing principles of modality
following, offering both a quantitative framework and mechanistic insight into
how MLLMs resolve conflicting information.

</details>


### [28] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 论文分析了多智能体推理中自然出现的懒惰行为问题，提出了测量因果影响的方法和可验证奖励机制，以解决协作中的问题。


<details>
  <summary>Details</summary>
Motivation: 虽然多智能体框架在复杂推理任务中表现良好，但存在懒惰行为问题（一个智能体主导，另一个贡献很少），这削弱了协作效果。论文旨在解决这一问题。

Method: 1. 理论分析懒惰行为的成因。2. 引入稳定的因果影响测量方法。3. 提出可验证奖励机制，允许推理智能体丢弃噪声输出并重启推理过程。

Result: 实验表明，所提框架有效缓解懒惰行为，充分发挥多智能体框架在复杂推理任务中的潜力。

Conclusion: 通过理论分析和实用方法，论文成功解决了多智能体协作中的懒惰行为问题，提升了推理任务的性能。

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [29] [Chronic Kidney Disease Prognosis Prediction Using Transformer](https://arxiv.org/abs/2511.02340)
*Yohan Lee,DongGyun Kang,SeHoon Park,Sa-Yoon Park,Kwangsoo Kim*

Main category: cs.AI

TL;DR: 本论文提出了一个基于Transformer的框架（ProQ-BERT）用于预测慢性肾脏病（CKD）的进展，通过多模态电子健康记录（EHR）进行量化标记化和注意力机制分析，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 慢性肾脏病（CKD）影响了全球近10%的人口，且常进展为终末期肾衰竭。准确的预后预测对及时干预和资源优化至关重要。

Method: 使用来自首尔国立大学医院的OMOP通用数据模型的多模态EHR数据，结合量化标记化处理连续实验室数据，并通过注意力机制提高可解释性。模型通过掩码语言建模预训练，并针对从CKD 3a期到5期的二元分类任务进行微调。

Result: 在91,816名患者的队列中评估显示，该模型在短期预测中ROC-AUC达到0.995，PR-AUC达到0.989，显著优于CEHR-BERT。

Conclusion: 该研究表明Transformer架构和时态设计在临床预后建模中的有效性，为个性化CKD护理提供了有前景的方向。

Abstract: Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and
often progresses to end-stage renal failure. Accurate prognosis prediction is
vital for timely interventions and resource optimization. We present a
transformer-based framework for predicting CKD progression using multi-modal
electronic health records (EHR) from the Seoul National University Hospital
OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates
demographic, clinical, and laboratory data, employing quantization-based
tokenization for continuous lab values and attention mechanisms for
interpretability. The model was pretrained with masked language modeling and
fine-tuned for binary classification tasks predicting progression from stage 3a
to stage 5 across varying follow-up and assessment periods. Evaluated on a
cohort of 91,816 patients, our model consistently outperformed CEHR-BERT,
achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.
These results highlight the effectiveness of transformer architectures and
temporal design choices in clinical prognosis modeling, offering a promising
direction for personalized CKD care.

</details>


### [30] [Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients](https://arxiv.org/abs/2511.02392)
*Muhammad Sheharyar Liaqat*

Main category: cs.AI

TL;DR: 提出了一种基于模糊软集理论的专家系统，通过BMI、胰岛素水平等临床参数评估乳腺癌风险。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌早期诊断对治疗和生存率至关重要，但复杂性和风险因素的多变性使及时检测成为挑战。

Method: 结合BMI、胰岛素水平等参数，使用模糊推理规则和软集计算评估风险。

Result: 系统利用常规血液分析数据，为非侵入性初步评估提供了可行方法。

Conclusion: 该专家系统有助于医疗专业人员识别高风险患者并决定是否需进一步诊断。

Abstract: Breast cancer remains one of the leading causes of mortality among women
worldwide, with early diagnosis being critical for effective treatment and
improved survival rates. However, timely detection continues to be a challenge
due to the complex nature of the disease and variability in patient risk
factors. This study presents a fuzzy soft set theory-based expert system
designed to assess the risk of breast cancer in patients using measurable
clinical and physiological parameters. The proposed system integrates Body Mass
Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input
variables to estimate breast cancer risk through a set of fuzzy inference rules
and soft set computations. These parameters can be obtained from routine blood
analyses, enabling a non-invasive and accessible method for preliminary
assessment. The dataset used for model development and validation was obtained
from the UCI Machine Learning Repository. The proposed expert system aims to
support healthcare professionals in identifying high-risk patients and
determining the necessity of further diagnostic procedures such as biopsies.

</details>


### [31] [A New Perspective on Precision and Recall for Generative Models](https://arxiv.org/abs/2511.02414)
*Benjamin Sykes,Loïc Simon,Julien Rabin,Jalal Fadili*

Main category: cs.AI

TL;DR: 提出了一个基于二元分类的新框架，用于估计生成模型的Precision和Recall（PR）曲线，并分析了其统计特性及在实验中的不同行为。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型在图像和文本领域的成功，评估方法成为研究热点。现有方法多依赖于标量指标，而PR曲线的引入为研究提供了新方向，但其估计仍面临挑战。

Method: 基于二元分类的视角，提出了一个新的框架来估计完整的PR曲线，并进行了统计分析。

Result: 获得了PR估计风险的极小极大上界，扩展了文献中仅适用于曲线极值的PR指标，并通过实验展示了不同设置下曲线的行为差异。

Conclusion: 该框架为生成模型的评估提供了更丰富的分析工具，并展示了PR曲线在不同场景下的多样性和实用性。

Abstract: With the recent success of generative models in image and text, the question
of their evaluation has recently gained a lot of attention. While most methods
from the state of the art rely on scalar metrics, the introduction of Precision
and Recall (PR) for generative model has opened up a new avenue of research.
The associated PR curve allows for a richer analysis, but their estimation
poses several challenges. In this paper, we present a new framework for
estimating entire PR curves based on a binary classification standpoint. We
conduct a thorough statistical analysis of the proposed estimates. As a
byproduct, we obtain a minimax upper bound on the PR estimation risk. We also
show that our framework extends several landmark PR metrics of the literature
which by design are restrained to the extreme values of the curve. Finally, we
study the different behaviors of the curves obtained experimentally in various
settings.

</details>


### [32] [Auditable-choice reframing unlocks RL-based verification for open-ended tasks](https://arxiv.org/abs/2511.02463)
*Mengyu Zhang,Xubo Liu,Siyu Ding,Weichong Yin,Yu Sun,Hua Wu,Wenya Guo,Ying Zhang*

Main category: cs.AI

TL;DR: 该论文探讨了强化学习在开放式任务中的应用，提出了一种名为VMR的新方法，通过将开放式任务转化为可验证的多选题形式，显著提升了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决强化学习只能应用于有标准答案的领域的问题，探索如何将其扩展到开放式任务（如创意写作和指令遵循）中。

Method: 作者提出Verifiable Multiple-Choice Reformulation (VMR)方法，将开放式数据转化为可验证的多选题格式，从而在不依赖显式标准答案的情况下实现有效训练。

Result: 在八个开放式任务基准测试中，VMR方法平均比基线提升了5.99分。

Conclusion: VMR方法成功地将强化学习扩展到开放式任务领域，显著提升了语言模型的性能，展示了推理能力在开放式任务中的潜在价值。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great
potential in enhancing the reasoning capabilities of large language models
(LLMs), achieving remarkable progress in domains such as mathematics and
programming where standard answers are available. However, for open-ended tasks
lacking ground-truth solutions (e.g., creative writing and instruction
following), existing studies typically regard them as non-reasoning scenarios,
thereby overlooking the latent value of reasoning capabilities. This raises a
key question: Can strengthening reasoning improve performance in open-ended
tasks? To address this, we explore the transfer of the RLVR paradigm to the
open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose
the existence of standard answers, it cannot be directly applied to open-ended
tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice
Reformulation (VMR), a novel training strategy that restructures open-ended
data into verifiable multiple-choice formats, enabling effective training even
in the absence of explicit ground truth. Experimental results on multiple
benchmarks validate the effectiveness of our method in improving LLM
performance on open-ended tasks. Notably, across eight open-ended benchmarks,
our VMR-based training delivers an average gain of 5.99 points over the
baseline. Code will be released upon acceptance to facilitate reproducibility.

</details>


### [33] [Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting](https://arxiv.org/abs/2511.02534)
*Enhong Mu,Jinyu Cai,Yijun Lu,Mingyue Zhang,Kenji Tei,Jialong Li*

Main category: cs.AI

TL;DR: 论文提出了一种名为KLPEG的框架，通过知识图谱和LLM结合，提高游戏增量更新测试的精确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现代游戏的快速迭代和频繁更新对测试的效率和针对性提出了挑战，现有基于LLM的自动化测试方法缺乏结构化知识积累机制。

Method: KLPEG框架构建并维护知识图谱以系统建模游戏元素、任务依赖和因果关系，结合LLM解析更新日志，通过多跳推理生成针对更新的测试用例。

Result: 实验表明，KLPEG在Overcooked和Minecraft两个游戏环境中能更准确地定位受更新影响的功能，并以更少的步骤完成测试，显著提高了测试效果和效率。

Conclusion: KLPEG框架通过知识图谱和LLM的结合，为游戏增量更新测试提供了有效的解决方案。

Abstract: The rapid iteration and frequent updates of modern video games pose
significant challenges to the efficiency and specificity of testing. Although
automated playtesting methods based on Large Language Models (LLMs) have shown
promise, they often lack structured knowledge accumulation mechanisms, making
it difficult to conduct precise and efficient testing tailored for incremental
game updates. To address this challenge, this paper proposes a KLPEG framework.
The framework constructs and maintains a Knowledge Graph (KG) to systematically
model game elements, task dependencies, and causal relationships, enabling
knowledge accumulation and reuse across versions. Building on this foundation,
the framework utilizes LLMs to parse natural language update logs, identify the
scope of impact through multi-hop reasoning on the KG, enabling the generation
of update-tailored test cases. Experiments in two representative game
environments, Overcooked and Minecraft, demonstrate that KLPEG can more
accurately locate functionalities affected by updates and complete tests in
fewer steps, significantly improving both playtesting effectiveness and
efficiency.

</details>


### [34] [The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models](https://arxiv.org/abs/2511.02589)
*Claudia Herambourg,Dawid Siuda,Anna Szczepanek,Julia Kopczyńska,Joao R. L. Santos,Wojciech Sas,Joanna Śmietańska-Nowak*

Main category: cs.AI

TL;DR: ORCA Benchmark是一个新颖的多领域、现实生活中的定量推理基准，用于评估大语言模型（LLM）在现实问题中的表现，结果显示现有模型在多个领域的准确率较低，错误主要集中在舍入和计算。


<details>
  <summary>Details</summary>
Motivation: 现有的数学数据集通常只关注单一领域或简单的数学问题，而ORCA Benchmark旨在评估LLM在多领域现实问题中的定量推理能力，填补这一研究空白。

Method: ORCA Benchmark通过500个跨领域的自然语言任务（如金融、物理、健康和统计）来评估LLM，任务输出经过Omni计算引擎验证，重点关注逐步推理、数值精度和领域泛化能力。

Result: 五个最先进的LLM模型（包括ChatGPT-5等）在ORCA Benchmark中的准确率仅为45%至63%，错误主要集中在舍入（35%）和计算错误（33%）。物理学和自然科学的表现较弱，而数学和工程领域较强。

Conclusion: ORCA Benchmark揭示了现有LLM在定量推理任务中的局限性，尤其是在多领域现实问题中的表现不佳，同时也显示了模型之间的部分互补性而非冗余性。

Abstract: We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel
benchmark that evaluates large language models (LLMs) on multi-domain,
real-life quantitative reasoning using verified outputs from Omni's calculator
engine. In 500 natural-language tasks across domains such as finance, physics,
health, and statistics, the five state-of-the-art systems (ChatGPT-5,
Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only
$45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$)
and calculation mistakes ($33\,\%$). Results in specific domains indicate
strengths in mathematics and engineering, but weaknesses in physics and natural
sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the
models often fail together but differ in the types of errors they make,
highlighting their partial complementarity rather than redundancy. Unlike
standard math datasets, ORCA evaluates step-by-step reasoning, numerical
precision, and domain generalization across real problems from finance,
physics, health, and statistics.

</details>


### [35] [Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning](https://arxiv.org/abs/2511.02605)
*Tiberiu-Andrei Georgescu,Alexander W. Goodall,Dalal Alrajeh,Francesco Belardinelli,Sebastian Uchitel*

Main category: cs.AI

TL;DR: 本文提出了一种基于GR(1)规范的动态屏蔽框架，通过在线检测和修复环境假设违反问题，提升了强化学习的安全性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统静态屏蔽方法无法适应环境假设的变化，导致安全性受限。为了解决这一问题，需要开发能够动态调整规范的屏蔽框架。

Method: 使用GR(1)规范和归纳逻辑编程(ILP)在线检测并修复环境假设违反问题，确保屏蔽的动态调整和可解释性。

Result: 实验证明，动态屏蔽框架在Minepump和Atari Seaquest任务中，既能保持逻辑合规性，又能实现接近最优的奖励性能。

Conclusion: 动态屏蔽框架显著优于静态方法，为强化学习的安全性和适应性提供了可靠的解决方案。

Abstract: Shielding is widely used to enforce safety in reinforcement learning (RL),
ensuring that an agent's actions remain compliant with formal specifications.
Classical shielding approaches, however, are often static, in the sense that
they assume fixed logical specifications and hand-crafted abstractions. While
these static shields provide safety under nominal assumptions, they fail to
adapt when environment assumptions are violated. In this paper, we develop the
first adaptive shielding framework - to the best of our knowledge - based on
Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and
expressive fragment of Linear Temporal Logic (LTL) that captures both safety
and liveness properties. Our method detects environment assumption violations
at runtime and employs Inductive Logic Programming (ILP) to automatically
repair GR(1) specifications online, in a systematic and interpretable way. This
ensures that the shield evolves gracefully, ensuring liveness is achievable and
weakening goals only when necessary. We consider two case studies: Minepump and
Atari Seaquest; showing that (i) static symbolic controllers are often severely
suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped
with our adaptive shield maintain near-optimal reward and perfect logical
compliance compared with static shields.

</details>


### [36] [DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning](https://arxiv.org/abs/2511.02627)
*Lachlan McPheat,Navdeep Kaur,Robert Blackwell,Alessandra Russo,Anthony G. Cohn,Pranava Madhyastha*

Main category: cs.AI

TL;DR: DecompSR是一个专为分析组合空间推理能力设计的大规模数据集（超过500万数据点）和生成框架，通过独立控制组合性的多个方面（如推理深度、语言变异等），为LLM的组合推理能力提供了细致评测。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLM）在组合空间推理任务中的能力，尤其是生产性（推理深度）和系统性（新颖语言元素）方面的表现，揭示其局限性。

Method: DecompSR通过程序化生成数据集，确保数据集构造的正确性，并使用符号求解器独立验证。独立调控组合性的多个维度（如生产率、替换性等）以评测LLM。

Result: 实验表明，LLM在生产性和系统性泛化方面表现不佳，但对语言变异更具鲁棒性。

Conclusion: DecompSR为LLM组合推理能力提供了精细且可验证的评测工具，揭示了LLM在组合性任务中的优缺点。

Abstract: We introduce DecompSR, decomposed spatial reasoning, a large benchmark
dataset (over 5m datapoints) and generation framework designed to analyse
compositional spatial reasoning ability. The generation of DecompSR allows
users to independently vary several aspects of compositionality, namely:
productivity (reasoning depth), substitutivity (entity and linguistic
variability), overgeneralisation (input order, distractors) and systematicity
(novel linguistic elements). DecompSR is built procedurally in a manner which
makes it is correct by construction, which is independently verified using a
symbolic solver to guarantee the correctness of the dataset. DecompSR is
comprehensively benchmarked across a host of Large Language Models (LLMs) where
we show that LLMs struggle with productive and systematic generalisation in
spatial reasoning tasks whereas they are more robust to linguistic variation.
DecompSR provides a provably correct and rigorous benchmarking dataset with a
novel ability to independently vary the degrees of several key aspects of
compositionality, allowing for robust and fine-grained probing of the
compositional reasoning abilities of LLMs.

</details>


### [37] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: 论文提出了一种评估异构智能体协作能力的迷宫求解基准，揭示了‘协作差距’现象，并提出了改进协作效率和设计的新方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI的发展，异构智能体系统的协作能力变得至关重要，但目前缺乏大规模的实证研究。论文旨在填补这一空白，通过设计一个基准测试来评估异构智能体的协作效果。

Method: 研究者提出了一个专注于协作能力的迷宫求解基准测试框架，允许调节问题复杂度、支持自动化评分并保持生态合理性。测试了32种主流开源和闭源模型，分别在单独、同质和异构配对下进行评估。

Result: 研究发现存在显著的‘协作差距’：单独表现良好的模型在协作时性能大幅下降。并提出‘接力推理’方法，即由更强的智能体引导后再传递给较弱的智能体，可以有效缩小这一差距。

Conclusion: 论文强调了协作评估的重要性，并提出了改进协作能力和交互设计的建议，这些发现对AI-AI协作和人机协作均具有指导意义。

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [38] [LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer](https://arxiv.org/abs/2511.02759)
*Julius Fiedler,Carsten Knoll,Klaus Röbenack*

Main category: cs.AI

TL;DR: 论文提出了一种利用大型语言模型支持的方法，用于半自动化生成结合人类可读性和机器可解释性的形式化知识表示，并展示了如何将自然语言描述和数学定义转化为知识图谱。


<details>
  <summary>Details</summary>
Motivation: 控制工程领域研究产出的快速增长需要新的方法来结构化和形式化领域知识，以促进知识传递和协作。

Method: 基于Imperative Representation of Knowledge (PyIRK)框架，利用语言模型将自然语言描述和数学定义（以LaTeX源代码形式）转化为形式化的知识图谱。

Result: 提出了一个生成“交互式语义层”的初步应用，用于增强源文档，以提高知识传递的效率。

Conclusion: 这一方法为实现控制工程领域易于访问、协作和验证的知识库提供了初步支持。

Abstract: The rapid growth of research output in control engineering calls for new
approaches to structure and formalize domain knowledge. This paper briefly
describes an LLM-supported method for semi-automated generation of formal
knowledge representations that combine human readability with machine
interpretability and increased expressiveness. Based on the Imperative
Representation of Knowledge (PyIRK) framework, we demonstrate how language
models can assist in transforming natural-language descriptions and
mathematical definitions (available as LaTeX source code) into a formalized
knowledge graph. As a first application we present the generation of an
``interactive semantic layer'' to enhance the source documents in order to
facilitate knowledge transfer. From our perspective this contributes to the
vision of easily accessible, collaborative, and verifiable knowledge bases for
the control engineering domain.

</details>


### [39] [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)
*Chloe Loughridge,Paul Colognese,Avery Griffin,Tyler Tracy,Jon Kutasov,Joe Benton*

Main category: cs.AI

TL;DR: 该论文提出了一种在复杂AI环境中优化攻击策略的方法，通过分解攻击能力并单独优化每个组件，利用概率模型解决数据不足问题，最终显著提升攻击强度。


<details>
  <summary>Details</summary>
Motivation: 随着AI部署的复杂性和风险性增加，评估其风险变得尤为重要。然而，在复杂环境中，由于计算约束导致数据不足，生成强攻击策略具有挑战性。

Method: 研究使用SHADE-Arena数据集，将攻击能力分解为五个组成部分（怀疑建模、攻击选择、计划合成、执行和隐蔽性），并单独优化每个组件。为解决数据不足问题，开发了攻击动态的概率模型，通过模拟优化攻击参数。

Result: 优化后的攻击策略显著提升了攻击强度，将安全评分从基线0.87降至0.41。

Conclusion: 该方法在复杂AI环境中有效提升了攻击策略的效果，为解决数据不足问题提供了新思路。

Abstract: As AI deployments become more complex and high-stakes, it becomes
increasingly important to be able to estimate their risk. AI control is one
framework for doing so. However, good control evaluations require eliciting
strong attack policies. This can be challenging in complex agentic environments
where compute constraints leave us data-poor. In this work, we show how to
optimize attack policies in SHADE-Arena, a dataset of diverse realistic control
environments. We do this by decomposing attack capability into five constituent
skills -- suspicion modeling, attack selection, plan synthesis, execution and
subtlety -- and optimizing each component individually. To get around the
constraint of limited data, we develop a probabilistic model of attack
dynamics, optimize our attack hyperparameters using this simulation, and then
show that the results transfer to SHADE-Arena. This results in a substantial
improvement in attack strength, reducing safety score from a baseline of 0.87
to 0.41 using our scaffold.

</details>
