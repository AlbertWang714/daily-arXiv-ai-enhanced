<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.AI](#cs.AI) [Total: 51]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Preliminary Study of RAG for Taiwanese Historical Archives](https://arxiv.org/abs/2511.07445)
*Claire Lin,Bo-Han Feng,Xuanjun Chen,Te-Lun Yang,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.CL

TL;DR: 本文研究了检索增强生成（RAG）在台湾历史档案中的应用，探讨了查询特性和元数据整合策略对系统性能的影响。结果显示早期元数据整合提升了性能，但也揭示了RAG的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索RAG在台湾历史档案中的潜力，填补相关研究的空白。

Method: 对两个繁体中文历史数据集（热兰遮城和台湾省议会公报）及其查询集应用RAG管道，分析查询特性和元数据整合策略的影响。

Result: 早期元数据整合提高了检索和回答的准确性，但也暴露了生成中的幻觉和处理多跳历史查询的困难。

Conclusion: RAG在台湾历史档案中表现有潜力但仍需解决幻觉和多跳查询等挑战。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.

</details>


### [2] [Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey](https://arxiv.org/abs/2511.07448)
*Fatemeh Shahhosseini,Arash Marioriyad,Ali Momen,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban,Shaghayegh Haghjooy Javanmard*

Main category: cs.CL

TL;DR: 该论文总结了利用大语言模型（LLMs）生成科学想法的方法，分析了如何平衡创造性与科学严谨性，并分类为五种方法家族。通过两个创造力框架评估这些方法，为LLMs在科学发现中的系统应用提供方向。


<details>
  <summary>Details</summary>
Motivation: 科学想法的生成对科学发现至关重要，但LLMs的创造力表现参差不齐且尚未被充分理解，因此需要系统梳理现有方法并将其与创造力理论框架结合，以推动LLMs在科学中的可靠应用。

Method: 论文将现有LLM驱动的科学构想方法分为五类：外部知识增强、基于提示的分布引导、推理时扩展、多智能体协作和参数级适应；并通过Boden的创造力分类学和Rhodes的4Ps框架评估这些方法。

Result: 研究揭示了不同方法在创造力类型和侧重点上的差异（如组合性、探索性或变革性创造力），以及它们在4Ps框架下的不同定位（如过程或产品）。

Conclusion: 论文为LLMs在科学构想中的应用提供了清晰的现状梳理和未来发展方向，为系统性、创新性的科学发现工具设计奠定了基础。

Abstract: Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.

</details>


### [3] [GRIP: In-Parameter Graph Reasoning through Fine-Tuning Large Language Models](https://arxiv.org/abs/2511.07457)
*Jiarui Feng,Donghong Cai,Yixin Chen,Muhan Zhang*

Main category: cs.CL

TL;DR: GRIP框架通过轻量级LoRA参数调整，使大语言模型能够内化图结构信息，无需推理时访问原图，高效完成图相关任务。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在处理序列文本数据上表现出色，但如何让其有效处理图结构数据仍具挑战性，现有方法存在高计算开销或模态对齐不佳的问题。

Method: 提出GRIP框架，通过设计精细的微调任务，让模型通过LoRA参数内化图的复杂关系信息，避免推理时对原图的依赖。

Result: 在多个基准测试中验证了方法的有效性和高效性。

Conclusion: GRIP为大语言模型处理图结构数据提供了一种轻量且高效的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modeling sequential textual data and generalizing across diverse tasks. However, adapting LLMs to effectively handle structural data, such as knowledge graphs or web data, remains a challenging problem. Some approaches adopt complex strategies to convert graphs into text sequences, resulting in significant token overhead and rendering them impractical for large-scale graphs. Others introduce additional modules to encode graphs into fixed-size token representations for LLMs. However, these methods typically require large-scale post-training on graph-text corpus and complex alignment procedures, yet often yield sub-optimal results due to poor modality alignment. Inspired by in-parameter knowledge injection for test-time adaptation of LLMs, we propose GRIP, a novel framework that equips LLMs with the ability to internalize complex relational information from graphs through carefully designed fine-tuning tasks. This knowledge is efficiently stored within lightweight LoRA parameters, enabling the fine-tuned LLM to perform a wide range of graph-related tasks without requiring access to the original graph at inference time. Extensive experiments across multiple benchmarks validate the effectiveness and efficiency of our approach.

</details>


### [4] [LLMs vs. Traditional Sentiment Tools in Psychology: An Evaluation on Belgian-Dutch Narratives](https://arxiv.org/abs/2511.07641)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TL;DR: 该研究比较了三种荷兰语LLM与传统工具LIWC和Pattern在佛兰芒语中的情感价预测能力，结果显示传统方法表现更优，并指出了LLM在情感分析任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为低资源语言变体（如佛兰芒语）开发更准确的情感价预测工具，并验证LLM是否优于传统方法。

Method: 使用三种荷兰语LLM与传统工具LIWC、Pattern对比，测试其在约25000条自发性文本响应中的情感价预测表现。

Result: 荷兰语LLM表现不如传统方法，尤其是Pattern表现最佳，表明LLM在情感分析任务中的优越性存疑。

Conclusion: 结论强调需要开发针对低资源语言变体的文化和语言定制评估框架，同时质疑当前LLM微调方法是否能有效捕捉日常语言中的情感细微差别。

Abstract: Understanding emotional nuances in everyday language is crucial for computational linguistics and emotion research. While traditional lexicon-based tools like LIWC and Pattern have served as foundational instruments, Large Language Models (LLMs) promise enhanced context understanding. We evaluated three Dutch-specific LLMs (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, and GEITje-7B-ultra) against LIWC and Pattern for valence prediction in Flemish, a low-resource language variant. Our dataset comprised approximately 25000 spontaneous textual responses from 102 Dutch-speaking participants, each providing narratives about their current experiences with self-assessed valence ratings (-50 to +50). Surprisingly, despite architectural advancements, the Dutch-tuned LLMs underperformed compared to traditional methods, with Pattern showing superior performance. These findings challenge assumptions about LLM superiority in sentiment analysis tasks and highlight the complexity of capturing emotional valence in spontaneous, real-world narratives. Our results underscore the need for developing culturally and linguistically tailored evaluation frameworks for low-resource language variants, while questioning whether current LLM fine-tuning approaches adequately address the nuanced emotional expressions found in everyday language use.

</details>


### [5] [Stress Testing Factual Consistency Metrics for Long-Document Summarization](https://arxiv.org/abs/2511.07689)
*Zain Muhammad Mujahid,Dustin Wright,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文系统评估了六种广泛使用的无参考事实性指标在长文档摘要中的可靠性，发现现有短文档指标在语义等效摘要和长上下文条件下表现不一致，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 评估抽象文本摘要的事实一致性仍是一个挑战，尤其是在长文档中，传统指标受到输入长度限制和长程依赖的影响。

Method: 通过七种保留事实性的扰动（如改述、简化、同义词替换等）分析六种无参考事实性指标的稳健性，并研究了其对检索上下文和断言信息密度的敏感性。

Result: 现有短文档指标在语义等效摘要上得分不一致，且在信息密度高的断言上可靠性下降，扩展检索上下文在某些领域能提高稳定性。

Conclusion: 研究为改进事实性评估提供了具体方向，包括多跨度推理、上下文感知校准等，并公开了代码和数据以支持研究。

Abstract: Evaluating the factual consistency of abstractive text summarization remains a significant challenge, particularly for long documents, where conventional metrics struggle with input length limitations and long-range dependencies. In this work, we systematically evaluate the reliability of six widely used reference-free factuality metrics, originally proposed for short-form summarization, in the long-document setting. We probe metric robustness through seven factuality-preserving perturbations applied to summaries, namely paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion, and further analyze their sensitivity to retrieval context and claim information density. Across three long-form benchmark datasets spanning science fiction, legal, and scientific domains, our results reveal that existing short-form metrics produce inconsistent scores for semantically equivalent summaries and exhibit declining reliability for information-dense claims whose content is semantically similar to many parts of the source document. While expanding the retrieval context improves stability in some domains, no metric consistently maintains factual alignment under long-context conditions. Finally, our results highlight concrete directions for improving factuality evaluation, including multi-span reasoning, context-aware calibration, and training on meaning-preserving variations to enhance robustness in long-form summarization. We release all code, perturbed data, and scripts required to reproduce our results at https://github.com/zainmujahid/metricEval-longSum.

</details>


### [6] [Design, Results and Industry Implications of the World's First Insurance Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2511.07794)
*Hua Zhou,Bing Ma,Yufei Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 论文详细介绍了CUFEInse v1.0的构建方法、多维评价体系和设计理念，并基于此对11个主流大语言模型进行评估。结果显示通用模型的精算能力和合规适应性较弱，专业模型在垂直场景优势明显但业务适应性和合规性不足。


<details>
  <summary>Details</summary>
Motivation: 填补保险领域专业评估基准的空白，为学术界和行业提供专业、系统、权威的评价工具，并为垂直领域大模型评估范式提供参考。

Method: 采用“量化导向、专家驱动、多轮验证”原则，构建包含5个核心维度、54个子指标和14,430个优质问题的评价框架。

Result: 通用模型在精算能力和合规适应性方面存在不足，专业模型在垂直场景表现优异但业务适应性和合规性有欠缺。

Conclusion: CUFEInse的建立为保险领域大模型评估提供了权威工具，其设计理念和方法对垂直领域评估范式具有重要参考价值，未来发展方向为“领域适应+推理增强”。

Abstract: This paper comprehensively elaborates on the construction methodology, multi-dimensional evaluation system, and underlying design philosophy of CUFEInse v1.0. Adhering to the principles of "quantitative-oriented, expert-driven, and multi-validation," the benchmark establishes an evaluation framework covering 5 core dimensions, 54 sub-indicators, and 14,430 high-quality questions, encompassing insurance theoretical knowledge, industry understanding, safety and compliance, intelligent agent application, and logical rigor. Based on this benchmark, a comprehensive evaluation was conducted on 11 mainstream large language models. The evaluation results reveal that general-purpose models suffer from common bottlenecks such as weak actuarial capabilities and inadequate compliance adaptation. High-quality domain-specific training demonstrates significant advantages in insurance vertical scenarios but exhibits shortcomings in business adaptation and compliance. The evaluation also accurately identifies the common bottlenecks of current large models in professional scenarios such as insurance actuarial, underwriting and claim settlement reasoning, and compliant marketing copywriting. The establishment of CUFEInse not only fills the gap in professional evaluation benchmarks for the insurance field, providing academia and industry with a professional, systematic, and authoritative evaluation tool, but also its construction concept and methodology offer important references for the evaluation paradigm of large models in vertical fields, serving as an authoritative reference for academic model optimization and industrial model selection. Finally, the paper looks forward to the future iteration direction of the evaluation benchmark and the core development direction of "domain adaptation + reasoning enhancement" for insurance large models.

</details>


### [7] [AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys](https://arxiv.org/abs/2511.07871)
*Chenxi Lin,Weikang Yuan,Zhuoren Jiang,Biao Huang,Ruitao Zhang,Jianan Ge,Yueqian Xu,Jianxing Yu*

Main category: cs.CL

TL;DR: AlignSurvey是首个系统性利用大语言模型（LLMs）复制和评估完整社会调查流程的基准，涵盖四个关键任务，并提供了评估指标和多样化的数据集。


<details>
  <summary>Details</summary>
Motivation: 传统调查方法存在固定问题格式、高成本、适应性和跨文化等效性等问题，而现有LLM研究多局限于结构化问题且缺乏多样性。AlignSurvey旨在填补这一空白。

Method: AlignSurvey定义了四个与社会调查阶段对齐的任务，提供任务特定评估指标，并构建了多层次数据集架构，包括社交基础语料库和全流程调查数据集。

Result: 发布了SurveyLM模型家族和参考模型，支持领域特定对齐评估，所有数据、模型和工具均已公开。

Conclusion: AlignSurvey为透明和社会责任研究提供了全面的工具和资源，推动了LLM在社会调查中的应用。

Abstract: Understanding human attitudes, preferences, and behaviors through social surveys is essential for academic research and policymaking. Yet traditional surveys face persistent challenges, including fixed-question formats, high costs, limited adaptability, and difficulties ensuring cross-cultural equivalence. While recent studies explore large language models (LLMs) to simulate survey responses, most are limited to structured questions, overlook the entire survey process, and risks under-representing marginalized groups due to training data biases. We introduce AlignSurvey, the first benchmark that systematically replicates and evaluates the full social survey pipeline using LLMs. It defines four tasks aligned with key survey stages: social role modeling, semi-structured interview modeling, attitude stance modeling and survey response modeling. It also provides task-specific evaluation metrics to assess alignment fidelity, consistency, and fairness at both individual and group levels, with a focus on demographic diversity. To support AlignSurvey, we construct a multi-tiered dataset architecture: (i) the Social Foundation Corpus, a cross-national resource with 44K+ interview dialogues and 400K+ structured survey records; and (ii) a suite of Entire-Pipeline Survey Datasets, including the expert-annotated AlignSurvey-Expert (ASE) and two nationally representative surveys for cross-cultural evaluation. We release the SurveyLM family, obtained through two-stage fine-tuning of open-source LLMs, and offer reference models for evaluating domain-specific alignment. All datasets, models, and tools are available at github and huggingface to support transparent and socially responsible research.

</details>


### [8] [Planned Event Forecasting using Future Mentions and Related Entity Extraction in News Articles](https://arxiv.org/abs/2511.07879)
*Neelesh Kumar Shukla,Pranay Sanghvi*

Main category: cs.CL

TL;DR: 该论文通过主题建模和word2vec筛选相关新闻，利用NER识别实体，并提出相关实体提取方法，预测社会动荡事件。


<details>
  <summary>Details</summary>
Motivation: 在印度等民主国家，公众表达诉求可能导致社会动荡事件，如抗议活动。这些事件往往未经事先批准，预测它们有助于行政部门采取必要措施。

Method: 使用主题建模和word2vec筛选新闻文章，通过NER识别实体（如人物、组织、地点、日期），并进行时间标准化。提出相关实体提取方法（Related Entity Extraction）。

Result: 开发了一个地理独立的通用模型，能够识别关键特征以过滤社会动荡事件，并成功提取相关实体。

Conclusion: 该研究为预测社会动荡事件提供了有效工具，有助于行政官员提前采取行动。

Abstract: In democracies like India, people are free to express their views and demands. Sometimes this causes situations of civil unrest such as protests, rallies, and marches. These events may be disruptive in nature and are often held without prior permission from the competent authority. Forecasting these events helps administrative officials take necessary action. Usually, protests are announced well in advance to encourage large participation. Therefore, by analyzing such announcements in news articles, planned events can be forecasted beforehand. We developed such a system in this paper to forecast social unrest events using topic modeling and word2vec to filter relevant news articles, and Named Entity Recognition (NER) methods to identify entities such as people, organizations, locations, and dates. Time normalization is applied to convert future date mentions into a standard format. In this paper, we have developed a geographically independent, generalized model to identify key features for filtering civil unrest events. There could be many mentions of entities, but only a few may actually be involved in the event. This paper calls such entities Related Entities and proposes a method to extract them, referred to as Related Entity Extraction.

</details>


### [9] [Breaking the Adversarial Robustness-Performance Trade-off in Text Classification via Manifold Purification](https://arxiv.org/abs/2511.07888)
*Chenhao Dang,Jing Ma*

Main category: cs.CL

TL;DR: MC^2F是一种结合了分层黎曼连续规范化流和测地纯化解算器的两模块系统，旨在通过建模干净样本在嵌入流形中的分布来提升文本分类模型对抗攻击的鲁棒性，同时保持甚至提升干净数据的性能。


<details>
  <summary>Details</summary>
Motivation: 解决文本分类中模型在增强对抗攻击鲁棒性时通常会降低干净数据性能的问题，通过建模干净样本的嵌入流形分布来实现两者兼顾。

Method: 提出MC^2F，包括分层黎曼连续规范化流（SR-CNF）学习干净数据流形密度，和测地纯化解算器将对抗点通过最短路径投影回学习到的流形上。

Result: 在三个数据集和多种对抗攻击下的实验显示，MC^2F不仅达到了对抗鲁棒性的新最佳水平，还完全保留了干净数据的性能，甚至略有提升。

Conclusion: MC^2F通过建模和修正嵌入流形，有效平衡了对抗鲁棒性和干净数据性能，为文本分类提供了新的解决方案。

Abstract: A persistent challenge in text classification (TC) is that enhancing model robustness against adversarial attacks typically degrades performance on clean data. We argue that this challenge can be resolved by modeling the distribution of clean samples in the encoder embedding manifold. To this end, we propose the Manifold-Correcting Causal Flow (MC^2F), a two-module system that operates directly on sentence embeddings. A Stratified Riemannian Continuous Normalizing Flow (SR-CNF) learns the density of the clean data manifold. It identifies out-of-distribution embeddings, which are then corrected by a Geodesic Purification Solver. This solver projects adversarial points back onto the learned manifold via the shortest path, restoring a clean, semantically coherent representation. We conducted extensive evaluations on text classification (TC) across three datasets and multiple adversarial attacks. The results demonstrate that our method, MC^2F, not only establishes a new state-of-the-art in adversarial robustness but also fully preserves performance on clean data, even yielding modest gains in accuracy.

</details>


### [10] [Last Layer Logits to Logic: Empowering LLMs with Logic-Consistent Structured Knowledge Reasoning](https://arxiv.org/abs/2511.07910)
*Songze Li,Zhiqiang Liu,Zhaoyan Gong,Xiaoke Guo,Zhengke Gui,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 论文提出Logits-to-Logic框架，通过加强和过滤LLM输出的logits来解决其在结构化知识推理中的逻辑一致性不足问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)在自然语言推理任务中表现出色，但其在结构化知识推理中因表示差异导致逻辑不一致，称为Logic Drift。现有方法仅提供输入级指导，无法从根本上解决问题。

Method: 提出Logits-to-Logic框架，包含logits加强和logits过滤模块，以纠正LLM输出的逻辑缺陷。

Result: 实验表明，该方法显著提高了LLMs在结构化知识推理中的逻辑一致性，并在多个KGQA基准测试中达到最先进水平。

Conclusion: Logits-to-Logic框架有效解决了LLM在结构化知识推理中的Logic Drift问题，为相关任务提供了灵活且高效的解决方案。

Abstract: Large Language Models (LLMs) achieve excellent performance in natural language reasoning tasks through pre-training on vast unstructured text, enabling them to understand the logic in natural language and generate logic-consistent responses. However, the representational differences between unstructured and structured knowledge make LLMs inherently struggle to maintain logic consistency, leading to \textit{Logic Drift} challenges in structured knowledge reasoning tasks such as Knowledge Graph Question Answering (KGQA). Existing methods address this limitation by designing complex workflows embedded in prompts to guide LLM reasoning. Nevertheless, these approaches only provide input-level guidance and fail to fundamentally address the \textit{Logic Drift} in LLM outputs. Additionally, their inflexible reasoning workflows cannot adapt to different tasks and knowledge graphs. To enhance LLMs' logic consistency in structured knowledge reasoning, we specifically target the logits output from the autoregressive generation process. We propose the \textit{Logits-to-Logic} framework, which incorporates logits strengthening and logits filtering as core modules to correct logical defects in LLM outputs. Extensive experiments show that our approach significantly improves LLMs' logic consistency in structured knowledge reasoning and achieves state-of-the-art performance on multiple KGQA benchmarks.

</details>


### [11] [Social Media for Mental Health: Data, Methods, and Findings](https://arxiv.org/abs/2511.07914)
*Nur Shazwani Kamarudin,Ghazaleh Beigi,Lydia Manikonda,Huan Liu*

Main category: cs.CL

TL;DR: 本文探讨了社交媒体数据在心理健康问题研究中的应用，总结了当前的方法和发现，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 社交媒体为研究心理健康问题提供了新数据来源，帮助解决因高污名化而难以公开讨论的抑郁、焦虑等问题。

Method: 文章采用语言学、视觉和情绪指标分析用户披露内容，结合机器学习、特征工程和自然语言处理技术。

Result: 研究发现社交媒体数据可为医疗实践、及时支持和政策制定提供新思路。

Conclusion: 本章展示了社交媒体数据在心理健康领域的潜力，并提出了未来研究方向。

Abstract: There is an increasing number of virtual communities and forums available on the web. With social media, people can freely communicate and share their thoughts, ask personal questions, and seek peer-support, especially those with conditions that are highly stigmatized, without revealing personal identity. We study the state-of-the-art research methodologies and findings on mental health challenges like de- pression, anxiety, suicidal thoughts, from the pervasive use of social media data. We also discuss how these novel thinking and approaches can help to raise awareness of mental health issues in an unprecedented way. Specifically, this chapter describes linguistic, visual, and emotional indicators expressed in user disclosures. The main goal of this chapter is to show how this new source of data can be tapped to improve medical practice, provide timely support, and influence government or policymakers. In the context of social media for mental health issues, this chapter categorizes social media data used, introduces different deployed machine learning, feature engineering, natural language processing, and surveys methods and outlines directions for future research.

</details>


### [12] [Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker](https://arxiv.org/abs/2511.07969)
*Matthias De Lange,Jens-Joris Decorte,Jeroen Van Hautte*

Main category: cs.CL

TL;DR: WorkBench是一个统一评估套件，用于评估工作相关任务中的通用嵌入模型性能。研究发现跨任务转移显著，并提出了一种任务无关的双编码器UWE，其在工作领域中表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于工作相关任务的复杂性和数据稀缺性，研究通用嵌入模型在工作领域的性能具有重要意义。

Method: 引入WorkBench作为统一评估套件，提出UWE（任务无关双编码器），利用多对多InfoNCE目标和软延迟交互。

Result: UWE在未见目标任务空间中表现优异，支持低延迟推理，并在性能上显著优于通用嵌入模型。

Conclusion: UWE为工作领域提供了一种高效且通用的嵌入模型解决方案。

Abstract: Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models.

</details>


### [13] [NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation](https://arxiv.org/abs/2511.07982)
*Maoqi Liu,Quan Fang,Yuhao Wu,Can Zhao,Yang Yang,Kaiquan Cai*

Main category: cs.CL

TL;DR: NOTAM-Evolve 是一种自我进化的框架，利用增强的知识图谱检索模块和闭环学习过程，显著提升了NOTAM解析的准确性，比基础LLM提升了30.4%。


<details>
  <summary>Details</summary>
Motivation: NOTAMs的解析对航空安全至关重要，但现有的自动化系统只能进行浅层解析，缺乏可操作的信息提取能力。

Method: 提出NOTAM-Evolve框架，结合动态知识基础和模式推理，通过闭环学习使LLM自主掌握复杂NOTAM解析。

Result: 实验表明，NOTAM-Evolve比基础LLM准确率提升了30.4%，在结构化NOTAM解析任务上达到最新水平。

Conclusion: NOTAM-Evolve通过自主学习和知识增强，显著提升了NOTAM解析的准确性和实用性，为航空安全提供了有力支持。

Abstract: Accurate interpretation of Notices to Airmen (NOTAMs) is critical for aviation safety, yet their condensed and cryptic language poses significant challenges to both manual and automated processing. Existing automated systems are typically limited to shallow parsing, failing to extract the actionable intelligence needed for operational decisions. We formalize the complete interpretation task as deep parsing, a dual-reasoning challenge requiring both dynamic knowledge grounding (linking the NOTAM to evolving real-world aeronautical data) and schema-based inference (applying static domain rules to deduce operational status). To tackle this challenge, we propose NOTAM-Evolve, a self-evolving framework that enables a large language model (LLM) to autonomously master complex NOTAM interpretation. Leveraging a knowledge graph-enhanced retrieval module for data grounding, the framework introduces a closed-loop learning process where the LLM progressively improves from its own outputs, minimizing the need for extensive human-annotated reasoning traces. In conjunction with this framework, we introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Our experiments demonstrate that NOTAM-Evolve achieves a 30.4% absolute accuracy improvement over the base LLM, establishing a new state of the art on the task of structured NOTAM interpretation.

</details>


### [14] [State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?](https://arxiv.org/abs/2511.07989)
*Taja Kuzman Pungeršek,Peter Rupnik,Ivan Porupski,Vuk Dinić,Nikola Ljubešić*

Main category: cs.CL

TL;DR: 论文评估了大语言模型（LLMs）在南斯拉夫语言文本分类任务中的表现，发现其零样本性能优于或持平微调的BERT类模型，但在输出稳定性、推理速度和计算成本方面存在不足，BERT类模型仍是大规模标注的更实用选择。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在资源较少语言（如南斯拉夫语系）中的文本分类性能是否与主流语言（如英语）相当，并比较其与BERT类模型的优劣势。

Method: 选择了三种任务（情感分类、主题分类、体裁识别），在多种领域（新闻、议会演讲、网页文本）中，对比了开源的微调BERT类模型与LLMs的零样本表现。

Result: LLMs在零样本设置下性能突出，尤其在南斯拉夫语言中表现与英语相当；但其输出不稳定，推理速度慢，计算成本高。

Conclusion: 尽管LLMs在零样本任务中表现优秀，但BERT类模型在大规模文本标注中仍更具实用性。

Abstract: Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.

</details>


### [15] [Self-Correction Distillation for Structured Data Question Answering](https://arxiv.org/abs/2511.07998)
*Yushan Zhu,Wen Zhang,Long Jin,Mengshu Sun,Ling Zhong,Zhiqiang Liu,Juan Li,Lei Liang,Chong Long,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 提出了一种自校正蒸馏（SCD）方法，通过错误提示机制（EPM）和两阶段蒸馏策略，提升小规模LLMs在结构化数据问答中的表现。


<details>
  <summary>Details</summary>
Motivation: 针对小规模LLMs在生成结构化查询时易出错的问题，研究如何通过蒸馏方法提升其性能。

Method: 设计了错误提示机制（EPM）检测错误并提供定制化信息，采用两阶段蒸馏策略将大规模LLMs的能力迁移至小规模LLMs。

Result: 实验表明SCD在小规模LLMs上表现最佳，部分数据集接近GPT4，且EPM加持的大规模LLMs在多数数据集上超越当前最优结果。

Conclusion: SCD方法有效提升了小规模LLMs的结构化数据问答能力，同时EPM进一步优化了大规模LLMs的性能。

Abstract: Structured data question answering (QA), including table QA, Knowledge Graph (KG) QA, and temporal KG QA, is a pivotal research area. Advances in large language models (LLMs) have driven significant progress in unified structural QA frameworks like TrustUQA. However, these frameworks face challenges when applied to small-scale LLMs since small-scale LLMs are prone to errors in generating structured queries. To improve the structured data QA ability of small-scale LLMs, we propose a self-correction distillation (SCD) method. In SCD, an error prompt mechanism (EPM) is designed to detect errors and provide customized error messages during inference, and a two-stage distillation strategy is designed to transfer large-scale LLMs' query-generation and error-correction capabilities to small-scale LLM. Experiments across 5 benchmarks with 3 structured data types demonstrate that our SCD achieves the best performance and superior generalization on small-scale LLM (8B) compared to other distillation methods, and closely approaches the performance of GPT4 on some datasets. Furthermore, large-scale LLMs equipped with EPM surpass the state-of-the-art results on most datasets.

</details>


### [16] [BARD10: A New Benchmark Reveals Significance of Bangla Stop-Words in Authorship Attribution](https://arxiv.org/abs/2511.08085)
*Abdullah Muhammad Moosa,Nusrat Sultana,Mahdi Muhammad Moosa,Md. Miraiz Hossain*

Main category: cs.CL

TL;DR: 该研究全面调查了孟加拉语作者归属问题，引入了新的平衡基准语料库BARD10，并系统分析了停用词去除对经典和深度学习模型的影响，揭示了停用词在孟加拉语中的风格意义。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过创建BARD10语料库，评估停用词去除对作者归属任务的影响，并探索孟加拉语停用词在风格识别中的作用。

Method: 研究使用BARD10和BAAD16语料库，评估了SVM、Bangla BERT、XGBoost和MLP四种模型，并对两者进行了统一的预处理。

Result: TF-IDF + SVM在BAAD16和BARD10上表现最佳，而Bangla BERT表现较差；停用词去除对BARD10作者敏感，但对BAAD16作者影响较小。

Conclusion: 研究揭示了孟加拉语停用词的风格重要性，提出了适用于短文本的精细机器学习模型，并为未来长上下文或领域适应型变换模型提供了基准。

Abstract: This research presents a comprehensive investigation into Bangla authorship attribution, introducing a new balanced benchmark corpus BARD10 (Bangla Authorship Recognition Dataset of 10 authors) and systematically analyzing the impact of stop-word removal across classical and deep learning models to uncover the stylistic significance of Bangla stop-words. BARD10 is a curated corpus of Bangla blog and opinion prose from ten contemporary authors, alongside the methodical assessment of four representative classifiers: SVM (Support Vector Machine), Bangla BERT (Bidirectional Encoder Representations from Transformers), XGBoost, and a MLP (Multilayer Perception), utilizing uniform preprocessing on both BARD10 and the benchmark corpora BAAD16 (Bangla Authorship Attribution Dataset of 16 authors). In all datasets, the classical TF-IDF + SVM baseline outperformed, attaining a macro-F1 score of 0.997 on BAAD16 and 0.921 on BARD10, while Bangla BERT lagged by as much as five points. This study reveals that BARD10 authors are highly sensitive to stop-word pruning, while BAAD16 authors remain comparatively robust highlighting genre-dependent reliance on stop-word signatures. Error analysis revealed that high frequency components transmit authorial signatures that are diminished or reduced by transformer models. Three insights are identified: Bangla stop-words serve as essential stylistic indicators; finely calibrated ML models prove effective within short-text limitations; and BARD10 connects formal literature with contemporary web dialogue, offering a reproducible benchmark for future long-context or domain-adapted transformers.

</details>


### [17] [Estranged Predictions: Measuring Semantic Category Disruption with Masked Language Modelling](https://arxiv.org/abs/2511.08109)
*Yuxuan Liu,Haim Dubossarsky,Ruth Ahnert*

Main category: cs.CL

TL;DR: 该研究通过掩码语言建模（MLM）分析科幻小说如何打破人类、动物和机器的本体论界限，发现科幻小说在概念渗透性上显著高于普通小说，尤其是机器类别的边界更模糊。


<details>
  <summary>Details</summary>
Motivation: 探讨科幻小说如何通过语言模型量化其‘陌生化’效应，揭示其对传统本体论类别的颠覆作用。

Method: 利用RoBERTa生成掩码词替代，并通过Gemini分类，比较科幻小说（Gollancz SF Masterworks）和普通小说（NovelTM）在概念渗透性上的差异。

Result: 科幻小说中机器类别的概念渗透性最高，人类类别则语义更稳定，表明科幻小说对以人类为中心的逻辑进行了特定重构。

Conclusion: MLM可作为揭示科幻小说‘陌生化’语言特征的工具，为计算文学研究提供了新方法。

Abstract: This paper examines how science fiction destabilises ontological categories by measuring conceptual permeability across the terms human, animal, and machine using masked language modelling (MLM). Drawing on corpora of science fiction (Gollancz SF Masterworks) and general fiction (NovelTM), we operationalise Darko Suvin's theory of estrangement as computationally measurable deviation in token prediction, using RoBERTa to generate lexical substitutes for masked referents and classifying them via Gemini. We quantify conceptual slippage through three metrics: retention rate, replacement rate, and entropy, mapping the stability or disruption of category boundaries across genres. Our findings reveal that science fiction exhibits heightened conceptual permeability, particularly around machine referents, which show significant cross-category substitution and dispersion. Human terms, by contrast, maintain semantic coherence and often anchor substitutional hierarchies. These patterns suggest a genre-specific restructuring within anthropocentric logics. We argue that estrangement in science fiction operates as a controlled perturbation of semantic norms, detectable through probabilistic modelling, and that MLMs, when used critically, serve as interpretive instruments capable of surfacing genre-conditioned ontological assumptions. This study contributes to the methodological repertoire of computational literary studies and offers new insights into the linguistic infrastructure of science fiction.

</details>


### [18] [Multimodal LLMs Do Not Compose Skills Optimally Across Modalities](https://arxiv.org/abs/2511.08113)
*Paula Ontalvilla,Aitor Ormazabal,Gorka Azkune*

Main category: cs.CL

TL;DR: 研究探讨了多模态大语言模型（MLLMs）跨模态技能组合的能力，设计了三个评估任务，并发现所有评估模型均存在显著的技能组合差距。通过链式思维提示和微调策略部分改善性能，但差距仍然显著。


<details>
  <summary>Details</summary>
Motivation: 研究MLLMs在跨模态任务中组合已学习技能的能力，以解决新任务。

Method: 设计三个任务评估MLLMs的跨模态技能组合能力，采用直接提示和两步级联推理两种设置。探索链式思维提示和特定微调策略以缓解组合差距。

Result: 所有评估的MLLMs均表现显著的跨模态技能组合差距，改进策略部分提升了性能，但差距仍明显。

Conclusion: 现有MLLMs在跨模态技能组合方面仍有不足，需进一步研究以提升能力。

Abstract: Skill composition is the ability to combine previously learned skills to solve new tasks. As neural networks acquire increasingly complex skills during their pretraining, it is not clear how successfully they can compose them. In this paper, we focus on Multimodal Large Language Models (MLLM), and study their ability to compose skills across modalities. To this end, we design three evaluation tasks which can be solved sequentially composing two modality-dependent skills, and evaluate several open MLLMs under two main settings: i) prompting the model to directly solve the task, and ii) using a two-step cascaded inference approach, which manually enforces the composition of the two skills for a given task. Even with these straightforward compositions, we find that all evaluated MLLMs exhibit a significant cross-modality skill composition gap. To mitigate the aforementioned gap, we explore two alternatives: i) use chain-of-thought prompting to explicitly instruct MLLMs for skill composition and ii) a specific fine-tuning recipe to promote skill composition. Although those strategies improve model performance, they still exhibit significant skill composition gaps, suggesting that more research is needed to improve cross-modal skill composition in MLLMs.

</details>


### [19] [Sentence-Anchored Gist Compression for Long-Context LLMs](https://arxiv.org/abs/2511.08128)
*Dmitrii Tarasov,Elizaveta Goncharova,Kuznetsov Andrey*

Main category: cs.CL

TL;DR: 该研究通过学习的压缩令牌降低了处理长序列的内存和计算需求，实现了对LLMs上下文的2至8倍压缩且性能无明显下降。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型处理长序列时的高内存和计算需求，研究提出了上下文压缩方法以减少资源消耗。

Method: 提出使用学习的压缩令牌，对预训练的大语言模型进行微调，实现上下文的高效压缩。

Result: 在3B参数的LLaMA模型上，该方法取得了与现有压缩技术相当的成果，同时实现了更高的压缩比。

Conclusion: 该方法有效地减少了资源需求，为大语言模型处理长序列提供了一种高效的解决方案。

Abstract: This work investigates context compression for Large Language Models (LLMs) using learned compression tokens to reduce the memory and computational demands of processing long sequences. We demonstrate that pre-trained LLMs can be fine-tuned to compress their context by factors of 2x to 8x without significant performance degradation, as evaluated on both short-context and long-context benchmarks. Furthermore, in experiments on a 3-billion-parameter LLaMA model, our method achieves results on par with alternative compression techniques while attaining higher compression ratios.

</details>


### [20] [On the Interplay between Positional Encodings, Morphological Complexity, and Word Order Flexibility](https://arxiv.org/abs/2511.08139)
*Kushal Tatariya,Wessel Poelman,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 论文探讨了语言模型架构（特别是位置编码）对非英语语言性能的影响，研究发现语言形态复杂性和词序灵活性与位置编码选择之间的关系不明显。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证是否针对英语设计的语言模型架构（如位置编码）对其他结构不同语言的性能有负面影响，尤其是基于形态复杂性与词序灵活性之间的权衡假设。

Method: 通过预训练七种类型多样语言的单语模型（分别使用绝对、相对和无位置编码），并在四个下游任务上评估其性能，研究位置编码与语言特性之间的关系。

Result: 研究发现，位置编码的选择与语言的形态复杂性或词序灵活性之间无明显交互作用，结果受任务、语言和指标选择的影响较大。

Conclusion: 结论表明，以往关于位置编码与语言特性关系的结论可能不够稳定，需要更细致的任务和语言选择来验证。

Abstract: Language model architectures are predominantly first created for English and subsequently applied to other languages. It is an open question whether this architectural bias leads to degraded performance for languages that are structurally different from English. We examine one specific architectural choice: positional encodings, through the lens of the trade-off hypothesis: the supposed interplay between morphological complexity and word order flexibility. This hypothesis posits a trade-off between the two: a more morphologically complex language can have a more flexible word order, and vice-versa. Positional encodings are a direct target to investigate the implications of this hypothesis in relation to language modelling. We pretrain monolingual model variants with absolute, relative, and no positional encodings for seven typologically diverse languages and evaluate them on four downstream tasks. Contrary to previous findings, we do not observe a clear interaction between position encodings and morphological complexity or word order flexibility, as measured by various proxies. Our results show that the choice of tasks, languages, and metrics are essential for drawing stable conclusions

</details>


### [21] [Relation as a Prior: A Novel Paradigm for LLM-based Document-level Relation Extraction](https://arxiv.org/abs/2511.08143)
*Qiankun Pi,Yepeng Sun,Jicang Lu,Qinlong Fan,Ningbo Huang,Shiyu Wang*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）在文档理解中表现突出，但在文档级关系抽取（DocRE）中存在性能差距。论文提出一种新范式RelPrior，通过关系作为先验过滤无关实体对并避免预设关系标签的误判，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在文档级关系抽取中因无关实体对和预设关系标签限制导致的性能问题。

Method: 提出RelPrior范式，利用二元关系先验过滤无关实体对，并通过匹配预设关系提取三元组。

Result: 在多个基准测试中，RelPrior的表现优于现有LLM方法。

Conclusion: RelPrior通过关系先验优化了文档级关系抽取的性能，为LLM方法提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated their remarkable capabilities in document understanding. However, recent research reveals that LLMs still exhibit performance gaps in Document-level Relation Extraction (DocRE) as requiring fine-grained comprehension. The commonly adopted "extract entities then predict relations" paradigm in LLM-based methods leads to these gaps due to two main reasons: (1) Numerous unrelated entity pairs introduce noise and interfere with the relation prediction for truly related entity pairs. (2) Although LLMs have identified semantic associations between entities, relation labels beyond the predefined set are still treated as prediction errors. To address these challenges, we propose a novel Relation as a Prior (RelPrior) paradigm for LLM-based DocRE. For challenge (1), RelPrior utilizes binary relation as a prior to extract and determine whether two entities are correlated, thereby filtering out irrelevant entity pairs and reducing prediction noise. For challenge (2), RelPrior utilizes predefined relation as a prior to match entities for triples extraction instead of directly predicting relation. Thus, it avoids misjudgment caused by strict predefined relation labeling. Extensive experiments on two benchmarks demonstrate that RelPrior achieves state-of-the-art performance, surpassing existing LLM-based methods.

</details>


### [22] [Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models on the Poetry-to-Prose Conversion Task?](https://arxiv.org/abs/2511.08145)
*Kunal Kingkar Das,Manoj Balaji Jagadeeshan,Nallani Chakravartula Sahith,Jivnesh Sandhan,Pawan Goyal*

Main category: cs.CL

TL;DR: 该论文比较了指令微调和大规模语言模型（LLM）与小规模任务特定模型在梵语诗歌到散文转换任务上的表现，发现任务特定模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 验证大规模语言模型（LLMs）是否适用于形态丰富的低资源语言（如梵语）的复杂任务。

Method: 通过对通用模型进行指令微调和上下文提示设计，并完全微调一个ByT5-Sanskrit Seq2Seq模型进行任务特定建模。

Result: 任务特定的ByT5-Sanskrit模型表现优于所有LLM方法，且提示策略在缺乏领域数据时提供了替代方案。

Conclusion: 在复杂且低资源的语言任务中，任务特定的模型优于通用LLM，同时提示策略为数据不足时提供了可行性。

Abstract: Large Language Models (LLMs) are increasingly treated as universal, general-purpose solutions across NLP tasks, particularly in English. But does this assumption hold for low-resource, morphologically rich languages such as Sanskrit? We address this question by comparing instruction-tuned and in-context-prompted LLMs with smaller task-specific encoder-decoder models on the Sanskrit poetry-to-prose conversion task. This task is intrinsically challenging: Sanskrit verse exhibits free word order combined with rigid metrical constraints, and its conversion to canonical prose (anvaya) requires multi-step reasoning involving compound segmentation, dependency resolution, and syntactic linearisation. This makes it an ideal testbed to evaluate whether LLMs can surpass specialised models. For LLMs, we apply instruction fine-tuning on general-purpose models and design in-context learning templates grounded in Paninian grammar and classical commentary heuristics. For task-specific modelling, we fully fine-tune a ByT5-Sanskrit Seq2Seq model. Our experiments show that domain-specific fine-tuning of ByT5-Sanskrit significantly outperforms all instruction-driven LLM approaches. Human evaluation strongly corroborates this result, with scores exhibiting high correlation with Kendall's Tau scores. Additionally, our prompting strategies provide an alternative to fine-tuning when domain-specific verse corpora are unavailable, and the task-specific Seq2Seq model demonstrates robust generalisation on out-of-domain evaluations.

</details>


### [23] [Do Syntactic Categories Help in Developmentally Motivated Curriculum Learning for Language Models?](https://arxiv.org/abs/2511.08199)
*Arzu Burcu Güven,Anna Rogers,Rob van der Goot*

Main category: cs.CL

TL;DR: 研究发现CHILDES语料库中年龄组间的句法差异不大，但训练数据的句法知识有助于解释模型在语言任务中的表现。课程学习中，探索了几种认知启发的课程方法，发现部分课程对阅读任务有帮助，但性能提升主要来自使用可句法分类的数据子集，而非整个嘈杂语料库。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过分析CHILDES语料库的句法特性和年龄组差异，探讨如何利用这些信息改进模型在语言任务中的表现，尤其是通过课程学习的方法。

Method: 分析了CHILDES语料库的句法特性，并进行不同年龄组的比较；探索了包括发展性课程在内的几种认知启发的课程学习方法；对比了使用完整语料库和可句法分类数据子集的模型性能。

Result: CHILDES语料库中年龄组间的句法差异不明显；句法知识有助于理解模型表现；部分课程学习方法对阅读任务有帮助，但主要性能提升来自可句法分类的数据子集。

Conclusion: 研究表明句法分类的数据子集而非整个语料库对模型性能更有帮助，同时课程学习方法在特定任务中具有潜力。

Abstract: We examine the syntactic properties of BabyLM corpus, and age-groups within CHILDES. While we find that CHILDES does not exhibit strong syntactic differentiation by age, we show that the syntactic knowledge about the training data can be helpful in interpreting model performance on linguistic tasks. For curriculum learning, we explore developmental and several alternative cognitively inspired curriculum approaches. We find that some curricula help with reading tasks, but the main performance improvement come from using the subset of syntactically categorizable data, rather than the full noisy corpus.

</details>


### [24] [Encoder Fine-tuning with Stochastic Sampling Outperforms Open-weight GPT in Astronomy Knowledge Extraction](https://arxiv.org/abs/2511.08204)
*Shivam Rawat,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 提出了一种基于编码器的系统，用于从天文学文章中提取知识，通过多任务Transformer模型显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 科学文献数量迅速增长，自动化提取关键实体和上下文信息变得尤为重要。

Method: 采用基于SciBERT的多任务Transformer系统，通过随机采样训练数据并进行多数投票优化。

Result: 系统虽然简单且低成本，但性能显著优于开放的GPT基线。

Conclusion: 该方法有效提升了从天文学文献中提取信息的准确性和效率。

Abstract: Scientific literature in astronomy is rapidly expanding, making it increasingly important to automate the extraction of key entities and contextual information from research papers. In this paper, we present an encoder-based system for extracting knowledge from astronomy articles. Our objective is to develop models capable of classifying telescope references, detecting auxiliary semantic attributes, and recognizing instrument mentions from textual content. To this end, we implement a multi-task transformer-based system built upon the SciBERT model and fine-tuned for astronomy corpora classification. To carry out the fine-tuning, we stochastically sample segments from the training data and use majority voting over the test segments at inference time. Our system, despite its simplicity and low-cost implementation, significantly outperforms the open-weight GPT baseline.

</details>


### [25] [Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias in Feedback](https://arxiv.org/abs/2511.08225)
*Yishan Du,Conrad Borchers,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文提出了一种基于嵌入的基准测试框架，用于检测大型语言模型（LLMs）在形成性反馈中的性别偏见。通过600篇真实学生论文和对照组实验，发现所有模型均对隐性性别线索表现出不对称语义响应，部分模型对显性性别线索敏感。


<details>
  <summary>Details</summary>
Motivation: 随着教师越来越多地将生成式AI（GenAI）应用于教育实践，需要可靠的方法评估LLMs在教学中的公平性。本文旨在开发一种框架，检测LLMs在学生反馈中的性别偏见。

Method: 使用600篇真实学生论文（AES 2.0语料库），构建了两类性别维度对照组（隐性和显性性别线索），评估了6种代表性LLMs的反应差异。通过余弦距离、欧氏距离和排列测试量化差异，并降维可视化结果。

Result: 所有模型在隐性性别替换中表现出不对称语义偏移（男性→女性比女性→男性更显著），仅GPT和Llama模型对显性性别线索敏感。定性分析还反馈了语言风格的差异（如男性导向反馈更支持自主性）。

Conclusion: 即使是前沿的LLMs也表现出对性别替换的语义偏见，提示其反馈中存在性别不平等。文章提出了公平性审计标准和提示设计建议，以保障教育AI的公平性。

Abstract: As teachers increasingly turn to GenAI in their educational practice, we need robust methods to benchmark large language models (LLMs) for pedagogical purposes. This article presents an embedding-based benchmarking framework to detect bias in LLMs in the context of formative feedback. Using 600 authentic student essays from the AES 2.0 corpus, we constructed controlled counterfactuals along two dimensions: (i) implicit cues via lexicon-based swaps of gendered terms within essays, and (ii) explicit cues via gendered author background in the prompt. We investigated six representative LLMs (i.e. GPT-5 mini, GPT-4o mini, DeepSeek-R1, DeepSeek-R1-Qwen, Gemini 2.5 Pro, Llama-3-8B). We first quantified the response divergence with cosine and Euclidean distances over sentence embeddings, then assessed significance via permutation tests, and finally, visualised structure using dimensionality reduction. In all models, implicit manipulations reliably induced larger semantic shifts for male-female counterfactuals than for female-male. Only the GPT and Llama models showed sensitivity to explicit gender cues. These findings show that even state-of-the-art LLMs exhibit asymmetric semantic responses to gender substitutions, suggesting persistent gender biases in feedback they provide learners. Qualitative analyses further revealed consistent linguistic differences (e.g., more autonomy-supportive feedback under male cues vs. more controlling feedback under female cues). We discuss implications for fairness auditing of pedagogical GenAI, propose reporting standards for counterfactual evaluation in learning analytics, and outline practical guidance for prompt design and deployment to safeguard equitable feedback.

</details>


### [26] [VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context](https://arxiv.org/abs/2511.08230)
*Heyang Liu,Ziyang Cheng,Yuhao Wang,Hongcheng Liu,Yiqi Li,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 提出VocalBench-zh，一个针对普通话语境的多模态大语言模型评估套件，包含10个子集和10K+高质量实例，测试14个主流模型，揭示当前技术的挑战。


<details>
  <summary>Details</summary>
Motivation: 普通话作为全球广泛使用的语言，缺乏全面的语音转语音评估基准，阻碍了系统评估和公平模型比较。

Method: 开发VocalBench-zh，包含10个子集和10K+实例，覆盖12种用户导向特征。

Result: 评估14个主流模型，发现当前技术路线的共同挑战，强调需要新思路。

Conclusion: VocalBench-zh为普通话多模态模型提供系统评估工具，推动下一代语音交互系统发展。

Abstract: The development of multi-modal large language models (LLMs) leads to intelligent approaches capable of speech interactions. As one of the most widely spoken languages globally, Mandarin is supported by most models to enhance their applicability and reach. However, the scarcity of comprehensive speech-to-speech (S2S) benchmarks in Mandarin contexts impedes systematic evaluation for developers and hinders fair model comparison for users. In this work, we propose VocalBench-zh, an ability-level divided evaluation suite adapted to Mandarin context consisting of 10 well-crafted subsets and over 10K high-quality instances, covering 12 user-oriented characters. The evaluation experiment on 14 mainstream models reveals the common challenges for current routes, and highlights the need for new insights into next-generation speech interactive systems. The evaluation codes and datasets will be available at https://github.com/SJTU-OmniAgent/VocalBench-zh.

</details>


### [27] [ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech](https://arxiv.org/abs/2511.08247)
*Marios Koniaris,Argyro Tsipi,Panayiotis Tsanakas*

Main category: cs.CL

TL;DR: ParliaBench是一个针对议会演讲生成的基准测试，提出了一种结合计算指标和LLM评估的框架，并引入了两个基于嵌入的指标来量化意识形态定位。实验表明微调能显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型缺乏针对议会场景的专门训练，且评估方法未能涵盖政治真实性。为了填补这一空白，作者提出了ParliaBench。

Method: 构建了英国议会的演讲数据集，设计了一个结合计算指标和LLM评估的框架，并提出了Political Spectrum Alignment和Party Alignment两个新指标。微调了五种LLM模型。

Result: 微调后的模型在多数指标上表现显著提升，新提出的指标在政治维度上表现出较强的区分能力。

Conclusion: ParliaBench为议会演讲生成提供了系统化的训练和评估方法，并验证了微调和新型指标的有效性。

Abstract: Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them using our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.

</details>


### [28] [Hierarchical structure understanding in complex tables with VLLMs: a benchmark and experiments](https://arxiv.org/abs/2511.08298)
*Luca Bindini,Simone Giovannini,Simone Marinai,Valeria Nardoni,Kimiya Noor Ali*

Main category: cs.CL

TL;DR: 研究了视觉大语言模型（VLLMs）理解和解析科学文章中表格结构的能力，并提出了一个复杂层次表格（CHiTab）基准数据集。通过实验发现，未经专门设计的通用VLLMs也能完成此任务。


<details>
  <summary>Details</summary>
Motivation: 探索VLLMs是否可以推断表格的层次结构，以扩展其在处理结构化数据方面的能力。

Method: 利用PubTables-1M数据集提取复杂层次表格作为基准，采用多种提示工程策略，评估多种VLLMs的性能，并与人类表现进行对比。

Result: 实验表明，通用VLLMs能够完成表格结构理解任务，尽管未针对该任务进行专门设计。

Conclusion: 研究揭示了VLLMs处理复杂表格的潜力与局限性，并为未来在通用VLLMs中集成结构化数据理解提供了指导。

Abstract: This work investigates the ability of Vision Large Language Models (VLLMs) to understand and interpret the structure of tables in scientific articles. Specifically, we explore whether VLLMs can infer the hierarchical structure of tables without additional processing. As a basis for our experiments we use the PubTables-1M dataset, a large-scale corpus of scientific tables. From this dataset, we extract a subset of tables that we introduce as Complex Hierarchical Tables (CHiTab): a benchmark collection of complex tables containing hierarchical headings. We adopt a series of prompt engineering strategies to probe the models' comprehension capabilities, experimenting with various prompt formats and writing styles. Multiple state-of-the-art open-weights VLLMs are evaluated on the benchmark first using their off-the-shelf versions and then fine-tuning some models on our task. We also measure the performance of humans to solve the task on a small set of tables comparing with performance of the evaluated VLLMs. The experiments support our intuition that generic VLLMs, not explicitly designed for understanding the structure of tables, can perform this task. This study provides insights into the potential and limitations of VLLMs to process complex tables and offers guidance for future work on integrating structured data understanding into general-purpose VLLMs.

</details>


### [29] [Adaptive Multi-Agent Response Refinement in Conversational Systems](https://arxiv.org/abs/2511.08319)
*Soyeong Jeong,Aparna Elangovan,Emine Yilmaz,Oleg Rokhlenko*

Main category: cs.CL

TL;DR: 提出了一个多智能体框架，通过动态通信策略协同优化LLMs生成回答的真实性、个性化和连贯性，显著提升对话质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在个性化或特定知识需求上表现不足，当前单模型优化方法难以兼顾多样性，亟需更高效的改进机制。

Method: 采用多智能体框架，各智能体专注一个关键对话维度（真实性、个性化、连贯性），通过动态通信策略灵活协作优化回答。

Result: 在挑战性对话数据集上验证，显著优于基线，尤其在涉及知识或用户角色的任务中表现突出。

Conclusion: 多智能体框架通过分角色协作和动态通信，有效提升LLMs对话质量，为复杂需求提供了可行解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.

</details>


### [30] [DPRM: A Dual Implicit Process Reward Model in Multi-Hop Question Answering](https://arxiv.org/abs/2511.08364)
*Xinyi Wang,Yiping Song,Zhiliang Tian,Bo Liu,Tingjin Luo,Minlie Huang*

Main category: cs.CL

TL;DR: 论文提出了一种双隐式过程奖励模型（DPRM），通过分别对CoT和KG推理进行奖励建模，解决现有方法在多跳问答任务中无法处理图结构约束和推理路径不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 在多跳问答任务中，现有方法如ORMs仅评估最终答案，而传统PRMs需要高昂的人工标注。此外，现有隐式PRMs无法处理KG的结构约束和推理路径不一致问题，因此需要一种新的方法来解决这些限制。

Method: 提出DPRM模型，包含KG-PRM和CoT-PRM两个隐式PRMs，通过奖励参数化从结果信号中推导步骤级奖励。KG-PRM利用偏好对学习KG的结构约束，并通过一致性约束使两者相互验证和优化推理路径。

Result: 实验结果表明，DPRM在多个数据集上优于13个基线模型，Hit@1指标最高提升16.6%。

Conclusion: DPRM通过双隐式PRMs和一致性约束，显著提升了多跳问答任务的推理质量，为处理KG和CoT路径不一致问题提供了一种有效解决方案。

Abstract: In multi-hop question answering (MHQA) tasks, Chain of Thought (CoT) improves the quality of generation by guiding large language models (LLMs) through multi-step reasoning, and Knowledge Graphs (KGs) reduce hallucinations via semantic matching. Outcome Reward Models (ORMs) provide feedback after generating the final answers but fail to evaluate the process for multi-step reasoning. Traditional Process Reward Models (PRMs) evaluate the reasoning process but require costly human annotations or rollout generation. While implicit PRM is trained only with outcome signals and derives step rewards through reward parameterization without explicit annotations, it is more suitable for multi-step reasoning in MHQA tasks. However, existing implicit PRM has only been explored for plain text scenarios. When adapting to MHQA tasks, it cannot handle the graph structure constraints in KGs and capture the potential inconsistency between CoT and KG paths. To address these limitations, we propose the DPRM (Dual Implicit Process Reward Model). It trains two implicit PRMs for CoT and KG reasoning in MHQA tasks. Both PRMs, namely KG-PRM and CoT-PRM, derive step-level rewards from outcome signals via reward parameterization without additional explicit annotations. Among them, KG-PRM uses preference pairs to learn structural constraints from KGs. DPRM further introduces a consistency constraint between CoT and KG reasoning steps, making the two PRMs mutually verify and collaboratively optimize the reasoning paths. We also provide a theoretical demonstration of the derivation of process rewards. Experimental results show that our method outperforms 13 baselines on multiple datasets with up to 16.6% improvement on Hit@1.

</details>


### [31] [The Dynamic Articulatory Model DYNARTmo: Dynamic Movement Generation and Speech Gestures](https://arxiv.org/abs/2511.08372)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: DYNARTmo模型通过语音手势和手势评分生成连续的发音器运动，模拟了从语言表达到发音-声学实现的层级控制。


<details>
  <summary>Details</summary>
Motivation: 为语音生成的层级控制提供一个神经生物学启发的计算框架。

Method: 使用手势库的结构、手势协调和连续发音器轨迹的转换来控制DYNARTmo声道模型。

Result: 模型成功实现了从语言表达到发音-声学实现的连续映射。

Conclusion: DYNARTmo为语音生成的动态模拟提供了一种有效方法。

Abstract: This paper describes the current implementation of the dynamic articulatory model DYNARTmo, which generates continuous articulator movements based on the concept of speech gestures and a corresponding gesture score. The model provides a neurobiologically inspired computational framework for simulating the hierarchical control of speech production from linguistic representation to articulatory-acoustic realization. We present the structure of the gesture inventory, the coordination of gestures in the gesture score, and their translation into continuous articulator trajectories controlling the DYNARTmo vocal tract model.

</details>


### [32] [TurkEmbed: Turkish Embedding Model on NLI & STS Tasks](https://arxiv.org/abs/2511.08376)
*Özay Ezerceli,Gizem Gümüşçekiçci,Tuğba Erkoç,Berke Özenç*

Main category: cs.CL

TL;DR: TurkEmbed是一种新颖的土耳其语言嵌入模型，旨在在NLI和STS任务中超越现有模型，特别针对资源受限环境提供了更快的编码能力。


<details>
  <summary>Details</summary>
Motivation: 当前土耳其嵌入模型通常依赖机器翻译数据集，可能限制了其准确性和语义理解。TurkEmbed旨在通过多样化数据集和先进训练技术克服这些限制。

Method: TurkEmbed结合了多样化数据集和matryoshka表示学习等先进训练技术，生成更稳健和准确的嵌入表示。

Result: 在土耳其STS-b-TR数据集上，TurkEmbed在Pearson和Spearman相关性指标上表现显著提升，并在All-NLI-TR和STS-b-TR基准测试中超越现有最佳模型1-4%。

Conclusion: TurkEmbed通过提供更细微的语言理解能力，有望推动土耳其NLP生态系统的发展，并为下游应用提供支持。

Abstract: This paper introduces TurkEmbed, a novel Turkish language embedding model designed to outperform existing models, particularly in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding. TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach enables the model to adapt to various resource-constrained environments, offering faster encoding capabilities. Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4\% improvement. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.

</details>


### [33] [PCRLLM: Proof-Carrying Reasoning with Large Language Models under Stepwise Logical Constraints](https://arxiv.org/abs/2511.08392)
*Tangrui Li,Pei Wang,Hongzheng Wang Christian Hahm,Matteo Spatola,Justin Shi*

Main category: cs.CL

TL;DR: 提出了一个名为PCRLLM的框架，通过约束推理为单步推理并保留自然语言表述，提升LLM的逻辑连贯性和可验证性。


<details>
  <summary>Details</summary>
Motivation: LLMs在逻辑连贯性上表现不足，缺乏明确的推理规则约束，导致可信度问题。PCRLLM旨在解决这一问题并支持多LLM协作。

Method: 通过明确指定前提、规则和结论，PCRLLM支持单步推理，并在黑盒设置下实现链级验证。此外，提出了一个基准模式用于生成大规模推理数据。

Result: PCRLLM提升了LLM推理的可信度和协调性，同时支持多LLM的系统协作和验证。

Conclusion: PCRLLM为LLM提供了更加严谨和可验证的推理框架，结合了自然语言的灵活性和形式化的严谨性。

Abstract: Large Language Models (LLMs) often exhibit limited logical coherence, mapping premises to conclusions without adherence to explicit inference rules. We propose Proof-Carrying Reasoning with LLMs (PCRLLM), a framework that constrains reasoning to single-step inferences while preserving natural language formulations. Each output explicitly specifies premises, rules, and conclusions, thereby enabling verification against a target logic. This mechanism mitigates trustworthiness concerns by supporting chain-level validation even in black-box settings. Moreover, PCRLLM facilitates systematic multi-LLM collaboration, allowing intermediate steps to be compared and integrated under formal rules. Finally, we introduce a benchmark schema for generating large-scale step-level reasoning data, combining natural language expressiveness with formal rigor.

</details>


### [34] [Interaction Dynamics as a Reward Signal for LLMs](https://arxiv.org/abs/2511.08394)
*Sian Gooding,Edward Grefenstette*

Main category: cs.CL

TL;DR: TRACE是一种基于对话嵌入轨迹几何特性的新型奖励信号，展示了交互动态与文本内容结合能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM对齐方法仅依赖文本内容奖励信号，忽略了交互动态的丰富信息。本文旨在探索交互动态如何作为预测成功协作的重要指标。

Method: 提出TRACE方法，通过分析对话嵌入轨迹的几何特性（即“对话几何”）生成奖励信号，并与文本内容信号结合。

Result: 仅基于交互动态的奖励模型准确率接近完整文本分析的LLM基线（68.20% vs 70.04%），而混合模型性能最高（80.17%）。

Conclusion: 交互动态与文本内容互补，证明“如何交流”与“交流内容”同样重要，为对齐模型提供了隐私友好的新框架。

Abstract: The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.

</details>


### [35] [Bot Meets Shortcut: How Can LLMs Aid in Handling Unknown Invariance OOD Scenarios?](https://arxiv.org/abs/2511.08455)
*Shiyan Zheng,Herun Wan,Minnan Luo,Junhang Huang*

Main category: cs.CL

TL;DR: 该论文研究了现有社交机器人检测器在真实场景中的鲁棒性问题，并提出基于大语言模型的缓解策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有检测器在多样化真实场景中表现有限，主要因模糊的基准真相和误导性线索，尤其是捷径学习问题未引起足够重视。

Method: 通过构建基于文本特征的虚假关联场景评估模型鲁棒性，并提出基于反事实数据增强的缓解策略。

Result: 在捷径场景下，基线模型的准确率平均下降32%，而提出的策略使其相对性能提升56%。

Conclusion: 论文提出的方法有效减少了捷径学习的影响，提升了社交机器人检测器的鲁棒性和性能。

Abstract: While existing social bot detectors perform well on benchmarks, their robustness across diverse real-world scenarios remains limited due to unclear ground truth and varied misleading cues. In particular, the impact of shortcut learning, where models rely on spurious correlations instead of capturing causal task-relevant features, has received limited attention. To address this gap, we conduct an in-depth study to assess how detectors are influenced by potential shortcuts based on textual features, which are most susceptible to manipulation by social bots. We design a series of shortcut scenarios by constructing spurious associations between user labels and superficial textual cues to evaluate model robustness. Results show that shifts in irrelevant feature distributions significantly degrade social bot detector performance, with an average relative accuracy drop of 32\% in the baseline models. To tackle this challenge, we propose mitigation strategies based on large language models, leveraging counterfactual data augmentation. These methods mitigate the problem from data and model perspectives across three levels, including data distribution at both the individual user text and overall dataset levels, as well as the model's ability to extract causal information. Our strategies achieve an average relative performance improvement of 56\% under shortcut scenarios.

</details>


### [36] [SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging for Efficient Financial LLM Adaptation](https://arxiv.org/abs/2511.08500)
*Berkcan Kapusuzoglu,Supriyo Chakraborty,Renkun Ni,Stephen Rawls,Sambit Sahu*

Main category: cs.CL

TL;DR: SPEAR-MM是一种新型框架，通过在模型合并中选择性冻结或恢复参数来平衡金融领域适应和通用推理能力，显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在金融领域适应过程中容易遗忘通用推理能力的问题。

Method: 通过后验分析评估层对基准的影响，使用球面插值合并选择性冻结或恢复层。

Result: SPEAR-MM在LLaMA-3.1-8B上实现了91.2%的通用能力保留和94%的领域适应增益。

Conclusion: SPEAR-MM在保留关键能力的同时显著提升了效率和适应性，适合资源有限的金融机构。

Abstract: Large language models (LLMs) adapted to financial domains often suffer from catastrophic forgetting of general reasoning capabilities essential for customer interactions and complex financial analysis. We introduce Selective Parameter Evaluation and Restoration via Model Merging (SPEAR-MM), a practical framework that preserves critical capabilities while enabling domain adaptation. Our method approximates layer-wise impact on external benchmarks through post-hoc analysis, then selectively freezes or restores transformer layers via spherical interpolation merging. Applied to LLaMA-3.1-8B for financial tasks, SPEAR-MM achieves 91.2% retention of general capabilities versus 69.7% for standard continual pretraining, while maintaining 94% of domain adaptation gains. The approach provides interpretable trade-off control and reduces computational costs by 90% crucial for resource-constrained financial institutions.

</details>


### [37] [Structured RAG for Answering Aggregative Questions](https://arxiv.org/abs/2511.08505)
*Omri Koshorek,Niv Granot,Aviv Alloni,Shahar Admati,Roee Hendel,Ido Weiss,Alan Arazi,Shay-Nitzan Cohen,Yonatan Belinkov*

Main category: cs.CL

TL;DR: S-RAG是一种针对聚合查询设计的检索增强生成方法，通过构建结构化语料库表示和形式化查询转换，显著优于传统RAG系统和长上下文LLM。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成方法在处理需要从大量文档中收集信息的聚合查询时表现不佳，S-RAG旨在填补这一空白。

Method: S-RAG在数据摄取时构建语料库的结构化表示，推理时将自然语言查询转换为形式化查询。

Result: 在新增数据集HOTELS、WORLD CUP及公共基准测试中，S-RAG显著优于传统RAG和长上下文LLM。

Conclusion: S-RAG为聚合查询提供了更有效的解决方案，并推动了相关领域的研究。

Abstract: Retrieval-Augmented Generation (RAG) has become the dominant approach for answering questions over large corpora. However, current datasets and methods are highly focused on cases where only a small part of the corpus (usually a few paragraphs) is relevant per query, and fail to capture the rich world of aggregative queries. These require gathering information from a large set of documents and reasoning over them. To address this gap, we propose S-RAG, an approach specifically designed for such queries. At ingestion time, S-RAG constructs a structured representation of the corpus; at inference time, it translates natural-language queries into formal queries over said representation. To validate our approach and promote further research in this area, we introduce two new datasets of aggregative queries: HOTELS and WORLD CUP. Experiments with S-RAG on the newly introduced datasets, as well as on a public benchmark, demonstrate that it substantially outperforms both common RAG systems and long-context LLMs.

</details>


### [38] [Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research](https://arxiv.org/abs/2511.08507)
*Neelavro Saha,Rafi Shahriyar,Nafis Ashraf Roudra,Saadman Sakib,Annajiat Alim Rasel*

Main category: cs.CL

TL;DR: 本文介绍了Bangla-SGP数据集，包含1000条人工标注的句子-手势对，并通过规则增强生成3000条合成数据。作者微调了多种Transformer模型，评估其在句子到手势翻译任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模数据集，Bangla手语的句子级翻译任务研究受限。本文旨在填补这一空白。

Method: 构建Bangla-SGP数据集，结合人工标注和规则增强生成数据，并使用多种Transformer模型进行微调和评估。

Result: 模型在句子到手势翻译任务中表现良好，并通过BLEU分数与基准数据集RWTH-PHOENIX-2014T进行比较。

Conclusion: Bangla-SGP数据集和提出的方法为低资源手语翻译任务提供了有效解决方案。

Abstract: Bangla Sign Language (BdSL) translation represents a low-resource NLP task due to the lack of large-scale datasets that address sentence-level translation. Correspondingly, existing research in this field has been limited to word and alphabet level detection. In this work, we introduce Bangla-SGP, a novel parallel dataset consisting of 1,000 human-annotated sentence-gloss pairs which was augmented with around 3,000 synthetically generated pairs using syntactic and morphological rules through a rule-based Retrieval-Augmented Generation (RAG) pipeline. The gloss sequences of the spoken Bangla sentences are made up of individual glosses which are Bangla sign supported words and serve as an intermediate representation for a continuous sign. Our dataset consists of 1000 high quality Bangla sentences that are manually annotated into a gloss sequence by a professional signer. The augmentation process incorporates rule-based linguistic strategies and prompt engineering techniques that we have adopted by critically analyzing our human annotated sentence-gloss pairs and by working closely with our professional signer. Furthermore, we fine-tune several transformer-based models such as mBart50, Google mT5, GPT4.1-nano and evaluate their sentence-to-gloss translation performance using BLEU scores, based on these evaluation metrics we compare the model's gloss-translation consistency across our dataset and the RWTH-PHOENIX-2014T benchmark.

</details>


### [39] [Investigating CoT Monitorability in Large Reasoning Models](https://arxiv.org/abs/2511.08525)
*Shu Yang,Junchao Wu,Xilin Gou,Xuansheng Wu,Derek Wong,Ninhao Liu,Di Wang*

Main category: cs.CL

TL;DR: 该论文探讨了大型推理模型（LRMs）中基于思维链（CoT）的监控潜力与挑战，重点关注模型是否真实表达其推理过程以及监控方法的可靠性，并提出了一种新型监控范式MoME。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用LRMs的推理过程（CoT）监控模型的不当行为（如走捷径或阿谀奉承），并解决两个核心问题：模型是否能真实表达其决策因素，以及监控方法是否可靠。

Method: 通过实证分析和相关性研究，探讨了推理表达质量与监控可靠性之间的关系，并测试了不同CoT干预方法对监控效果的影响。最后提出MoME范式，通过结构化判断和证据支持的监控方式。

Result: 研究发现模型在推理表达上存在不真实性，且监控方法在敏感性和可靠性上面临挑战。同时，CoT干预方法的效果各不相同，而MoME范式显示出潜在的应用价值。

Conclusion: 论文认为CoT监控具有潜力，但需解决模型表达真实性和监控可靠性问题。MoME为未来研究提供了新方向。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence.

</details>


### [40] [From Semantic Roles to Opinion Roles: SRL Data Extraction for Multi-Task and Transfer Learning in Low-Resource ORL](https://arxiv.org/abs/2511.08537)
*Amirmohammad Omidi Galdiani,Sepehr Rezaei Melal,Mohammad Norasteh,Arash Yousefi Jordehi,Seyed Abolghasem Mirroshandel*

Main category: cs.CL

TL;DR: 该论文提出了一种从OntoNotes 5.0语料库中构建高质量语义角色标注（SRL）数据集的方法，并将其调整用于观点角色标注（ORL）任务。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为低资源观点挖掘场景下的观点角色标注（ORL）任务提供可复用的数据集，以增强SRL在ORL中的应用。

Method: 通过PropBank标注框架，实现了一个可重复的提取流程，包括对齐谓词-论元结构与表层文本、转换句法树指针为连贯的文本跨度，并进行严格的清洗以确保语义准确性。

Result: 最终数据集包含97,169个谓词-论元实例，并映射到ORL的Holder、Expression和Target角色。

Conclusion: 该工作为研究人员提供了一个可重复使用的资源，特别是在低资源观点挖掘中，利用SRL增强ORL任务。

Abstract: This report presents a detailed methodology for constructing a high-quality Semantic Role Labeling (SRL) dataset from the Wall Street Journal (WSJ) portion of the OntoNotes 5.0 corpus and adapting it for Opinion Role Labeling (ORL) tasks. Leveraging the PropBank annotation framework, we implement a reproducible extraction pipeline that aligns predicate-argument structures with surface text, converts syntactic tree pointers to coherent spans, and applies rigorous cleaning to ensure semantic fidelity. The resulting dataset comprises 97,169 predicate-argument instances with clearly defined Agent (ARG0), Predicate (REL), and Patient (ARG1) roles, mapped to ORL's Holder, Expression, and Target schema. We provide a detailed account of our extraction algorithms, discontinuous argument handling, annotation corrections, and statistical analysis of the resulting dataset. This work offers a reusable resource for researchers aiming to leverage SRL for enhancing ORL, especially in low-resource opinion mining scenarios.

</details>


### [41] [Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models](https://arxiv.org/abs/2511.08565)
*Davi Bastos Costa,Felippe Alves,Renato Vicente*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在角色扮演情境下的道德反应，通过道德基础问卷（MFQ）量化了道德易感性和道德鲁棒性，发现模型家族对鲁棒性影响显著，而模型大小对易感性有明确影响。


<details>
  <summary>Details</summary>
Motivation: 分析LLMs在社交环境中如何表达和改变道德判断，尤其是通过角色扮演情境下的行为表现。

Method: 使用Moral Foundations Questionnaire（MFQ）设计基准，量化道德易感性和道德鲁棒性，分析不同模型家族和大小的表现。

Result: Claude家族道德鲁棒性最高，家族内模型大小对道德易感性有显著影响；鲁棒性和易感性呈正相关。

Conclusion: 角色扮演条件对LLMs的道德行为有系统性影响，模型家族和大小是关键因素。

Abstract: Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.

</details>


### [42] [Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models](https://arxiv.org/abs/2511.08577)
*Tianyu Fu,Yichen You,Zekai Chen,Guohao Dai,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为Think-at-Hard（TaH）的动态潜在思考方法，仅对难以预测的标记进行深度迭代，以提高大型语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型（LLMs）的推理能力，尤其是在参数受限的情况下，对实际应用至关重要。现有方法存在过度思考现象（easy tokens改进为错误），需针对性优化。

Method: TaH通过轻量级神经决策器动态触发潜在迭代，仅对标准前向传播后可能错误的标记进行额外处理。使用Low-Rank Adaptation（LoRA）模块和duo-causal注意力机制，实现跨迭代信息流。

Result: 实验表明，TaH在五个挑战性基准测试中显著提升了推理性能，同时保持参数数量不变。与基线相比，准确率提升了8.1-11.3%，且94%的标记无需二次迭代。

Conclusion: TaH是一种高效的方法，通过动态聚焦于困难标记，显著提升了LLMs的推理能力，同时保持了模型轻量化和并行性。

Abstract: Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.

</details>


### [43] [Training Language Models to Explain Their Own Computations](https://arxiv.org/abs/2511.08579)
*Belinda Z. Li,Zifan Carl Guo,Vincent Huang,Jacob Steinhardt,Jacob Andreas*

Main category: cs.CL

TL;DR: 研究语言模型是否能通过学习可靠地描述其内部计算，并利用其特权访问自身内部结构的能力开发新的解释技术。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否能够利用对其内部的特权访问，生成对其行为的自然语言描述，以补充现有的可解释性方法。

Method: 使用现有可解释性技术作为基准，微调语言模型以生成对LM特征信息、内部激活的因果结构及输入令牌对输出影响的描述。

Result: 经过数万个解释示例训练的模型表现出对新查询的非平凡泛化能力，且自我解释的效果优于不同模型的解释。

Conclusion: 语言模型能够可靠地解释其内部计算，这种解释为现有可解释性方法提供了可扩展的补充。

Abstract: Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.

</details>


### [44] [REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment](https://arxiv.org/abs/2511.07458)
*Priyanka Mudgal*

Main category: cs.CL

TL;DR: REFLEX是一种基于大语言模型的无参考日志摘要评估指标，能够在缺乏高质量参考摘要的情况下评估摘要质量。


<details>
  <summary>Details</summary>
Motivation: 现有的日志摘要评估方法（如ROUGE和BLEU）依赖于表面词汇重叠且需要参考摘要，REFLEX旨在解决这些局限性，提供一种无需参考摘要的评估方法。

Method: REFLEX利用大语言模型作为零样本评估器，从相关性、信息量和连贯性等维度评估日志摘要质量，无需人工标注或参考摘要。

Result: REFLEX在多数据集上表现稳定且可解释，能更有效地区分不同模型的输出，优于传统评估指标。

Conclusion: REFLEX为实际应用中缺乏参考数据的情况提供了一种可扩展的日志摘要评估解决方案。

Abstract: Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free evaluation metric for log summarization based on large language model (LLM) judgment. REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations. We show that REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization dataset, and more effectively distinguishes model outputs than traditional metrics. REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.

</details>


### [45] [It Takes Two: A Dual Stage Approach for Terminology-Aware Translation](https://arxiv.org/abs/2511.07461)
*Akshat Singh Jaswal*

Main category: cs.CL

TL;DR: DuTerm 是一种新颖的两阶段术语约束机器翻译架构，结合了微调的术语感知 NMT 模型和基于提示的 LLM 后编辑，效果优于严格约束方法。


<details>
  <summary>Details</summary>
Motivation: 解决术语约束机器翻译中的灵活性与准确性平衡问题，提出一种更高效的翻译方法。

Method: 两阶段架构：首先使用术语感知 NMT 模型进行初步翻译，再通过基于提示的 LLM 进行后编辑以优化术语使用。

Result: 在英德、英西和英俄翻译任务中，DuTerm 表现出色，LLM 的上下文驱动术语处理效果优于严格约束。

Conclusion: LLM 更适合作为上下文驱动的优化工具而非生成工具，在高质量翻译中发挥关键作用。

Abstract: This paper introduces DuTerm, a novel two-stage architecture for terminology-constrained machine translation. Our system combines a terminology-aware NMT model, adapted via fine-tuning on large-scale synthetic data, with a prompt-based LLM for post-editing. The LLM stage refines NMT output and enforces terminology adherence. We evaluate DuTerm on English-to German, English-to-Spanish, and English-to-Russian with the WMT 2025 Terminology Shared Task corpus. We demonstrate that flexible, context-driven terminology handling by the LLM consistently yields higher quality translations than strict constraint enforcement. Our results highlight a critical trade-off, revealing that an LLM's work best for high-quality translation as context-driven mutators rather than generators.

</details>


### [46] [Motif 2 12.7B technical report](https://arxiv.org/abs/2511.07464)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.CL

TL;DR: Motif-2-12.7B是一个新型开源基础模型，通过结合架构创新与系统级优化，提升了大型语言模型的效率。它采用分组差分注意力（GDA）和改进的训练系统，在有限计算资源下实现高效语言理解和指令泛化。


<details>
  <summary>Details</summary>
Motivation: 旨在在有限计算资源下提升大型语言模型的效率，同时保持其在多样化任务中的高性能。

Method: 通过整合分组差分注意力（GDA）、使用课程驱动的数据调度器、优化器MuonClip和定制高性能内核（如融合的PolyNorm激活和并行Muon算法）进行预训练，并通过三阶段微调流程提升模型性能。

Result: 在多样化基准测试中展示了竞争力，表明其架构扩展和优化训练设计能与更大模型匹敌。

Conclusion: Motif-2-12.7B通过精心设计的架构和训练优化，实现了高效且高性能的语言模型能力。

Abstract: We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.

</details>


### [47] [Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models](https://arxiv.org/abs/2511.07498)
*Xin Liu,Qiyang Song,Qihang Zhou,Haichao Du,Shaowen Xu,Wenbo Jiang,Weijuan Zhang,Xiaoqi Jia*

Main category: cs.CL

TL;DR: 提出了一种名为LAHIS的方法，通过单次前向和后向传播识别多语言处理中注意力头的重要性，揭示了语言特定和通用头的作用，并通过轻量级适应改善了多语言LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 探索多头自注意力（MHA）在多语言处理中的作用，以增强大型语言模型（LLM）的多语言能力和可解释性。

Method: 提出Language Attention Head Importance Scores（LAHIS）方法，通过单次前向和后向传播评估注意力头的重要性；并引入轻量级适应机制，通过软头掩码调制注意力输出。

Result: 在Aya-23-8B、Llama-3.2-3B和Mistral-7B-v0.1模型中，揭示了语言特定和通用头的存在，且轻量级适应仅需20个可调参数即可提升XQuAD准确性。

Conclusion: 从MHA的角度提升了LLM的可解释性和多语言能力，语言特定头的发现有助于解决多语言LLM中的挑战。

Abstract: Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.

</details>


### [48] [Revisiting NLI: Towards Cost-Effective and Human-Aligned Metrics for Evaluating LLMs in Question Answering](https://arxiv.org/abs/2511.07659)
*Sai Shridhar Balamurali,Lu Cheng*

Main category: cs.CL

TL;DR: NLI-based evaluation matches GPT-4o's accuracy in QA with fewer computational resources, supported by a new benchmark DIVER-QA.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of evaluating LLM answers by proposing a cost-effective alternative.

Method: Utilizes off-the-shelf NLI scoring with a lexical-match flag, tested on DIVER-QA benchmark.

Result: NLI-based method achieves 89.9% accuracy, comparable to GPT-4o but more efficient.

Conclusion: Lightweight NLI evaluation is competitive, and DIVER-QA provides a resource for future research.

Abstract: Evaluating answers from state-of-the-art large language models (LLMs) is challenging: lexical metrics miss semantic nuances, whereas "LLM-as-Judge" scoring is computationally expensive. We re-evaluate a lightweight alternative -- off-the-shelf Natural Language Inference (NLI) scoring augmented by a simple lexical-match flag and find that this decades-old technique matches GPT-4o's accuracy (89.9%) on long-form QA, while requiring orders-of-magnitude fewer parameters. To test human alignment of these metrics rigorously, we introduce DIVER-QA, a new 3000-sample human-annotated benchmark spanning five QA datasets and five candidate LLMs. Our results highlight that inexpensive NLI-based evaluation remains competitive and offer DIVER-QA as an open resource for future metric research.

</details>


### [49] [CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences](https://arxiv.org/abs/2511.07691)
*Rhitabrat Pokharel,Yufei Tao,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 论文提出了一种名为CAPO的置信感知偏好优化方法，用于在多语言环境中优化大型语言模型与人类偏好的对齐效果，其通过动态损失缩放机制提升了鲁棒性和表现。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）在英语环境中表现良好，但在多语言环境中鲁棒性不足，需要一种更适应多语言噪声和低边界比较的新方法。

Method: CAPO通过引入基于相对奖励的动态损失缩放机制替代了DPO的固定处理方式，根据每个偏好对的置信度调整学习信号。

Result: 实验表明，CAPO在奖励准确率上比现有基线方法至少提升16%，并扩大了不同语言中偏好与非偏好响应之间的差距。

Conclusion: CAPO是一种简单而有效的多语言偏好优化方法，显著提升了模型的表现和对齐能力。

Abstract: Preference optimization is a critical post-training technique used to align large language models (LLMs) with human preferences, typically by fine-tuning on ranked response pairs. While methods like Direct Preference Optimization (DPO) have proven effective in English, they often fail to generalize robustly to multilingual settings. We propose a simple yet effective alternative, Confidence-Aware Preference Optimization (CAPO), which replaces DPO's fixed treatment of preference pairs with a dynamic loss scaling mechanism based on a relative reward. By modulating the learning signal according to the confidence in each preference pair, CAPO enhances robustness to noisy or low-margin comparisons, typically encountered in multilingual text. Empirically, CAPO outperforms existing preference optimization baselines by at least 16% in reward accuracy, and improves alignment by widening the gap between preferred and dispreferred responses across languages.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [50] [Analysing Environmental Efficiency in AI for X-Ray Diagnosis](https://arxiv.org/abs/2511.07436)
*Liam Kearns*

Main category: cs.AI

TL;DR: 论文分析了将大型语言模型（LLMs）和小型判别模型集成到医疗应用中以检测Covid-19的效果，比较了14种模型的准确性和环境影响，发现小型模型虽降低碳足迹但输出有偏差，而LLMs受限时表现不佳。Covid-Net模型在准确性和环保性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨生成模型和判别模型在医疗诊断中的性能差异及其环境影响，特别是针对Covid-19检测，为AI工具的合理选择提供依据。

Method: 集成LLMs（如ChatGPT和Claude）与小型判别模型到Mendix应用中，比较14种不同模型配置的准确性和碳足迹。

Result: 小型模型虽减少碳足迹，但输出偏向阳性诊断且置信度不足；LLMs受限时表现不佳；Covid-Net模型准确率达95.5%，碳排放比GPT-4.5-Preview低99.9%。

Conclusion: 生成模型不适合分类任务，应优先选择高效的小型判别模型如Covid-Net，以兼顾环保性和准确性。

Abstract: The integration of AI tools into medical applications has aimed to improve the efficiency of diagnosis. The emergence of large language models (LLMs), such as ChatGPT and Claude, has expanded this integration even further. Because of LLM versatility and ease of use through APIs, these larger models are often utilised even though smaller, custom models can be used instead. In this paper, LLMs and small discriminative models are integrated into a Mendix application to detect Covid-19 in chest X-rays. These discriminative models are also used to provide knowledge bases for LLMs to improve accuracy. This provides a benchmark study of 14 different model configurations for comparison of accuracy and environmental impact. The findings indicated that while smaller models reduced the carbon footprint of the application, the output was biased towards a positive diagnosis and the output probabilities were lacking confidence. Meanwhile, restricting LLMs to only give probabilistic output caused poor performance in both accuracy and carbon footprint, demonstrating the risk of using LLMs as a universal AI solution. While using the smaller LLM GPT-4.1-Nano reduced the carbon footprint by 94.2% compared to the larger models, this was still disproportionate to the discriminative models; the most efficient solution was the Covid-Net model. Although it had a larger carbon footprint than other small models, its carbon footprint was 99.9% less than when using GPT-4.5-Preview, whilst achieving an accuracy of 95.5%, the highest of all models examined. This paper contributes to knowledge by comparing generative and discriminative models in Covid-19 detection as well as highlighting the environmental risk of using generative tools for classification tasks.

</details>


### [51] [Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning](https://arxiv.org/abs/2511.07483)
*Qianxi He,Qingyu Ren,Shanzhe Lei,Xuhong Wang,Yingchun Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于置信度的奖励模型，专门用于提升STEM领域的推理能力，通过惩罚低置信度的正确答案和错误答案，促进更稳健和逻辑一致的推理过程。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的强化学习奖励在推理任务中容易导致低质量的推理链或推理过程与最终答案不一致的问题，尤其在小规模模型中表现更差。本文旨在解决这一问题，为资源有限的机构提供更高效的训练方法。

Method: 提出了一种新型的基于置信度的奖励模型，不仅惩罚错误答案，还惩罚低置信度的正确答案。通过静态评估、Best-of-N推理测试和基于PPO的强化学习训练验证其有效性。

Result: 该方法在多个STEM基准测试中优于其他开源奖励模型，证明了其在提升推理能力方面的优越性。

Conclusion: 基于置信度的奖励模型能够显著提升小规模模型的推理能力，为资源有限的机构提供了高效的训练方案，代码和模型已开源。

Abstract: Recent advancements in large language models (LLMs) have shifted the post-training paradigm from traditional instruction tuning and human preference alignment toward reinforcement learning (RL) focused on reasoning capabilities. However, numerous technical reports indicate that purely rule-based reward RL frequently results in poor-quality reasoning chains or inconsistencies between reasoning processes and final answers, particularly when the base model is of smaller scale. During the RL exploration process, models might employ low-quality reasoning chains due to the lack of knowledge, occasionally producing correct answers randomly and receiving rewards based on established rule-based judges. This constrains the potential for resource-limited organizations to conduct direct reinforcement learning training on smaller-scale models. We propose a novel confidence-based reward model tailored for enhancing STEM reasoning capabilities. Unlike conventional approaches, our model penalizes not only incorrect answers but also low-confidence correct responses, thereby promoting more robust and logically consistent reasoning. We validate the effectiveness of our approach through static evaluations, Best-of-N inference tests, and PPO-based RL training. Our method outperforms several state-of-the-art open-source reward models across diverse STEM benchmarks. We release our codes and model in https://github.com/qianxiHe147/C2RM.

</details>


### [52] [Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces](https://arxiv.org/abs/2511.07587)
*Shreyas Rajesh,Pavan Holur,Chenda Duan,David Chong,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: 该论文提出了一种名为GSW的生成性语义工作空间框架，用于解决大型语言模型在长上下文推理中的挑战。该框架通过构建结构化的情景表示，显著优于现有检索增强生成方法，并在效率和性能上均有提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文推理中存在性能下降和无法处理超长文档的问题。现有解决方案无法构建时空锚定的叙述表示，需要新的方法来增强模型的推理能力。

Method: 提出GSW框架，包含Operator和Reconciler两个组件，用于生成和整合语义结构，确保时空和逻辑一致性。

Result: 在Episodic Memory Benchmark上，GSW比现有方法性能提升20%，查询时上下文token减少51%。

Conclusion: GSW为大型语言模型提供了类似人类情景记忆的能力，显著提升了长时推理能力。

Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the \textbf{Generative Semantic Workspace} (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an \textit{Operator}, which maps incoming observations to intermediate semantic structures, and a \textit{Reconciler}, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) \cite{huet_episodic_2025} comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to \textbf{20\%}. Furthermore, GSW is highly efficient, reducing query-time context tokens by \textbf{51\%} compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.

</details>


### [53] [AI-Driven Contribution Evaluation and Conflict Resolution: A Framework & Design for Group Workload Investigation](https://arxiv.org/abs/2511.07667)
*Jakub Slapek,Mir Seyedebrahimi,Yang Jianhua*

Main category: cs.AI

TL;DR: 本文提出了一种基于AI的框架，用于评估团队成员的个人贡献，通过整合多维数据和LLM分析，解决团队中不公平评估的问题。


<details>
  <summary>Details</summary>
Motivation: 团队中个人贡献的公平评估是一个持续性挑战，传统方法成本高且效率低，需要通过AI工具填补现有冲突解决方法和AI整合的空白。

Method: 设计了一个框架，将异构数据（如提交内容、通信记录、协调日志等）组织为三个维度和九个基准，利用客观指标和LLM进行分析。

Result: 框架能够通过标准化指标和不平等度量（如基尼系数）识别冲突标志，并结合LLM生成可解释的评估建议。

Conclusion: 该方案在当前法规和制度下可行，提出了实用分析工具和偏见防护措施，但也面临实际挑战。

Abstract: The equitable assessment of individual contribution in teams remains a persistent challenge, where conflict and disparity in workload can result in unfair performance evaluation, often requiring manual intervention - a costly and challenging process. We survey existing tool features and identify a gap in conflict resolution methods and AI integration. To address this, we propose a framework and implementation design for a novel AI-enhanced tool that assists in dispute investigation. The framework organises heterogeneous artefacts - submissions (code, text, media), communications (chat, email), coordination records (meeting logs, tasks), peer assessments, and contextual information - into three dimensions with nine benchmarks: Contribution, Interaction, and Role. Objective measures are normalised, aggregated per dimension, and paired with inequality measures (Gini index) to surface conflict markers. A Large Language Model (LLM) architecture performs validated and contextual analysis over these measures to generate interpretable and transparent advisory judgments. We argue for feasibility under current statutory and institutional policy, and outline practical analytics (sentimental, task fidelity, word/line count, etc.), bias safeguards, limitations, and practical challenges.

</details>


### [54] [Making LLMs Reliable When It Matters Most: A Five-Layer Architecture for High-Stakes Decisions](https://arxiv.org/abs/2511.07669)
*Alejandro R. Jadad*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在验证性领域和高风险战略决策中的表现差异，提出了一种框架，通过校准序列和保护架构实现人类与AI的认知伙伴关系，以减少决策中的遗憾。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补LLMs在高风险战略决策中的可靠性空白，解决人类和AI系统在决策中相互强化的认知偏差问题，以提高投资的可持续性和估值的合理性。

Method: 研究通过系统性定性评估，对7种前沿级LLMs和3个市场导向的创业案例进行测试。设计了详细的提示框架，包括决策伙伴关系和避免常见认知偏差的指令，并开发了7阶段校准序列和5层保护架构。

Result: 研究发现，通过有序校准可以实现伙伴关系状态，但需要维护协议；可靠性会因架构漂移和上下文耗尽而下降；解散纪律可以避免错误方向的持续投入。不同LLM架构之间存在系统性性能差异。

Conclusion: 研究表明，人类与AI团队可以通过认知伙伴关系在高风险决策中避免本可避免的遗憾，满足依赖AI支持决策的投资回报期望，同时避免因验证滞后而引入的认知陷阱。

Abstract: Current large language models (LLMs) excel in verifiable domains where outputs can be checked before action but prove less reliable for high-stakes strategic decisions with uncertain outcomes. This gap, driven by mutually reinforcing cognitive biases in both humans and artificial intelligence (AI) systems, threatens the defensibility of valuations and sustainability of investments in the sector.
  This report describes a framework emerging from systematic qualitative assessment across 7 frontier-grade LLMs and 3 market-facing venture vignettes under time pressure. Detailed prompting specifying decision partnership and explicitly instructing avoidance of sycophancy, confabulation, solution drift, and nihilism achieved initial partnership state but failed to maintain it under operational pressure. Sustaining protective partnership state required an emergent 7-stage calibration sequence, built upon a 4-stage initialization process, within a 5-layer protection architecture enabling bias self-monitoring, human-AI adversarial challenge, partnership state verification, performance degradation detection, and stakeholder protection.
  Three discoveries resulted: partnership state is achievable through ordered calibration but requires emergent maintenance protocols; reliability degrades when architectural drift and context exhaustion align; and dissolution discipline prevents costly pursuit of fundamentally wrong directions. Cross-model validation revealed systematic performance differences across LLM architectures.
  This approach demonstrates that human-AI teams can achieve cognitive partnership capable of preventing avoidable regret in high-stakes decisions, addressing return-on-investment expectations that depend on AI systems supporting consequential decision-making without introducing preventable cognitive traps when verification arrives too late.

</details>


### [55] [ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents](https://arxiv.org/abs/2511.07685)
*Manasi Sharma,Chen Bo Calvin Zhang,Chaithanya Bandi,Clinton Wang,Ankit Aich,Huy Nghiem,Tahseen Rabbani,Ye Htet,Brian Jang,Sumana Basu,Aishwarya Balwani,Denis Peskoff,Marcos Ayestaran,Sean M. Hendryx,Brad Kenstler,Bing Liu*

Main category: cs.AI

TL;DR: 论文提出了一个名为ResearchRubrics的标准化基准，用于评估深度研究（DR）应用的表现，包括500+专家编写的细粒度评分标准和复杂的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究（DR）应用的评估存在挑战，因为其回答长度和多样性高，且依赖动态信息源。论文旨在提供一个标准化评估工具。

Method: 通过2,800+人工小时构建ResearchRubrics基准，包含领域多样化的提示和500+评分标准，并提出新的复杂性框架。

Result: 评估结果显示，即使是先进的DR系统（如Gemini和OpenAI的DR）在评分标准上的平均符合率也低于68%，主要问题是隐式上下文缺失和检索信息推理不足。

Conclusion: 研究强调了开发健壮、可扩展的DR评估工具的重要性，并开源ResearchRubrics以促进进一步研究。

Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.

</details>


### [56] [Towards AI-Assisted Generation of Military Training Scenarios](https://arxiv.org/abs/2511.07690)
*Soham Hans,Volkan Ustun,Benjamin Nye,James Sterrett,Matthew Green*

Main category: cs.AI

TL;DR: 该论文提出了一种基于大型语言模型的多智能体框架，用于自动生成军事训练中的复杂场景和操作指令（如OPORD），通过分层子问题解决和多样化智能体协作，显著提高了生成文档的一致性和动态适应性。


<details>
  <summary>Details</summary>
Motivation: 传统的仿真训练场景生成过程繁琐且资源密集，现有的AI工具难以生成足够复杂或适应性强的场景。为此，研究旨在解决这一问题，推动自动化在军事训练场景生成中的应用。

Method: 论文提出了一种多智能体、多模态推理框架，将场景生成分解为子问题层次结构，并定义了AI工具在每个子问题中的角色。通过专门设计的基于LLM的智能体，逐步处理文本和视觉信息，确保逻辑一致性和文档准确性。

Result: 通过概念验证，框架成功生成了OPORD中的机动和移动部分，并估计了地图位置和移动情况，展示了其可行性和准确性。生成的结果具有一致性和动态适应性。

Conclusion: 研究表明，LLM驱动的多智能体系统能够生成复杂且适应性强的训练文档，为军事训练中的自动化场景生成提供了新的可能性。

Abstract: Achieving expert-level performance in simulation-based training relies on the creation of complex, adaptable scenarios, a traditionally laborious and resource intensive process. Although prior research explored scenario generation for military training, pre-LLM AI tools struggled to generate sufficiently complex or adaptable scenarios. This paper introduces a multi-agent, multi-modal reasoning framework that leverages Large Language Models (LLMs) to generate critical training artifacts, such as Operations Orders (OPORDs). We structure our framework by decomposing scenario generation into a hierarchy of subproblems, and for each one, defining the role of the AI tool: (1) generating options for a human author to select from, (2) producing a candidate product for human approval or modification, or (3) generating textual artifacts fully automatically. Our framework employs specialized LLM-based agents to address distinct subproblems. Each agent receives input from preceding subproblem agents, integrating both text-based scenario details and visual information (e.g., map features, unit positions and applies specialized reasoning to produce appropriate outputs. Subsequent agents process these outputs sequentially, preserving logical consistency and ensuring accurate document generation. This multi-agent strategy overcomes the limitations of basic prompting or single-agent approaches when tackling such highly complex tasks. We validate our framework through a proof-of-concept that generates the scheme of maneuver and movement section of an OPORD while estimating map positions and movements as a precursor demonstrating its feasibility and accuracy. Our results demonstrate the potential of LLM-driven multi-agent systems to generate coherent, nuanced documents and adapt dynamically to changing conditions, advancing automation in scenario generation for military training.

</details>


### [57] [Alignment-Aware Quantization for LLM Safety](https://arxiv.org/abs/2511.07842)
*Sunghyun Wee,Suyoung Kim,Hyeonjin Kim,Kyomin Hwang,Nojun Kwak*

Main category: cs.AI

TL;DR: 论文提出了一种名为Alignment-Aware Quantization (AAQ)的新方法，通过在量化过程中加入Alignment-Preserving Contrastive (APC)损失函数，解决大语言模型（LLM）在量化后安全性与效率的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 传统的后训练量化（PTQ）方法通常仅关注低困惑度，忽视了量化可能对模型的安全性产生负面影响，导致模型虽然困惑度低但在安全策略上表现不佳。

Method: AAQ方法将APC损失函数整合到PTQ流程中，通过让量化模型模仿安全的指令调优模型，同时远离未对齐的预训练模型，从而保持模型的安全性。

Result: AAQ方法在不依赖专门的安全校准数据集的情况下，实现了对LLaMA、Qwen和Mistral等模型家族的稳健4位（W4A4）量化，并保持了安全性。

Conclusion: AAQ方法成功解决了效率与安全性之间的关键平衡问题，为实现高效且可信的大语言模型铺平了道路。

Abstract: Safety and efficiency are both important factors when deploying large language models(LLMs). LLMs are trained to follow human alignment for safety, and post training quantization(PTQ) is applied afterward for efficiency. However, these two objectives are often in conflict, revealing a fundamental flaw in the conventional PTQ paradigm: quantization can turn into a safety vulnerability if it only aims to achieve low perplexity. Models can demonstrate low perplexity yet exhibit significant degradation in alignment with the safety policy, highlighting that perplexity alone is an insufficient and often misleading proxy for model safety. To address this, we propose Alignment-Aware Quantization(AAQ), a novel approach that integrates Alignment-Preserving Contrastive(APC) loss into the PTQ pipeline. Compared to simple reconstruction loss, ours explicitly preserves alignment by encouraging the quantized model to mimic its safe, instruction-tuned model while diverging from the unaligned, pre-trained counterpart. Our method achieves this robust safety alignment without resorting to specialized safety-focused calibration datasets, highlighting its practical utility and broad applicability. AAQ is compatible with standard PTQ techniques and enables robust 4-bit (W4A4) quantization across diverse model families such as LLaMA, Qwen, and Mistral while maintaining safety where previous methods fail. Our work resolves the critical trade-off between efficiency and safety, paving the way toward LLMs that are both efficient and trustworthy. Anonymized code is available in the supplementary material.

</details>


### [58] [WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking](https://arxiv.org/abs/2511.07863)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.AI

TL;DR: WaterMod是一种基于概率感知模块化规则的水印方法，通过保持高概率令牌的可用性，既实现强水印检测性能，又维持生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型生成的内容需要满足监管要求（如欧盟AI法案）的可追溯性，但传统基于logit的水印方法可能因随机分割而影响生成质量。

Method: WaterMod通过对词汇按概率排序并用模运算分区，保持高概率令牌可用性，并通过小幅度偏置嵌入信号。

Result: 实验表明，WaterMod在零比特和多比特设置下均能实现强水印检测性能且保持生成质量，适用于多种任务。

Conclusion: WaterMod通过模块化算术同时支持二进制属性和丰富载荷，兼顾可追溯性与生成效果。

Abstract: Large language models now draft news, legal analyses, and software code with human-level fluency. At the same time, regulations such as the EU AI Act mandate that each synthetic passage carry an imperceptible, machine-verifiable mark for provenance. Conventional logit-based watermarks satisfy this requirement by selecting a pseudorandom green vocabulary at every decoding step and boosting its logits, yet the random split can exclude the highest-probability token and thus erode fluency. WaterMod mitigates this limitation through a probability-aware modular rule. The vocabulary is first sorted in descending model probability; the resulting ranks are then partitioned by the residue rank mod k, which distributes adjacent-and therefore semantically similar-tokens across different classes. A fixed bias of small magnitude is applied to one selected class. In the zero-bit setting (k=2), an entropy-adaptive gate selects either the even or the odd parity as the green list. Because the top two ranks fall into different parities, this choice embeds a detectable signal while guaranteeing that at least one high-probability token remains available for sampling. In the multi-bit regime (k>2), the current payload digit d selects the color class whose ranks satisfy rank mod k = d. Biasing the logits of that class embeds exactly one base-k digit per decoding step, thereby enabling fine-grained provenance tracing. The same modular arithmetic therefore supports both binary attribution and rich payloads. Experimental results demonstrate that WaterMod consistently attains strong watermark detection performance while maintaining generation quality in both zero-bit and multi-bit settings. This robustness holds across a range of tasks, including natural language generation, mathematical reasoning, and code synthesis. Our code and data are available at https://github.com/Shinwoo-Park/WaterMod.

</details>


### [59] [Toward Robust EEG-based Intention Decoding during Misarticulated Speech in Aphasia](https://arxiv.org/abs/2511.07895)
*Ha-Na Jo,Jung-Sun Lee,Eunyeong Ko*

Main category: cs.AI

TL;DR: 本文提出了一种基于脑电图（EEG）的辅助系统，用于帮助表达性失语症患者在言语表达困难时实现意图解码。通过分析EEG信号的频谱特征，设计了一种软多任务学习框架，显著提高了对错误发音情况的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 表达性失语症患者因语言功能受损，常出现发音错误，影响了言语交流。目前基于脑机接口的辅助系统较少关注此类患者的需求。本研究旨在填补这一空白，探索EEG信号在支持失语症患者交流中的潜力。

Method: 研究招募了一名表达性失语症患者，进行韩语自动言语任务，并记录了任务执行中的EEG信号。通过频谱分析识别了正确和错误发音的神经激活模式差异，并设计了基于最大均值差异正则化的软多任务学习框架，重点利用delta波段特征优化分类性能。

Result: 模型在正确和错误发音试验中的分类准确率分别为58.6%和45.5%，后者较基线方法提升了45%以上，表明即使在发音错误情况下也能有效解码意图。

Conclusion: 研究证明了基于EEG的辅助系统在支持失语症患者现实言语交流中的可行性，为未来开发更实用的辅助技术提供了基础。

Abstract: Aphasia severely limits verbal communication due to impaired language production, often leading to frequent misarticulations during speech attempts. Despite growing interest in brain-computer interface technologies, relatively little attention has been paid to developing EEG-based communication support systems tailored for aphasic patients. To address this gap, we recruited a single participant with expressive aphasia and conducted an Korean-based automatic speech task. EEG signals were recorded during task performance, and each trial was labeled as either correct or incorrect depending on whether the intended word was successfully spoken. Spectral analysis revealed distinct neural activation patterns between the two trial types: misarticulated trials exhibited excessive delta power across widespread channels and increased theta-alpha activity in frontal regions. Building upon these findings, we developed a soft multitask learning framework with maximum mean discrepancy regularization that focus on delta features to jointly optimize class discrimination while aligning the EEG feature distributions of correct and misarticulated trials. The proposed model achieved 58.6 % accuracy for correct and 45.5 % for misarticulated trials-outperforming the baseline by over 45 % on the latter-demonstrating robust intention decoding even under articulation errors. These results highlight the feasibility of EEG-based assistive systems capable of supporting real-world, imperfect speech conditions in aphasia patients.

</details>


### [60] [SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder](https://arxiv.org/abs/2511.07896)
*Dengcan Liu,Jiahao Li,Zheren Fu,Yi Tu,Jiajun Li,Zhendong Mao,Yongdong Zhang*

Main category: cs.AI

TL;DR: 提出了SparseRM，利用稀疏自编码器（SAE）从大型语言模型（LLM）表示中提取与偏好相关的信息，构建轻量级且可解释的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型（RM）依赖于大规模偏好标注和昂贵的LLM微调，资源消耗大。希望通过稀疏自编码器提取关键特征，降低训练成本。

Method: SparseRM通过SAE分解LLM表示，捕捉偏好相关特征，并投影到这些特征方向计算对齐分数，最后通过简单奖励头预测偏好分数。

Result: 实验表明，SparseRM在三个偏好建模任务中优于主流RM，且仅使用不到1%的可训练参数，下游对齐任务中也表现优异。

Conclusion: SparseRM为高效对齐提供了一种轻量级、可解释的解决方案。

Abstract: Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.

</details>


### [61] [Data Descriptions from Large Language Models with Influence Estimation](https://arxiv.org/abs/2511.07897)
*Chaeri Kim,Jaeyeon Bae,Taehwan Kim*

Main category: cs.AI

TL;DR: 论文提出了一种通过语言解释深度学习模型训练数据的方法，结合外部知识库和大语言模型生成文本描述，并通过影响估计和CLIP分数筛选有效信息。同时提出跨模态转移分类任务验证文本描述效果，实验证明其优于基准方法并能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型行为仍被视为“黑箱”，现有可解释AI方法主要关注模型预测的解释，而本文旨在通过语言这种易于理解的方式解释数据本身。

Method: 提出一种结合外部知识库和大语言模型的文本描述生成流程，利用影响估计和CLIP分数筛选最相关的文本描述，并设计跨模态转移分类任务验证其效果。

Result: 零样本实验中，生成的文本描述优于其他基准描述，且能提升仅基于图像训练的模型在9个分类数据集上的性能，经GPT-4o评估进一步验证。

Conclusion: 该方法为模型决策过程提供了内在可解释性的新视角。

Abstract: Deep learning models have been successful in many areas but understanding their behaviors still remains a black-box. Most prior explainable AI (XAI) approaches have focused on interpreting and explaining how models make predictions. In contrast, we would like to understand how data can be explained with deep learning model training and propose a novel approach to understand the data via one of the most common media - language - so that humans can easily understand. Our approach proposes a pipeline to generate textual descriptions that can explain the data with large language models by incorporating external knowledge bases. However, generated data descriptions may still include irrelevant information, so we introduce to exploit influence estimation to choose the most informative textual descriptions, along with the CLIP score. Furthermore, based on the phenomenon of cross-modal transferability, we propose a novel benchmark task named cross-modal transfer classification to examine the effectiveness of our textual descriptions. In the experiment of zero-shot setting, we show that our textual descriptions are more effective than other baseline descriptions, and furthermore, we successfully boost the performance of the model trained only on images across all nine image classification datasets. These results are further supported by evaluation using GPT-4o. Through our approach, we may gain insights into the inherent interpretability of the decision-making process of the model.

</details>


### [62] [DANS-KGC: Diffusion Based Adaptive Negative Sampling for Knowledge Graph Completion](https://arxiv.org/abs/2511.07901)
*Haoning Li,Qinghua Huang*

Main category: cs.AI

TL;DR: 提出了一种基于扩散的自适应负采样方法DANS-KGC，用于知识图谱补全，通过动态调整负样本的难度分布提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有负采样策略存在假阴性、泛化能力有限和样本难度不可控等问题，亟需改进。

Method: DANS-KGC包含三个模块：难度评估模块（DAM）、自适应负采样模块（ANS）和动态训练机制（DTM），结合语义和结构特征生成多样化的负样本。

Result: 在六个基准数据集上验证了DANS-KGC的有效性，尤其是在UMLS和YAGO3-10数据集上取得了最优结果。

Conclusion: DANS-KGC通过动态调整负样本难度，显著提升了知识图谱补全任务的性能，表现出良好的泛化能力。

Abstract: Negative sampling (NS) strategies play a crucial role in knowledge graph representation. In order to overcome the limitations of existing negative sampling strategies, such as vulnerability to false negatives, limited generalization, and lack of control over sample hardness, we propose DANS-KGC (Diffusion-based Adaptive Negative Sampling for Knowledge Graph Completion). DANS-KGC comprises three key components: the Difficulty Assessment Module (DAM), the Adaptive Negative Sampling Module (ANS), and the Dynamic Training Mechanism (DTM). DAM evaluates the learning difficulty of entities by integrating semantic and structural features. Based on this assessment, ANS employs a conditional diffusion model with difficulty-aware noise scheduling, leveraging semantic and neighborhood information during the denoising phase to generate negative samples of diverse hardness. DTM further enhances learning by dynamically adjusting the hardness distribution of negative samples throughout training, enabling a curriculum-style progression from easy to hard examples. Extensive experiments on six benchmark datasets demonstrate the effectiveness and generalization ability of DANS-KGC, with the method achieving state-of-the-art results on all three evaluation metrics for the UMLS and YAGO3-10 datasets.

</details>


### [63] [Neurophysiological Characteristics of Adaptive Reasoning for Creative Problem-Solving Strategy](https://arxiv.org/abs/2511.07912)
*Jun-Young Kim,Young-Seok Kweon,Gi-Hwan Shin,Seong-Whan Lee*

Main category: cs.AI

TL;DR: 本文研究了人类适应性推理的神经机制，通过脑电图和多模态大型语言模型对比，揭示了人类特有的delta-theta-alpha协调动态，并指出人工智能需要类似的振荡反馈协调以实现真正的上下文敏感适应。


<details>
  <summary>Details</summary>
Motivation: 探索人类适应性推理的神经机制，并比较人类与多模态大型语言模型在动态推理任务中的表现差异。

Method: 采用卡片分类范式结合脑电图技术，分析刺激和反馈锁时的神经活动，并与多模态大型语言模型的表现对比。

Result: 人类表现出delta-theta-alpha协调动态，而模型仅能进行短时反馈驱动调整，缺乏真正的适应性推理能力。

Conclusion: 人类适应性推理具有独特的神经特征，未来的脑启发人工智能需整合振荡反馈协调以实现上下文敏感适应。

Abstract: Adaptive reasoning enables humans to flexibly adjust inference strategies when environmental rules or contexts change, yet its underlying neural dynamics remain unclear. This study investigated the neurophysiological mechanisms of adaptive reasoning using a card-sorting paradigm combined with electroencephalography and compared human performance with that of a multimodal large language model. Stimulus- and feedback-locked analyses revealed coordinated delta-theta-alpha dynamics: early delta-theta activity reflected exploratory monitoring and rule inference, whereas occipital alpha engagement indicated confirmatory stabilization of attention after successful rule identification. In contrast, the multimodal large language model exhibited only short-term feedback-driven adjustments without hierarchical rule abstraction or genuine adaptive reasoning. These findings identify the neural signatures of human adaptive reasoning and highlight the need for brain-inspired artificial intelligence that incorporates oscillatory feedback coordination for true context-sensitive adaptation.

</details>


### [64] [Lightweight Diffusion-based Framework for Online Imagined Speech Decoding in Aphasia](https://arxiv.org/abs/2511.07920)
*Eunyeong Ko,Soowon Kim,Ha-Na Jo*

Main category: cs.AI

TL;DR: 提出了一种基于扩散模型的神经解码框架，用于实时分类失语症患者的想象语音，通过轻量级条件扩散编码器和卷积分类器实现了高准确性，并在实际临床环境中验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 为解决失语症患者因表达性语言障碍而面临的沟通困难，开发一种能够在临床约束条件下稳定运行的想象语音脑机接口。

Method: 结合轻量级条件扩散编码器和卷积分类器，使用特定受试者的脑电图数据进行训练，采用双标准早停策略和正则化技术优化模型性能。

Result: 在线测试中，框架的准确率达到65%（top-1）和70%（top-2），显著优于离线评估结果（50% top-1）。

Conclusion: 该框架在临床环境中表现出可靠性能，为想象语音脑机接口的实际应用提供了重要进展。

Abstract: A diffusion-based neural decoding framework optimized for real-time imagined speech classification in individuals with aphasia. The system integrates a lightweight conditional diffusion encoder and convolutional classifier trained using subject-specific EEG data acquired from a Korean-language paradigm. A dual-criterion early stopping strategy enabled rapid convergence under limited calibration data, while dropout regularization and grouped temporal convolutions ensured stable generalization. During online operation, continuous EEG streams were processed in two-second sliding windows to generate class probabilities that dynamically modulated visual and auditory feedback according to decoding confidence. Across twenty real-time trials, the framework achieved 65% top-1 and 70% top-2 accuracy, outperforming offline evaluation (50% top-1). These results demonstrate the feasibility of deploying diffusion-based EEG decoding under practical clinical constraints, maintaining reliable performance despite environmental variability and minimal preprocessing. The proposed framework advances the translation of imagined speech brain-computer interfaces toward clinical communication support for individuals with severe expressive language impairment.

</details>


### [65] [Computational Blueprints: Generating Isomorphic Mathematics Problems with Large Language Models](https://arxiv.org/abs/2511.07932)
*Jeong-Hoon Kim,Jinwoo Nam,Geunsik Jo*

Main category: cs.AI

TL;DR: 该论文提出了一个名为同构数学问题生成（IMPG）的新任务，并开发了基于LLM的框架CBIT，用于自动生成数学问题的变体，具有高正确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 个性化数学教育需求增长，现有研究主要关注数据增强而非直接教育应用，因此需要生成结构一致的数学问题变体。

Method: 通过逐步优化的LLM框架CBIT，结合元级生成和基于模板的选择性变体，实现高效的数学问题生成。

Result: CBIT在生成准确性和成本效益上表现优异，生成的题目错误率比专家编写的低17.8%，并在商业教育平台上成功部署。

Conclusion: CBIT为数学教育提供了一个高效且准确的问题生成工具，具有实际应用潜力。

Abstract: Personalized mathematics education is growing rapidly, creating a strong demand for large sets of similar practice problems. Yet existing studies on mathematics problem generation have focused on data augmentation for training neural language models rather than on direct educational deployment. To bridge this gap, we define a new task, Isomorphic Math Problem Generation (IMPG), designed to produce structurally consistent variants of source problems. Subsequently, we explored LLM-based frameworks for automatic IMPG through successive refinements, and established Computational Blueprints for Isomorphic Twins (CBIT). With meta-level generation and template-based selective variation, CBIT achieves high mathematical correctness and structural consistency while reducing the cost of generation. Empirical results across refinements demonstrate that CBIT is superior on generation accuracy and cost-effectiveness at scale. Most importantly, CBIT-generated problems exhibited an error rate 17.8% lower than expert-authored items, with deployment to 6,732 learners on a commercial education platform yielding 186,870 interactions.

</details>


### [66] [Toward Practical BCI: A Real-time Wireless Imagined Speech EEG Decoding System](https://arxiv.org/abs/2511.07936)
*Ji-Ha Park,Heon-Gyu Kwak,Gi-Hwan Shin,Yoo-In Jeon,Sun-Min Park,Ji-Yeon Hwang,Seong-Whan Lee*

Main category: cs.AI

TL;DR: 该论文提出了一种实时无线想象语音脑电图（EEG）解码系统，旨在推动脑机接口（BCI）技术在日常生活中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 当前BCI研究多局限于静态和固定环境，限制了其实际应用。为开发更实用的BCI技术，作者设计了一种灵活的无线EEG解码系统。

Method: 系统采用实验室流层（LSL）管理实时EEG信号流，结合用户识别模块提供个性化服务。支持从有线EEG设备扩展到便携式无线硬件。

Result: 该系统在4类任务中实现了有线设备62.00%和无线设备46.67%的准确率。

Conclusion: 该研究为实用化和个性化神经接口技术的发展提供了重要方向。

Abstract: Brain-computer interface (BCI) research, while promising, has largely been confined to static and fixed environments, limiting real-world applicability. To move towards practical BCI, we introduce a real-time wireless imagined speech electroencephalogram (EEG) decoding system designed for flexibility and everyday use. Our framework focuses on practicality, demonstrating extensibility beyond wired EEG devices to portable, wireless hardware. A user identification module recognizes the operator and provides a personalized, user-specific service. To achieve seamless, real-time operation, we utilize the lab streaming layer to manage the continuous streaming of live EEG signals to the personalized decoder. This end-to-end pipeline enables a functional real-time application capable of classifying user commands from imagined speech EEG signals, achieving an overall 4-class accuracy of 62.00 % on a wired device and 46.67 % on a portable wireless headset. This paper demonstrates a significant step towards truly practical and accessible BCI technology, establishing a clear direction for future research in robust, practical, and personalized neural interfaces.

</details>


### [67] [Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction](https://arxiv.org/abs/2511.07943)
*Jun Xu,Xinkai Du,Yu Ao,Peilong Zhao,Yang Li,Ling Zhong,Lin Yuan,Zhongpu Bo,Xiaorui Wang,Mengshu Sun,Zhengke Gui,Dalong Zhang,Zhaoyang Wang,Qiwei Wang,Yangyang Hou,Zhiying Yin,Haofen Wang,Huajun Chen,Lei Liang,Jun Zhou*

Main category: cs.AI

TL;DR: Thinker是一个分层思维模型，通过多轮交互进行深度搜索，增强LLM的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了对推理过程的监督，导致逻辑一致性和严谨性不足，需改进这一问题。

Method: 将复杂问题分解为子问题，用自然语言和逻辑函数双重表示，并通过逻辑函数传递依赖关系。

Result: 实验表明，Thinker在少量训练样本下与基线方法竞争，全量训练时显著优于其他方法。

Conclusion: Thinker使推理过程可监督和验证，同时提升逻辑一致性，显著优于传统方法。

Abstract: Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.

</details>


### [68] [TimeFlow: Towards Stochastic-Aware and Efficient Time Series Generation via Flow Matching Modeling](https://arxiv.org/abs/2511.07968)
*He Panjing,Cheng Mingyue,Li Li,Zhang XiaoHan*

Main category: cs.AI

TL;DR: TimeFlow是一种基于SDE的流匹配框架，通过组件分解的速度场和随机项优化，高效生成高质量时间序列数据，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据的随机性和局部变化是生成高质量数据的挑战，当前扩散模型效率低，流匹配框架的ODE公式无法明确捕捉随机性。

Method: 提出了TimeFlow框架，结合SDE和编码器架构，设计了组件分解的速度场，并引入随机项增强表达能力。

Result: 在多个数据集上的实验表明，TimeFlow在生成质量、多样性和效率上均优于基线方法。

Conclusion: TimeFlow是一种灵活通用的框架，适用于条件和非条件生成任务，显著提升了时间序列数据的生成效果。

Abstract: Generating high-quality time series data has emerged as a critical research topic due to its broad utility in supporting downstream time series mining tasks. A major challenge lies in modeling the intrinsic stochasticity of temporal dynamics, as real-world sequences often exhibit random fluctuations and localized variations. While diffusion models have achieved remarkable success, their generation process is computationally inefficient, often requiring hundreds to thousands of expensive function evaluations per sample. Flow matching has emerged as a more efficient paradigm, yet its conventional ordinary differential equation (ODE)-based formulation fails to explicitly capture stochasticity, thereby limiting the fidelity of generated sequences. By contrast, stochastic differential equation (SDE) are naturally suited for modeling randomness and uncertainty. Motivated by these insights, we propose TimeFlow, a novel SDE-based flow matching framework that integrates a encoder-only architecture. Specifically, we design a component-wise decomposed velocity field to capture the multi-faceted structure of time series and augment the vanilla flow-matching optimization with an additional stochastic term to enhance representational expressiveness. TimeFlow is flexible and general, supporting both unconditional and conditional generation tasks within a unified framework. Extensive experiments across diverse datasets demonstrate that our model consistently outperforms strong baselines in generation quality, diversity, and efficiency.

</details>


### [69] [Versatile and Risk-Sensitive Cardiac Diagnosis via Graph-Based ECG Signal Representation](https://arxiv.org/abs/2511.07973)
*Yue Wang,Yuyang Xu,Renjun Hu,Fanqi Shen,Hanyun Jiang,Jun Wang,Jintai Chen,Danny Z. Chen,Jian Wu,Haochao Ying*

Main category: cs.AI

TL;DR: VARS是一种基于图模型的方法，用于解决心电图信号处理中的多样性和风险信号检测不足问题，通过图结构和对比学习提升诊断能力。


<details>
  <summary>Details</summary>
Motivation: 解决心电图信号诊断中因信号多样性（导联数、采样频率、持续时间）和样本不平衡导致的风险信号检测不足问题。

Method: VARS将ECG信号转换为通用的图结构，结合去噪重建和对比学习，保留原始信息并突出关键诊断特征。

Result: VARS在三个ECG数据集上表现优异，优于现有方法，且能准确定位异常ECG波形，提供可解释性。

Conclusion: VARS有望成为心脏健康评估的强大工具，兼具性能和临床实用性。

Abstract: Despite the rapid advancements of electrocardiogram (ECG) signal diagnosis and analysis methods through deep learning, two major hurdles still limit their clinical adoption: the lack of versatility in processing ECG signals with diverse configurations, and the inadequate detection of risk signals due to sample imbalances. Addressing these challenges, we introduce VersAtile and Risk-Sensitive cardiac diagnosis (VARS), an innovative approach that employs a graph-based representation to uniformly model heterogeneous ECG signals. VARS stands out by transforming ECG signals into versatile graph structures that capture critical diagnostic features, irrespective of signal diversity in the lead count, sampling frequency, and duration. This graph-centric formulation also enhances diagnostic sensitivity, enabling precise localization and identification of abnormal ECG patterns that often elude standard analysis methods. To facilitate representation transformation, our approach integrates denoising reconstruction with contrastive learning to preserve raw ECG information while highlighting pathognomonic patterns. We rigorously evaluate the efficacy of VARS on three distinct ECG datasets, encompassing a range of structural variations. The results demonstrate that VARS not only consistently surpasses existing state-of-the-art models across all these datasets but also exhibits substantial improvement in identifying risk signals. Additionally, VARS offers interpretability by pinpointing the exact waveforms that lead to specific model outputs, thereby assisting clinicians in making informed decisions. These findings suggest that our VARS will likely emerge as an invaluable tool for comprehensive cardiac health assessment.

</details>


### [70] [Towards Fine-Grained Interpretability: Counterfactual Explanations for Misclassification with Saliency Partition](https://arxiv.org/abs/2511.07974)
*Lintong Zhang,Kang Yin,Seong-Whan Lee*

Main category: cs.AI

TL;DR: 论文提出了一种细粒度反事实解释框架，通过生成对象级和部分级可解释性，解决模型误分类中缺乏细节的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于归因的解释技术缺乏足够的细粒度信息，尤其在模型误分类时难以提供详细解释。

Method: 提出了一种非生成式方法，通过量化相似性和权重贡献生成反事实解释，并引入基于Shapley值的显著性分区模块。

Result: 实验表明，该方法在捕捉细粒度、直观有意义的区域方面表现优越，超越了其他细粒度方法。

Conclusion: 该框架有效提升了模型解释的细粒度和可理解性，尤其在处理误分类任务时表现出色。

Abstract: Attribution-based explanation techniques capture key patterns to enhance visual interpretability; however, these patterns often lack the granularity needed for insight in fine-grained tasks, particularly in cases of model misclassification, where explanations may be insufficiently detailed. To address this limitation, we propose a fine-grained counterfactual explanation framework that generates both object-level and part-level interpretability, addressing two fundamental questions: (1) which fine-grained features contribute to model misclassification, and (2) where dominant local features influence counterfactual adjustments. Our approach yields explainable counterfactuals in a non-generative manner by quantifying similarity and weighting component contributions within regions of interest between correctly classified and misclassified samples. Furthermore, we introduce a saliency partition module grounded in Shapley value contributions, isolating features with region-specific relevance. Extensive experiments demonstrate the superiority of our approach in capturing more granular, intuitively meaningful regions, surpassing fine-grained methods.

</details>


### [71] [Benchmarking Multi-Step Legal Reasoning and Analyzing Chain-of-Thought Effects in Large Language Models](https://arxiv.org/abs/2511.07979)
*Wenhan Yu,Xinbo Lin,Lanxin Ni,Jinhua Cheng,Lei Sha*

Main category: cs.AI

TL;DR: 论文提出了首个基于真实司法决策的中文多步法律推理数据集MSLR，采用IRAC框架建模专家推理，并设计了可扩展的人机协作标注流程，评估显示LLMs在法律推理上仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 现有法律基准常混淆事实记忆与真实推理，且忽视了推理质量，因此需开发更贴合真实法律推理的数据集和方法。

Method: 引入MSLR数据集，采用IRAC框架建模推理，并设计人机协作标注流程生成细粒度推理注释。

Result: 评估显示LLMs在MSLR上表现一般，但自主生成的CoT提示能提升推理质量。

Conclusion: MSLR推动了LLM推理和CoT策略的发展，为未来研究提供了开放资源。

Abstract: Large language models (LLMs) have demonstrated strong reasoning abilities across specialized domains, motivating research into their application to legal reasoning. However, existing legal benchmarks often conflate factual recall with genuine inference, fragment the reasoning process, and overlook the quality of reasoning. To address these limitations, we introduce MSLR, the first Chinese multi-step legal reasoning dataset grounded in real-world judicial decision making. MSLR adopts the IRAC framework (Issue, Rule, Application, Conclusion) to model structured expert reasoning from official legal documents. In addition, we design a scalable Human-LLM collaborative annotation pipeline that efficiently produces fine-grained step-level reasoning annotations and provides a reusable methodological framework for multi-step reasoning datasets. Evaluation of multiple LLMs on MSLR shows only moderate performance, highlighting the challenges of adapting to complex legal reasoning. Further experiments demonstrate that Self-Initiated Chain-of-Thought prompts generated by models autonomously improve reasoning coherence and quality, outperforming human-designed prompts. MSLR contributes to advancing LLM reasoning and Chain-of-Thought strategies and offers open resources for future research. The dataset and code are available at https://github.com/yuwenhan07/MSLR-Bench and https://law.sjtu.edu.cn/flszyjzx/index.html.

</details>


### [72] [Capturing Complex Spatial-Temporal Dependencies in Traffic Forecasting: A Self-Attention Approach](https://arxiv.org/abs/2511.07980)
*Zheng Chenghong,Zongyin Deng,Liu Cheng,Xiong Simin,Di Deshi,Li Guanyao*

Main category: cs.AI

TL;DR: 提出了一种名为ST-SAM的时空自注意力模型，用于交通流量预测，通过联合学习时空依赖性显著提升了预测准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的交通流量预测方法通常将时空依赖性分开研究，未能捕捉其联合效应，导致预测效果受限。

Method: 采用区域嵌入层和基于自注意力机制的时空依赖学习模块，联合捕捉邻近和远距离区域的时空依赖性。

Result: 在两个真实数据集上的实验表明，ST-SAM在预测准确性和效率上显著优于现有方法（RMSE平均提升15%，MAPE提升17%，训练时间缩短32倍）。

Conclusion: ST-SAM通过自注意力机制有效捕捉时空依赖性，为交通流量预测提供了更高效和准确的解决方案。

Abstract: We study the problem of traffic forecasting, aiming to predict the inflow and outflow of a region in the subsequent time slot. The problem is complex due to the intricate spatial and temporal interdependence among regions. Prior works study the spatial and temporal dependency in a decouple manner, failing to capture their joint effect. In this work, we propose ST-SAM, a novel and efficient Spatial-Temporal Self-Attention Model for traffic forecasting. ST-SAM uses a region embedding layer to learn time-specific embedding from traffic data for regions. Then, it employs a spatial-temporal dependency learning module based on self-attention mechanism to capture the joint spatial-temporal dependency for both nearby and faraway regions. ST-SAM entirely relies on self-attention to capture both local and global spatial-temporal correlations, which make it effective and efficient. Extensive experiments on two real world datasets show that ST-SAM is substantially more accurate and efficient than the state-of-the-art approaches (with an average improvement of up to 15% on RMSE, 17% on MAPE, and 32 times on training time in our experiments).

</details>


### [73] [The One Where They Brain-Tune for Social Cognition: Multi-Modal Brain-Tuning on Friends](https://arxiv.org/abs/2511.07988)
*Nico Policzer,Cameron Braunstein,Mariya Toneva*

Main category: cs.AI

TL;DR: 该研究将音频模型的大脑调谐扩展到多模态（音频-视频）领域，针对社交认知脑区STS进行调谐，显著提高了大脑对齐效果，并改善了与训练数据相关的下游任务（情景喜剧中的讽刺检测）表现。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过多模态大脑调谐方法（音频-视频）增强社交认知能力，特别是针对社交处理的关键脑区STS，以验证其在相关任务中的应用效果。

Method: 扩展音频模型的大脑调谐方法至多模态（音频-视频）模型，针对STS及相邻ROI进行调谐，并使用情景喜剧《Friends》作为训练数据。

Result: 结果显示，STS及相邻ROI的大脑对齐效果显著提升，同时在情景喜剧讽刺检测这一社交认知任务中表现改善。

Conclusion: 该研究成功将大脑调谐方法扩展到多模态领域，并证明其对相关功能脑区调谐后可提升下游任务表现。

Abstract: Recent studies on audio models show brain-tuning - fine-tuning models to better predict corresponding fMRI activity - improves brain alignment and increases performance on downstream semantic and audio tasks. We extend this approach to a multimodal audio-video model to enhance social cognition, targeting the Superior Temporal Sulcus (STS), a key region for social processing, while subjects watch Friends. We find significant increases in brain alignment to the STS and an adjacent ROI, as well as improvements to a social cognition task related to the training data - sarcasm detection in sitcoms. In summary, our study extends brain-tuning to the multi-modal domain, demonstrating improvements to a downstream task after tuning to a relevant functional region.

</details>


### [74] [VSPO: Validating Semantic Pitfalls in Ontology via LLM-Based CQ Generation](https://arxiv.org/abs/2511.07991)
*Hyojun Choi,Seokju Hwang,Kyong-Ho Lee*

Main category: cs.AI

TL;DR: 论文提出了一种名为VSPO的新方法，用于自动生成能力问题（CQs）以验证本体设计中的语义陷阱，通过微调LLaMA模型实现了比GPT-4.1更高的准确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 手动设计能力问题耗时且成本高，现有方法难以验证语义陷阱，如误用“allValuesFrom”。

Method: 使用LLMs生成自然语言定义并故意引入与本体不符的语义偏差，然后微调LLaMA-3.1-8B-Instruct生成验证这些偏差的CQs。

Result: 微调模型表现优于基线，生成CQs的精确率和召回率分别比GPT-4.1高26%和28.2%，能检测更广泛的建模错误。

Conclusion: 该方法能自动生成验证TBox的CQs，显著减少人工工作量并提高本体与专家知识的语义对齐。

Abstract: Competency Questions (CQs) play a crucial role in validating ontology design. While manually crafting CQs can be highly time-consuming and costly for ontology engineers, recent studies have explored the use of large language models (LLMs) to automate this process. However, prior approaches have largely evaluated generated CQs based on their similarity to existing datasets, which often fail to verify semantic pitfalls such as "Misusing allValuesFrom". Since such pitfalls cannot be reliably detected through rule-based methods, we propose a novel dataset and model of Validating Semantic Pitfalls in Ontology (VSPO) for CQ generation specifically designed to verify the semantic pitfalls. To simulate missing and misused axioms, we use LLMs to generate natural language definitions of classes and properties and introduce misalignments between the definitions and the ontology by removing axioms or altering logical operators (e.g., substituting union with intersection). We then fine-tune LLaMA-3.1-8B-Instruct to generate CQs that validate these semantic discrepancies between the provided definitions and the corresponding axioms. The resulting CQs can detect a broader range of modeling errors compared to existing public datasets. Our fine-tuned model demonstrates superior performance over baselines, showing 26% higher precision and 28.2% higher recall than GPT-4.1 in generating CQs for pitfall validation. This research enables automatic generation of TBox-validating CQs using LLMs, significantly reducing manual effort while improving semantic alignment between ontologies and expert knowledge. To the best of our knowledge, this is the first study to target semantic pitfall validation in CQ generation using LLMs.

</details>


### [75] [Multivariate Time series Anomaly Detection:A Framework of Hidden Markov Models](https://arxiv.org/abs/2511.07995)
*Jinbo Li,Witold Pedrycz,Iqbal Jamal*

Main category: cs.AI

TL;DR: 研究开发了一种将多变量时间序列转换为单变量时间序列的异常检测方法，结合了FCM聚类和模糊积分技术，并通过HMM进行异常检测，比较了多种转换方法的效果。


<details>
  <summary>Details</summary>
Motivation: 针对多变量时间序列异常检测的复杂性，提出一种通过转换简化为单变量问题的方法，以提高检测效率和准确性。

Method: 采用Fuzzy C-Means聚类和模糊积分技术将多变量时间序列转换为单变量，然后利用HMM构建异常检测器，并比较不同转换方法的性能。

Result: 实验研究和对比分析表明，所提出的方法在多变量时间序列异常检测中具有有效性和实用性。

Conclusion: 通过转换和HMM的结合，该方法为多变量时间序列异常检测提供了一种有效的解决方案。

Abstract: In this study, we develop an approach to multivariate time series anomaly detection focused on the transformation of multivariate time series to univariate time series. Several transformation techniques involving Fuzzy C-Means (FCM) clustering and fuzzy integral are studied. In the sequel, a Hidden Markov Model (HMM), one of the commonly encountered statistical methods, is engaged here to detect anomalies in multivariate time series. We construct HMM-based anomaly detectors and in this context compare several transformation methods. A suite of experimental studies along with some comparative analysis is reported.

</details>


### [76] [Numerical Sensitivity and Robustness: Exploring the Flaws of Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2511.08022)
*Zhishen Sun,Guang Dai,Ivor Tsang,Haishan Ye*

Main category: cs.AI

TL;DR: 本文提出了一种新的扰动框架，通过添加语义无关的扰动句子和逐步增加扰动强度，评估大型语言模型（LLMs）在复杂环境中的数学推理能力，揭示了LLMs在面对数值信息扰动时的脆弱性及其潜在缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在数学推理方面取得显著进展，但其是否真正具备数学理解能力仍存在争议。为了探讨这一问题，作者通过扰动实验分析LLMs的推理能力及其局限性。

Method: 设计了一个新的扰动框架，包括注入语义无关的扰动句子、逐步增加扰动强度，以及采用核心提问指令缺失的方法，进一步分析LLMs的问题解决机制。

Result: 实验结果显示，LLMs在面对无数字的扰动句子时表现稳定，但性能随扰动强度增加而下降；面对含数字的扰动句子时性能下降更显著，部分模型最大降幅达51.55%。此外，即使在核心指令缺失时，LLMs仍保持20%-40%的准确率。

Conclusion: LLMs在数学推理中存在对数值信息扰动的敏感性和依赖记忆模板的倾向。此研究揭示了LLMs的局限性，对其进一步发展具有重要意义。

Abstract: LLMs have made significant progress in the field of mathematical reasoning, but whether they have true the mathematical understanding ability is still controversial. To explore this issue, we propose a new perturbation framework to evaluate LLMs' reasoning ability in complex environments by injecting additional semantically irrelevant perturbation sentences and gradually increasing the perturbation intensity. At the same time, we use an additional perturbation method: core questioning instruction missing, to further analyze the LLMs' problem-solving mechanism. The experimental results show that LLMs perform stably when facing perturbation sentences without numbers, but there is also a robustness boundary. As the perturbation intensity increases, the performance exhibits varying degrees of decline; when facing perturbation sentences with numbers, the performance decreases more significantly, most open source models with smaller parameters decrease by nearly or even more than 10%, and further increasing with the enhancement of perturbation intensity, with the maximum decrease reaching 51.55%. Even the most advanced commercial LLMs have seen a 3%-10% performance drop. By analyzing the reasoning process of LLMs in detail, We find that models are more sensitive to perturbations with numerical information and are more likely to give incorrect answers when disturbed by irrelevant numerical information. The higher the perturbation intensity, the more obvious these defects are. At the same time, in the absence of core questioning instruction, models can still maintain an accuracy of 20%-40%, indicating that LLMs may rely on memory templates or pattern matching to complete the task, rather than logical reasoning. In general, our work reveals the shortcomings and limitations of current LLMs in their reasoning capabilities, which is of great significance for the further development of LLMs.

</details>


### [77] [Knowledge-Augmented Long-CoT Generation for Complex Biomolecular Reasoning](https://arxiv.org/abs/2511.08024)
*Tianwen Lyu,Xiang Zhuang,Keyan Ding,Xinzhe Cao,Lei Liang,Wei Zhao,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: 提出了一种结合知识图谱和多跳推理链的框架，用于提升生物分子问题的推理能力，并在新数据集上验证其效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生物分子问题上存在逻辑不一致和缺乏领域知识基础的问题，需要改进。

Method: 提出Knowledge-Augmented Long-CoT Reasoning框架，结合知识图谱和多跳推理链，并通过监督微调和强化学习优化。

Result: 在新数据集PrimeKGQA和现有数据集上验证，本方法在多跳推理任务上表现优异。

Conclusion: 结构化知识与高级推理策略的结合对可靠且可解释的生物分子推理有效。

Abstract: Understanding complex biomolecular mechanisms requires multi-step reasoning across molecular interactions, signaling cascades, and metabolic pathways. While large language models(LLMs) show promise in such tasks, their application to biomolecular problems is hindered by logical inconsistencies and the lack of grounding in domain knowledge. Existing approaches often exacerbate these issues: reasoning steps may deviate from biological facts or fail to capture long mechanistic dependencies. To address these challenges, we propose a Knowledge-Augmented Long-CoT Reasoning framework that integrates LLMs with knowledge graph-based multi-hop reasoning chains. The framework constructs mechanistic chains via guided multi-hop traversal and pruning on the knowledge graph; these chains are then incorporated into supervised fine-tuning to improve factual grounding and further refined with reinforcement learning to enhance reasoning reliability and consistency. Furthermore, to overcome the shortcomings of existing benchmarks, which are often restricted in scale and scope and lack annotations for deep reasoning chains, we introduce PrimeKGQA, a comprehensive benchmark for biomolecular question answering. Experimental results on both PrimeKGQA and existing datasets demonstrate that although larger closed-source models still perform well on relatively simple tasks, our method demonstrates clear advantages as reasoning depth increases, achieving state-of-the-art performance on multi-hop tasks that demand traversal of structured biological knowledge. These findings highlight the effectiveness of combining structured knowledge with advanced reasoning strategies for reliable and interpretable biomolecular reasoning.

</details>


### [78] [Dual-Process Scaffold Reasoning for Enhancing LLM Code Debugging](https://arxiv.org/abs/2511.08052)
*Po-Chung Hsieh,Chin-Po Chen,Jeng-Lin Li,Ming-Ching Chang*

Main category: cs.AI

TL;DR: 论文提出了一种基于心理学理论的新型Scaffold Reasoning框架，用于代码调试，通过优化认知路径提高推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的推理步骤在复杂性和计算效率之间的平衡尚不明确，且对System 2推理的深入研究不足。

Method: 提出Scaffold Reasoning框架，包含Scaffold Stream、Analytic Stream和Integration Stream，整合参考代码构建与错误代码分析。

Result: 在DebugBench上达到88.91%的通过率和平均5.36秒的推理时间，优于其他推理方法。

Conclusion: 该框架与人类认知过程一致，展示了在不同问题难度和错误类型上的优势与局限。

Abstract: Recent LLMs have demonstrated sophisticated problem-solving capabilities on various benchmarks through advanced reasoning algorithms. However, the key research question of identifying reasoning steps that balance complexity and computational efficiency remains unsolved. Recent research has increasingly drawn upon psychological theories to explore strategies for optimizing cognitive pathways. The LLM's final outputs and intermediate steps are regarded as System 1 and System 2, respectively. However, an in-depth exploration of the System 2 reasoning is still lacking. Therefore, we propose a novel psychologically backed Scaffold Reasoning framework for code debugging, which encompasses the Scaffold Stream, Analytic Stream, and Integration Stream. The construction of reference code within the Scaffold Stream is integrated with the buggy code analysis results produced by the Analytic Stream through the Integration Stream. Our framework achieves an 88.91% pass rate and an average inference time of 5.36 seconds per-problem on DebugBench, outperforming other reasoning approaches across various LLMs in both reasoning accuracy and efficiency. Further analyses elucidate the advantages and limitations of various cognitive pathways across varying problem difficulties and bug types. Our findings also corroborate the alignment of the proposed Scaffold Reasoning framework with human cognitive processes.

</details>


### [79] [Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression](https://arxiv.org/abs/2511.08066)
*Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 本文提出了一种基于文本压缩性能与计算复杂度的模型效率衡量标准“信息容量”，用于公平比较不同模型系列的效率，并准确预测模型系列内的性能。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLM）的广泛应用和对计算资源的高需求，缺乏一个统一的指标来衡量不同模型规模和架构的效率。

Method: 通过信息容量这一指标，结合文本压缩性能和计算复杂度，评估模型效率，并将其应用于主流开源模型。

Result: 实验证明，信息容量能够在不同模型系列间公平比较效率，并在同一模型系列内准确预测性能。

Conclusion: 信息容量是一个有效的模型效率衡量标准，能够统一评估LLM的效率，并考虑了通常被忽视的分词器效率等因素。

Abstract: Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture.

</details>


### [80] [Clustering-based Anomaly Detection in Multivariate Time Series Data](https://arxiv.org/abs/2511.08072)
*Jinbo Li,Hesam Izakian,Witold Pedrycz,Iqbal Jamal*

Main category: cs.AI

TL;DR: 本文提出了一种基于聚类的方法，用于检测多元时间序列中的振幅和形状异常，通过滑动窗口、扩展模糊聚类和重建准则实现，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列数据在许多科学和工程领域具有广泛应用，但如何同时考虑时间和变量关系来检测异常仍是一个具有挑战性的问题。

Method: 使用滑动窗口生成多元子序列，应用扩展模糊聚类获取子序列结构，通过重建准则和置信指数量化异常，并利用粒子群优化进行优化。

Result: 实验表明，该方法能在多元时间序列中有效检测异常，适用于医疗、天气数据分析、金融和疾病爆发检测等领域。

Conclusion: 该框架通过聚类分析成功检测多元时间序列中的异常，尤其是振幅和形状模式，具有广泛的应用潜力。

Abstract: Multivariate time series data come as a collection of time series describing different aspects of a certain temporal phenomenon. Anomaly detection in this type of data constitutes a challenging problem yet with numerous applications in science and engineering because anomaly scores come from the simultaneous consideration of the temporal and variable relationships. In this paper, we propose a clustering-based approach to detect anomalies concerning the amplitude and the shape of multivariate time series. First, we use a sliding window to generate a set of multivariate subsequences and thereafter apply an extended fuzzy clustering to reveal a structure present within the generated multivariate subsequences. Finally, a reconstruction criterion is employed to reconstruct the multivariate subsequences with the optimal cluster centers and the partition matrix. We construct a confidence index to quantify a level of anomaly detected in the series and apply Particle Swarm Optimization as an optimization vehicle for the problem of anomaly detection. Experimental studies completed on several synthetic and six real-world datasets suggest that the proposed methods can detect the anomalies in multivariate time series. With the help of available clusters revealed by the extended fuzzy clustering, the proposed framework can detect anomalies in the multivariate time series and is suitable for identifying anomalous amplitude and shape patterns in various application domains such as health care, weather data analysis, finance, and disease outbreak detection.

</details>


### [81] [Prudential Reliability of Large Language Models in Reinsurance: Governance, Assurance, and Capital Efficiency](https://arxiv.org/abs/2511.08082)
*Stella C. Dong*

Main category: cs.AI

TL;DR: 本文提出了一个评估大型语言模型（LLMs）在再保险中可靠性的审慎框架，通过五支柱架构将监管要求转化为可衡量的生命周期控制，并实现了更高的准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为大型语言模型在再保险领域的应用开发一个可靠性评估框架，以满足监管要求和实际需求。

Method: 采用五支柱架构（治理、数据谱系、保障、弹性和监管一致性）构建审慎框架，并通过再保险AI可靠性与保障基准（RAIRAB）进行评估。

Result: 在六类任务中，检索式模型配置提高了准确性（0.90），减少了40%的幻觉和解释偏差，并显著提升了透明度。

Conclusion: 研究表明，当治理明确、数据可追溯且保障可验证时，现有审慎原则已能支持可靠的AI应用。

Abstract: This paper develops a prudential framework for assessing the reliability of large language models (LLMs) in reinsurance. A five-pillar architecture--governance, data lineage, assurance, resilience, and regulatory alignment--translates supervisory expectations from Solvency II, SR 11-7, and guidance from EIOPA (2025), NAIC (2023), and IAIS (2024) into measurable lifecycle controls. The framework is implemented through the Reinsurance AI Reliability and Assurance Benchmark (RAIRAB), which evaluates whether governance-embedded LLMs meet prudential standards for grounding, transparency, and accountability. Across six task families, retrieval-grounded configurations achieved higher grounding accuracy (0.90), reduced hallucination and interpretive drift by roughly 40%, and nearly doubled transparency. These mechanisms lower informational frictions in risk transfer and capital allocation, showing that existing prudential doctrines already accommodate reliable AI when governance is explicit, data are traceable, and assurance is verifiable.

</details>


### [82] [Gateways to Tractability for Satisfiability in Pearl's Causal Hierarchy](https://arxiv.org/abs/2511.08091)
*Robert Ganian,Marlene Gründel,Simon Wietheger*

Main category: cs.AI

TL;DR: 该论文通过参数化复杂性的视角研究了Pearl的因果层次（PCH）的可满足性问题，提出了首个可处理性的途径，包括固定参数和XP算法，并匹配了硬度结果。


<details>
  <summary>Details</summary>
Motivation: PCH的公式可满足性问题在经典设置中计算上难以处理，作者试图通过参数化复杂性寻找可处理性。

Method: 利用原始树宽和变量数量等参数，结合动态编程和结构表征，提出新算法工具包。

Result: 提出了针对关键概率和反事实片段的固定参数和XP算法，并匹配了硬度结果。

Conclusion: 通过结构表征和参数化方法，为因果推理提供了新的可处理性途径。

Abstract: Pearl's Causal Hierarchy (PCH) is a central framework for reasoning about probabilistic, interventional, and counterfactual statements, yet the satisfiability problem for PCH formulas is computationally intractable in almost all classical settings. We revisit this challenge through the lens of parameterized complexity and identify the first gateways to tractability. Our results include fixed-parameter and XP-algorithms for satisfiability in key probabilistic and counterfactual fragments, using parameters such as primal treewidth and the number of variables, together with matching hardness results that map the limits of tractability. Technically, we depart from the dynamic programming paradigm typically employed for treewidth-based algorithms and instead exploit structural characterizations of well-formed causal models, providing a new algorithmic toolkit for causal reasoning.

</details>


### [83] [Improving Industrial Injection Molding Processes with Explainable AI for Quality Classification](https://arxiv.org/abs/2511.08108)
*Georg Rottenwalter,Marcel Tilly,Victor Owolabi*

Main category: cs.AI

TL;DR: 论文研究了通过可解释人工智能（XAI）技术减少特征对注塑件质量分类的影响，发现减少特征可提高泛化能力且保持分类性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习在工业质量控制中应用广泛，但模型复杂性和数据不完整性限制了其实用性。XAI为这些问题提供了解决方案。

Method: 使用SHAP、Grad-CAM和LIME分析LSTM模型中的特征重要性，将输入特征从19个减少到9个和6个，评估模型准确性、推理速度和可解释性之间的权衡。

Result: 减少特征可以提高泛化能力，同时保持高分类性能，推理速度略有提升。

Conclusion: 该方法为传感器能力有限的工业环境提供了更高效和可解释的机器学习应用方案。

Abstract: Machine learning is an essential tool for optimizing industrial quality control processes. However, the complexity of machine learning models often limits their practical applicability due to a lack of interpretability. Additionally, many industrial machines lack comprehensive sensor technology, making data acquisition incomplete and challenging. Explainable Artificial Intelligence offers a solution by providing insights into model decision-making and identifying the most relevant features for classification. In this paper, we investigate the impact of feature reduction using XAI techniques on the quality classification of injection-molded parts. We apply SHAP, Grad-CAM, and LIME to analyze feature importance in a Long Short-Term Memory model trained on real production data. By reducing the original 19 input features to 9 and 6, we evaluate the trade-off between model accuracy, inference speed, and interpretability. Our results show that reducing features can improve generalization while maintaining high classification performance, with an small increase in inference speed. This approach enhances the feasibility of AI-driven quality control, particularly for industrial settings with limited sensor capabilities, and paves the way for more efficient and interpretable machine learning applications in manufacturing.

</details>


### [84] [Advancements in synthetic data extraction for industrial injection molding](https://arxiv.org/abs/2511.08117)
*Georg Rottenwalter,Marcel Tilly,Christian Bielenberg,Katharina Obermeier*

Main category: cs.AI

TL;DR: 通过合成数据增强机器学习模型的训练效果，特别适用于注塑成型过程，展示了减少人工、机器使用和材料浪费的潜力。


<details>
  <summary>Details</summary>
Motivation: 工业过程中数据获取成本高且耗时，合成数据可以解决数据不足问题，提升模型鲁棒性。

Method: 模拟生产周期生成合成数据，并通过不同比例的合成数据迭代实验，寻找最优平衡。

Result: 合成数据的加入提高了模型处理不同场景的能力，具有实际工业应用潜力。

Conclusion: 该方法为数据采集昂贵或不切实际的情况提供了实用替代方案，有望提升未来制造效率。

Abstract: Machine learning has significant potential for optimizing various industrial processes. However, data acquisition remains a major challenge as it is both time-consuming and costly. Synthetic data offers a promising solution to augment insufficient data sets and improve the robustness of machine learning models. In this paper, we investigate the feasibility of incorporating synthetic data into the training process of the injection molding process using an existing Long Short-Term Memory architecture. Our approach is to generate synthetic data by simulating production cycles and incorporating them into the training data set. Through iterative experimentation with different proportions of synthetic data, we attempt to find an optimal balance that maximizes the benefits of synthetic data while preserving the authenticity and relevance of real data. Our results suggest that the inclusion of synthetic data improves the model's ability to handle different scenarios, with potential practical industrial applications to reduce manual labor, machine use, and material waste. This approach provides a valuable alternative for situations where extensive data collection and maintenance has been impractical or costly and thus could contribute to more efficient manufacturing processes in the future.

</details>


### [85] [National Institute on Aging PREPARE Challenge: Early Detection of Cognitive Impairment Using Speech - The SpeechCARE Solution](https://arxiv.org/abs/2511.08132)
*Maryam Zolnoori,Hossein Azadmaleki,Yasaman Haghbin,Ali Zolnour,Mohammad Javad Momeni Nezhad,Sina Rashidi,Mehdi Naserian,Elyas Esmaeili,Sepehr Karimi Arpanahi*

Main category: cs.AI

TL;DR: SpeechCARE是一种多模态语音处理管道，用于通过语音特征检测认知障碍，解决了传统方法的性能不足问题，并展示了高分类准确性。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病及相关痴呆（ADRD）影响五分之一的60岁以上成年人，但超过一半的认知衰退患者未被诊断。语音评估有望用于早期检测，但现有方法性能有限。

Method: SpeechCARE结合预训练的多语言声学和语言变换器模型，采用动态融合架构整合多模态输入（如人口统计学数据），并进行预处理和可解释性分析。

Result: SpeechCARE在分类健康、轻度认知障碍（MCI）和AD患者中AUC为0.88，F1为0.72；在MCI检测中AUC为0.90，F1为0.62。偏倚分析显示对80岁以上人群存在少量差异。

Conclusion: SpeechCARE展示了高准确性和可解释性，未来计划在真实医疗环境中部署，并集成电子健康记录以覆盖代表性不足人群。

Abstract: Alzheimer's disease and related dementias (ADRD) affect one in five adults over 60, yet more than half of individuals with cognitive decline remain undiagnosed. Speech-based assessments show promise for early detection, as phonetic motor planning deficits alter acoustic features (e.g., pitch, tone), while memory and language impairments lead to syntactic and semantic errors. However, conventional speech-processing pipelines with hand-crafted features or general-purpose audio classifiers often exhibit limited performance and generalizability. To address these limitations, we introduce SpeechCARE, a multimodal speech processing pipeline that leverages pretrained, multilingual acoustic and linguistic transformer models to capture subtle speech-related cues associated with cognitive impairment. Inspired by the Mixture of Experts (MoE) paradigm, SpeechCARE employs a dynamic fusion architecture that weights transformer-based acoustic, linguistic, and demographic inputs, allowing integration of additional modalities (e.g., social factors, imaging) and enhancing robustness across diverse tasks. Its robust preprocessing includes automatic transcription, large language model (LLM)-based anomaly detection, and task identification. A SHAP-based explainability module and LLM reasoning highlight each modality's contribution to decision-making. SpeechCARE achieved AUC = 0.88 and F1 = 0.72 for classifying cognitively healthy, MCI, and AD individuals, with AUC = 0.90 and F1 = 0.62 for MCI detection. Bias analysis showed minimal disparities, except for adults over 80. Mitigation techniques included oversampling and weighted loss. Future work includes deployment in real-world care settings (e.g., VNS Health, Columbia ADRC) and EHR-integrated explainability for underrepresented populations in New York City.

</details>


### [86] [SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning](https://arxiv.org/abs/2511.08151)
*Xuchen Li,Ruitao Wu,Xuanbo Liu,Xukai Wang,Jinbo Hu,Zhixin Bai,Bohan Zeng,Hao Liang,Leheng Chen,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Xu-Yao Zhang,Liu Liu,Jia Li,Kaiqi Huang,Jiahao Xu,Haitao Mi,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: SciAgent是一个统一的多智能体系统，旨在实现跨学科和难度层次的通用化科学推理能力，通过动态协调专用子智能体，在数学和物理奥林匹克竞赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在特定科学任务上表现优异，但其系统仍局限且手动定制。SciAgent旨在解决这一问题，实现跨学科的科学推理能力。

Method: SciAgent采用分层结构，包括协调智能体动态调度专用工作系统，每个工作系统由交互的子智能体组成，负责符号推理、概念建模、数值计算和验证等任务。这些智能体协同构建和优化推理流程。

Result: SciAgent在数学和物理奥林匹克竞赛中（如IMO、IMC、IPhO、CPhO）表现优于人类金牌选手，并在国际化学奥林匹克（IChO）和HLE基准测试中展现出跨学科泛化能力。

Conclusion: SciAgent是实现通用化科学智能的重要一步，展现了跨学科专家级推理能力。

Abstract: Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.

</details>


### [87] [oboro: Text-to-Image Synthesis on Limited Data using Flow-based Diffusion Transformer with MMH Attention](https://arxiv.org/abs/2511.08168)
*Ryusuke Mizutani,Kazuaki Matano,Tsugumi Kadowaki,Haruki Tenya,Layris,nuigurumi,Koki Hashimoto,Yu Tanaka*

Main category: cs.AI

TL;DR: 该项目旨在解决日本动画制作行业劳动力短缺等问题，开发了名为'oboro:'的图像生成模型，特点是即使在有限数据集下也能生成高质量图像，模型权重和推理代码已公开。


<details>
  <summary>Details</summary>
Motivation: 针对日本动画行业劳动力短缺问题，开发自主创新的图像生成技术，推动国内AI生态发展。

Method: 使用仅经过版权清理的图像从头训练，设计了适用于小数据集的架构，确保高质量图像生成。

Result: 成功开发'oboro:'模型并公开了模型权重和代码，成为日本首个开源、商业导向的图像生成AI。

Conclusion: 项目通过透明开发过程支持日本AI社区，促进国内AI技术发展。

Abstract: This project was conducted as a 2nd-term adopted project of the "Post-5G Information and Communication System Infrastructure Enhancement R&D Project Development of Competitive Generative AI Foundation Models (GENIAC)," a business of the Ministry of Economy, Trade and Industry (METI) and the New Energy and Industrial Technology Development Organization (NEDO). To address challenges such as labor shortages in Japan's anime production industry, this project aims to develop an image generation model from scratch. This report details the technical specifications of the developed image generation model, "oboro:." We have developed "oboro:," a new image generation model built from scratch, using only copyright-cleared images for training. A key characteristic is its architecture, designed to generate high-quality images even from limited datasets. The foundation model weights and inference code are publicly available alongside this report. This project marks the first release of an open-source, commercially-oriented image generation AI fully developed in Japan. AiHUB originated from the OSS community; by maintaining transparency in our development process, we aim to contribute to Japan's AI researcher and engineer community and promote the domestic AI development ecosystem.

</details>


### [88] [Towards Provably Unlearnable Examples via Bayes Error Optimisation](https://arxiv.org/abs/2511.08191)
*Ruihan Zhang,Jun Sun,Ee-Peng Lim,Peixin Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种通过最大化贝叶斯误差构建不可学习样本的新方法，解决了现有启发式方法缺乏理论保证的问题，并在混合干净数据时仍保持有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大规模机器学习模型依赖海量数据训练，但数据来源往往未经用户同意，引发隐私保护问题。已有不可学习样本方法缺乏理论支持且混合干净数据时失效，需改进。

Method: 通过系统性最大化贝叶斯误差构建不可学习样本，采用基于优化的投影梯度上升法提供高效解决方案。

Result: 理论分析表明方法能有效增加贝叶斯误差，实验验证其在多种数据集和模型架构下能限制数据可学习性。

Conclusion: 该方法为不可学习样本提供了理论保证和实践有效性，尤其在混合干净数据场景下仍表现良好。

Abstract: The recent success of machine learning models, especially large-scale classifiers and language models, relies heavily on training with massive data. These data are often collected from online sources. This raises serious concerns about the protection of user data, as individuals may not have given consent for their data to be used in training. To address this concern, recent studies introduce the concept of unlearnable examples, i.e., data instances that appear natural but are intentionally altered to prevent models from effectively learning from them. While existing methods demonstrate empirical effectiveness, they typically rely on heuristic trials and lack formal guarantees. Besides, when unlearnable examples are mixed with clean data, as is often the case in practice, their unlearnability disappears. In this work, we propose a novel approach to constructing unlearnable examples by systematically maximising the Bayes error, a measurement of irreducible classification error. We develop an optimisation-based approach and provide an efficient solution using projected gradient ascent. Our method provably increases the Bayes error and remains effective when the unlearning examples are mixed with clean samples. Experimental results across multiple datasets and model architectures are consistent with our theoretical analysis and show that our approach can restrict data learnability, effectively in practice.

</details>


### [89] [EHRStruct: A Comprehensive Benchmark Framework for Evaluating Large Language Models on Structured Electronic Health Record Tasks](https://arxiv.org/abs/2511.08206)
*Xiao Yang,Xuejiao Zhao,Zhiqi Shen*

Main category: cs.AI

TL;DR: 本文介绍了EHRStruct基准，用于评估LLM在结构化电子健康记录（EHR）任务中的表现，并通过实验分析了影响模型性能的关键因素。最终提出了EHRMaster方法，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在处理结构化EHR数据方面表现出潜力，但缺乏标准化的评估框架和明确定义的任务，难以系统评估和比较LLM的性能。

Method: 作者提出了EHRStruct基准，包含11个代表性任务和2200个评估样本，并评估了20种先进LLM。通过分析输入格式、少样本泛化和微调策略等因素，提出了EHRMaster方法。

Result: 实验结果表明，许多结构化EHR任务对LLM的理解和推理能力提出了较高要求。EHRMaster方法达到了最先进的性能。

Conclusion: EHRStruct为评估LLM在结构化EHR任务中的表现提供了标准化框架，同时EHRMaster方法为解决这些任务提供了有效方案。

Abstract: Structured Electronic Health Record (EHR) data stores patient information in relational tables and plays a central role in clinical decision-making. Recent advances have explored the use of large language models (LLMs) to process such data, showing promise across various clinical tasks.However, the absence of standardized evaluation frameworks and clearly defined tasks makes it difficult to systematically assess and compare LLM performance on structured EHR data.To address these evaluation challenges, we introduce EHRStruct, a benchmark specifically designed to evaluate LLMs on structured EHR tasks.EHRStruct defines 11 representative tasks spanning diverse clinical needs and includes 2,200 task-specific evaluation samples derived from two widely used EHR datasets.We use EHRStruct to evaluate 20 advanced and representative LLMs, covering both general and medical models.We further analyze key factors influencing model performance, including input formats, few-shot generalisation, and finetuning strategies, and compare results with 11 state-of-the-art LLM-based enhancement methods for structured data reasoning. Our results indicate that many structured EHR tasks place high demands on the understanding and reasoning capabilities of LLMs.In response, we propose EHRMaster, a code-augmented method that achieves state-of-the-art performance and offers practical

</details>


### [90] [MADD: Multi-Agent Drug Discovery Orchestra](https://arxiv.org/abs/2511.08217)
*Gleb V. Solovev,Alina B. Zhidkovskaya,Anastasia Orlova,Nina Gubina,Anastasia Vepreva,Rodion Golovinskii,Ilya Tonkii,Ivan Dubrovsky,Ivan Gurev,Dmitry Gilemkhanov,Denis Chistiakov,Timur A. Aliev,Ivan Poddiakov,Galina Zubkova,Ekaterina V. Skorb,Vladimir Vinogradov,Alexander Boukhanovsky,Nikolay Nikitin,Andrei Dmitrenko,Anna Kalyuzhnaya,Andrey Savchenko*

Main category: cs.AI

TL;DR: MADD是一种多智能体系统，通过自然语言查询构建和执行定制化的命中识别流程，结合了语言模型的易用性和精确工具的准确性，显著提升了药物发现效率。


<details>
  <summary>Details</summary>
Motivation: 早期药物发现中的命中识别传统上需要大量实验资源，而复杂的人工智能工具限制了湿实验室研究人员的可及性。

Method: MADD利用四个协调的智能体处理从头化合物生成和筛选中的关键子任务。

Result: 在七个药物发现案例中，MADD表现出优于现有LLM解决方案的性能，并应用于五个生物靶点。

Conclusion: MADD为药物设计提供了高效的工具，并贡献了新的基准数据集，推动了药物设计的智能化未来。

Abstract: Hit identification is a central challenge in early drug discovery, traditionally requiring substantial experimental resources. Recent advances in artificial intelligence, particularly large language models (LLMs), have enabled virtual screening methods that reduce costs and improve efficiency. However, the growing complexity of these tools has limited their accessibility to wet-lab researchers. Multi-agent systems offer a promising solution by combining the interpretability of LLMs with the precision of specialized models and tools. In this work, we present MADD, a multi-agent system that builds and executes customized hit identification pipelines from natural language queries. MADD employs four coordinated agents to handle key subtasks in de novo compound generation and screening. We evaluate MADD across seven drug discovery cases and demonstrate its superior performance compared to existing LLM-based solutions. Using MADD, we pioneer the application of AI-first drug design to five biological targets and release the identified hit molecules. Finally, we introduce a new benchmark of query-molecule pairs and docking scores for over three million compounds to contribute to the agentic future of drug design.

</details>


### [91] [Beyond Distributions: Geometric Action Control for Continuous Reinforcement Learning](https://arxiv.org/abs/2511.08234)
*Zhihao Lin*

Main category: cs.AI

TL;DR: 论文提出了Geometric Action Control (GAC)方法，在保留球形分布几何优势的同时简化了计算，实现了在连续控制任务中的高效表现。


<details>
  <summary>Details</summary>
Motivation: 解决高斯策略在连续控制任务中因无界支持而需要临时压缩函数的问题，同时克服vMF分布计算复杂和实用性差的缺点。

Method: GAC将动作生成分解为方向向量和可学习的集中参数，实现简单且高效的O(d)复杂度操作。

Result: 在六个MuJoCo任务中表现出色，Ant-v4任务上比SAC提升了37.6%，并在4项任务中达到最佳效果。

Conclusion: 研究表明，连续控制的鲁棒性和高效性不需要复杂的分布，而应基于动作空间的几何特性进行设计。

Abstract: Gaussian policies have dominated continuous control in deep reinforcement learning (RL), yet they suffer from a fundamental mismatch: their unbounded support requires ad-hoc squashing functions that distort the geometry of bounded action spaces. While von Mises-Fisher (vMF) distributions offer a theoretically grounded alternative on the sphere, their reliance on Bessel functions and rejection sampling hinders practical adoption. We propose \textbf{Geometric Action Control (GAC)}, a novel action generation paradigm that preserves the geometric benefits of spherical distributions while \textit{simplifying computation}. GAC decomposes action generation into a direction vector and a learnable concentration parameter, enabling efficient interpolation between deterministic actions and uniform spherical noise. This design reduces parameter count from \(2d\) to \(d+1\), and avoids the \(O(dk)\) complexity of vMF rejection sampling, achieving simple \(O(d)\) operations. Empirically, GAC consistently matches or exceeds state-of-the-art methods across six MuJoCo benchmarks, achieving 37.6\% improvement over SAC on Ant-v4 and the best results on 4 out of 6 tasks. Our ablation studies reveal that both \textbf{spherical normalization} and \textbf{adaptive concentration control} are essential to GAC's success. These findings suggest that robust and efficient continuous control does not require complex distributions, but a principled respect for the geometry of action spaces. Code and pretrained models are available in supplementary materials.

</details>


### [92] [Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning](https://arxiv.org/abs/2511.08246)
*Ziyu Ma,Chenhui Gou,Yiming Hu,Yong Wang,Xiangxiang Chu,Bohan Zhuang,Jianfei Cai*

Main category: cs.AI

TL;DR: 本文提出了一种基于敏感性的任务向量插入框架（STV），以解决在大型多模态模型（LMMs）中多示例上下文学习（ICL）的上下文长度限制和高推理成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有的任务向量方法未能有效解决任务向量插入的位置和内容问题，导致模型性能受限。

Method: 通过分析查询-上下文对的激活差异，提出了敏感性感知的插入位置选择，并通过聚类和强化学习确定插入内容。

Result: 实验表明，STV在多模态模型和任务中显著优于现有方法，具有强泛化性。

Conclusion: STV为多示例ICL提供了一种高效且通用的解决方案。

Abstract: Large Multimodal Models (LMMs) have shown promising in-context learning (ICL) capabilities, but scaling to many-shot settings remains difficult due to limited context length and high inference cost. To address these challenges, task-vector-based methods have been explored by inserting compact representations of many-shot in-context demonstrations into model activations. However, existing task-vector-based methods either overlook the importance of where to insert task vectors or struggle to determine suitable values for each location. To this end, we propose a novel Sensitivity-aware Task Vector insertion framework (STV) to figure out where and what to insert. Our key insight is that activation deltas across query-context pairs exhibit consistent structural patterns, providing a reliable cue for insertion. Based on the identified sensitive-aware locations, we construct a pre-clustered activation bank for each location by clustering the activation values, and then apply reinforcement learning to choose the most suitable one to insert. We evaluate STV across a range of multimodal models (e.g., Qwen-VL, Idefics-2) and tasks (e.g., VizWiz, OK-VQA), demonstrating its effectiveness and showing consistent improvements over previous task-vector-based methods with strong generalization.

</details>


### [93] [DiagramIR: An Automatic Pipeline for Educational Math Diagram Evaluation](https://arxiv.org/abs/2511.08283)
*Vishal Kumar,Shubhra Mishra,Rebecca Hao,Rizwaan Malik,David Broman,Dorottya Demszky*

Main category: cs.AI

TL;DR: DiagramIR是一个自动化和可扩展的几何图形评估流水线，通过中间表示（IRs）优化LLM生成的LaTeX TikZ代码评估，并在评测效果上优于其他基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在数学等领域的学习中多以文本为主，限制了其可视化生成能力的实用性。现有的LLM生成代码编译为教育图表的方法缺乏可扩展的评估手段。

Method: 提出DiagramIR，基于LaTeX TikZ代码的中间表示（IRs），实现自动化和可扩展的几何图形评估流水线。

Result: DiagramIR与人类评估者的评分一致性更高，且能让较小的模型（如GPT-4.1-Mini）在10倍更低推理成本下表现与大型模型（如GPT-5）相当。

Conclusion: 该方法为可访问和可扩展的教育技术部署提供了高效评估方案。

Abstract: Large Language Models (LLMs) are increasingly being adopted as tools for learning; however, most tools remain text-only, limiting their usefulness for domains where visualizations are essential, such as mathematics. Recent work shows that LLMs are capable of generating code that compiles to educational figures, but a major bottleneck remains: scalable evaluation of these diagrams. We address this by proposing DiagramIR: an automatic and scalable evaluation pipeline for geometric figures. Our method relies on intermediate representations (IRs) of LaTeX TikZ code. We compare our pipeline to other evaluation baselines such as LLM-as-a-Judge, showing that our approach has higher agreement with human raters. This evaluation approach also enables smaller models like GPT-4.1-Mini to perform comparably to larger models such as GPT-5 at a 10x lower inference cost, which is important for deploying accessible and scalable education technologies.

</details>


### [94] [AI-Powered Data Visualization Platform: An Intelligent Web Application for Automated Dataset Analysis](https://arxiv.org/abs/2511.08363)
*Srihari R,Pallavi M,Tejaswini S,Vaishnavi R C*

Main category: cs.AI

TL;DR: 论文介绍了一个AI驱动的数据可视化平台，自动化整个数据分析流程，从数据集上传到生成交互式可视化。系统通过机器学习算法自动清理、预处理数据，智能选择可视化方式，显著减少人工操作。


<details>
  <summary>Details</summary>
Motivation: 传统数据分析过程耗时且依赖人工，该研究旨在通过AI技术实现数据分析和可视化的全流程自动化，提高效率并减少人为错误。

Method: 平台结合Python Flask后端和React前端，利用Firebase Cloud Storage处理数据。采用机器学习算法实现数据清理、特征选择和可视化自动生成。

Result: 实验验证了平台在实时处理大规模数据集（10万行）的能力，并能同时服务多用户，生成高质量的视觉输出。

Conclusion: 该云平台显著减少了人工干预，同时确保高质量的可视化效果和用户体验。

Abstract: An AI-powered data visualization platform that automates the entire data analysis process, from uploading a dataset to generating an interactive visualization. Advanced machine learning algorithms are employed to clean and preprocess the data, analyse its features, and automatically select appropriate visualizations. The system establishes the process of automating AI-based analysis and visualization from the context of data-driven environments, and eliminates the challenge of time-consuming manual data analysis. The combination of a Python Flask backend to access the dataset, paired with a React frontend, provides a robust platform that automatically interacts with Firebase Cloud Storage for numerous data processing and data analysis solutions and real-time sources. Key contributions include automatic and intelligent data cleaning, with imputation for missing values, and detection of outliers, via analysis of the data set. AI solutions to intelligently select features, using four different algorithms, and intelligent title generation and visualization are determined by the attributes of the dataset. These contributions were evaluated using two separate datasets to assess the platform's performance. In the process evaluation, the initial analysis was performed in real-time on datasets as large as 100000 rows, while the cloud-based demand platform scales to meet requests from multiple users and processes them simultaneously. In conclusion, the cloud-based data visualization application allowed for a significant reduction of manual inputs to the data analysis process while maintaining a high quality, impactful visual outputs, and user experiences

</details>


### [95] [SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models](https://arxiv.org/abs/2511.08379)
*Giorgio Piras,Raffaele Mura,Fabio Brau,Luca Oneto,Fabio Roli,Battista Biggio*

Main category: cs.AI

TL;DR: 论文提出了一种新方法，利用自组织映射（SOMs）提取多个拒绝方向，以改进现有单一方向拒绝行为的局限性，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将拒绝行为编码为潜在空间中的单一方向，但证据表明LLMs中的概念可能以低维流形嵌入高维空间，因此需要更精细的方法。

Method: 利用自组织映射（SOMs）从有害提示表示中识别多个神经元，通过减去无害表示的质心，提取多个拒绝方向。

Result: 实验表明，从模型内部消除多个方向不仅优于单一方向基线，还能有效抑制拒绝行为，甚至优于专门设计的越狱算法。

Conclusion: 该方法在拒绝行为研究中进行了机制性分析，为理解LLMs的工作机制提供了新视角。

Abstract: Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Motivated by these findings, we propose a novel method leveraging Self-Organizing Maps (SOMs) to extract multiple refusal directions. To this end, we first prove that SOMs generalize the prior work's difference-in-means technique. We then train SOMs on harmful prompt representations to identify multiple neurons. By subtracting the centroid of harmless representations from each neuron, we derive a set of multiple directions expressing the refusal concept. We validate our method on an extensive experimental setup, demonstrating that ablating multiple directions from models' internals outperforms not only the single-direction baseline but also specialized jailbreak algorithms, leading to an effective suppression of refusal. Finally, we conclude by analyzing the mechanistic implications of our approach.

</details>


### [96] [FaithAct: Faithfulness Planning and Acting in MLLMs](https://arxiv.org/abs/2511.08409)
*Junxian Li,Xinyue Xu,Sai Ma,Sichao Li*

Main category: cs.AI

TL;DR: 论文提出了FaithEval和FaithAct框架，用于评估和增强多模态推理中的忠实性，解决了LLM推理链与感知证据或结论的不一致问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的推理链常与感知证据或结论不一致，因此需要区分行为忠实性和感知忠实性，并提出评估和增强方法。

Method: 通过FaithEval量化步骤和链级忠实性，并设计FaithAct框架，强制每个推理步骤的证据支持。

Result: 实验证明，FaithAct在多个基准测试中将感知忠实性提升高达26%，且不影响任务准确性。

Conclusion: 忠实性作为指导原则不仅能减少幻觉，还能稳定推理轨迹，为多模态推理提供了统一的评估和增强框架。

Abstract: Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.

</details>


### [97] [Dataset Safety in Autonomous Driving: Requirements, Risks, and Assurance](https://arxiv.org/abs/2511.08439)
*Alireza Abbaspour,Tejaskumar Balgonda Patil,B Ravi Kiran,Russel Mohr,Senthil Yogamani*

Main category: cs.AI

TL;DR: 本文提出了一种结构化框架，用于开发符合ISO/PAS 8800指南的安全数据集，强调数据完整性对自动驾驶AI系统安全的重要性。


<details>
  <summary>Details</summary>
Motivation: 数据完整性是自动驾驶AI系统安全和可靠的基础，但目前缺乏系统化的框架来确保数据集的安全性。

Method: 通过AI数据飞轮和数据集生命周期（包括数据收集、标注、整理和维护）引入结构化框架，并结合严格的安全分析和风险缓解策略。

Result: 框架定义了数据集安全要求并提出了验证策略，以确保符合安全标准，同时总结了数据集安全领域的最新研究和趋势。

Conclusion: 该框架旨在推动自动驾驶应用中稳健且安全可靠的AI系统发展。

Abstract: Dataset integrity is fundamental to the safety and reliability of AI systems, especially in autonomous driving. This paper presents a structured framework for developing safe datasets aligned with ISO/PAS 8800 guidelines. Using AI-based perception systems as the primary use case, it introduces the AI Data Flywheel and the dataset lifecycle, covering data collection, annotation, curation, and maintenance. The framework incorporates rigorous safety analyses to identify hazards and mitigate risks caused by dataset insufficiencies. It also defines processes for establishing dataset safety requirements and proposes verification and validation strategies to ensure compliance with safety standards. In addition to outlining best practices, the paper reviews recent research and emerging trends in dataset safety and autonomous vehicle development, providing insights into current challenges and future directions. By integrating these perspectives, the paper aims to advance robust, safety-assured AI systems for autonomous driving applications.

</details>


### [98] [Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models](https://arxiv.org/abs/2511.08484)
*Huzaifa Arif,Keerthiram Murugesan,Ching-Yun Ko,Pin-Yu Chen,Payel Das,Alex Gittens*

Main category: cs.AI

TL;DR: 该论文提出了一种类似于软件版本更新的轻量级模块化方法，通过为大型语言模型（LLMs）添加可学习的前缀来快速修复安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM主要版本更新成本高且不灵活，难以满足客户需求，导致已发布模型存在已知安全漏洞。

Method: 通过为现有模型添加一个紧凑、可学习的前缀（'补丁'，仅增加0.003%参数），实现对模型行为的调整，使其更接近安全参考模型的行为。

Result: 在毒性缓解、偏见减少和有害拒绝三个关键领域，该方法实现了与下一代安全对齐模型相当的安全改进，同时保持了模型流畅性。

Conclusion: LLMs可以像软件一样进行'补丁'更新，为供应商和从业者提供了一个实用的机制，用于在主要模型版本之间分发可扩展、高效且可组合的安全更新。

Abstract: We propose patching for large language models (LLMs) like software versions, a lightweight and modular approach for addressing safety vulnerabilities. While vendors release improved LLM versions, major releases are costly, infrequent, and difficult to tailor to customer needs, leaving released models with known safety gaps. Unlike full-model fine-tuning or major version updates, our method enables rapid remediation by prepending a compact, learnable prefix to an existing model. This "patch" introduces only 0.003% additional parameters, yet reliably steers model behavior toward that of a safer reference model. Across three critical domains (toxicity mitigation, bias reduction, and harmfulness refusal) policy patches achieve safety improvements comparable to next-generation safety-aligned models while preserving fluency. Our results demonstrate that LLMs can be "patched" much like software, offering vendors and practitioners a practical mechanism for distributing scalable, efficient, and composable safety updates between major model releases.

</details>


### [99] [A Matter of Interest: Understanding Interestingness of Math Problems in Humans and Language Models](https://arxiv.org/abs/2511.08548)
*Shubhra Mishra,Yuka Machino,Gabriel Poesia,Albert Jiang,Joy Hsu,Adrian Weller,Challenger Mishra,David Broman,Joshua B. Tenenbaum,Mateja Jamnik,Cedegao E. Zhang,Katherine M. Collins*

Main category: cs.AI

TL;DR: 研究探讨人类与LLM在数学问题趣味性和难度判断上的一致性，发现LLM虽与人类有部分共识，但整体判断分布和理由相关性较弱。


<details>
  <summary>Details</summary>
Motivation: 随着AI（如LLM）在数学研究和教育中的参与度增加，了解其与人类在数学问题趣味性和难度判断上的对齐性变得重要。

Method: 通过两项实证研究，对比人类与LLM对数学问题趣味性和难度的评估，研究对象包括众包平台参与者和国际数学奥赛选手。

Result: LLM在趣味性判断上与人类有部分共识，但未能完全捕捉人类判断分布，且与人类选择的趣味性理由相关性较弱。

Conclusion: 当前LLM在数学问题上捕捉人类趣味性判断的能力既有潜力也有局限性，为未来数学AI合作提供了启示。

Abstract: The evolution of mathematics has been guided in part by interestingness. From researchers choosing which problems to tackle next, to students deciding which ones to engage with, people's choices are often guided by judgments about how interesting or challenging problems are likely to be. As AI systems, such as LLMs, increasingly participate in mathematics with people -- whether for advanced research or education -- it becomes important to understand how well their judgments align with human ones. Our work examines this alignment through two empirical studies of human and LLM assessment of mathematical interestingness and difficulty, spanning a range of mathematical experience. We study two groups: participants from a crowdsourcing platform and International Math Olympiad competitors. We show that while many LLMs appear to broadly agree with human notions of interestingness, they mostly do not capture the distribution observed in human judgments. Moreover, most LLMs only somewhat align with why humans find certain math problems interesting, showing weak correlation with human-selected interestingness rationales. Together, our findings highlight both the promises and limitations of current LLMs in capturing human interestingness judgments for mathematical AI thought partnerships.

</details>


### [100] [Simulating the Visual World with Artificial Intelligence: A Roadmap](https://arxiv.org/abs/2511.08585)
*Jingtong Yue,Ziqi Huang,Zhaoxi Chen,Xintao Wang,Pengfei Wan,Ziwei Liu*

Main category: cs.AI

TL;DR: 视频生成领域正从视觉吸引力转向构建支持交互和物理合理性的虚拟环境，通过视频基础模型结合隐式世界模型和视频渲染器实现。


<details>
  <summary>Details</summary>
Motivation: 探讨视频生成模型如何发展为具备物理合理性、多模态交互和规划能力的隐式世界模型。

Method: 提出视频基础模型由隐式世界模型和视频渲染器组成，分析四代技术的演进与应用场景。

Result: 总结了不同代技术的发展特点和代表性工作，例如在机器人、自动驾驶和交互游戏中的应用。

Conclusion: 讨论了下一代世界模型的开放挑战和设计原则，强调智能体在系统评估中的重要性。

Abstract: The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a "window" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.

</details>
