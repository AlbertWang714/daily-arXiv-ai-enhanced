<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 论文研究了语言预测的信息熵限制，提出了一种结合编码器的因果解码器模型架构，在有限硬件条件下实现了更高的训练效率和压缩率，并通过实验证明考虑信息熵的模型具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 语言预测受到语言固有信息熵的限制，现有方法难以高效估计语言熵。研究旨在通过改进模型架构和训练方法，提升语言模型的压缩率和泛化性能。

Method: 引入结合编码器的因果解码器模型架构，通过优化训练效率，在有限硬件条件下实现更高的语言压缩率，并提出基于信息熵的训练方法。

Result: 实验表明，新架构在压缩率和训练效率上优于传统因果变换器。通过精确估计每个标记的熵，模型表现出更好的泛化能力。

Conclusion: 在训练中考虑语言的信息熵，能够提升模型的泛化性能，避免过度训练带来的性能下降。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [2] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: 提出了一种名为SSR的新框架，通过细粒度评估和精确改进来优化大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试框架依赖粗粒度的自验证和自校正，限制了复杂任务的有效性。

Method: SSR将模型响应分解为可验证的子问题和子答案对，通过重新求解和自一致性检查进行步骤级置信度估计。

Result: 在五个推理基准和三个LLM上，SSR表现优于现有自改进基线。

Conclusion: SSR不仅提升性能，还提供了评估和理解LLM内部推理过程的黑盒方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [3] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella是一个完全开放的30亿参数语言模型家族，使用公开数据训练，提供高性能且透明的替代方案，并包含两个专门变体。


<details>
  <summary>Details</summary>
Motivation: 解决现有高性能语言模型多为闭源或部分开源的问题，推动语言模型研究的透明性和可复现性。

Method: 采用大规模预训练、通用指令调优和人类偏好对齐技术，并使用AMD Instinct MI300X GPU支持。

Result: Instella在完全开源模型中达到领先水平，并且与类似规模的开源权重模型竞争。推出了两个专门变体。

Conclusion: Instella为社区提供了一个高性能、透明的语言模型选择，推动了开源和可复现研究的进展。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [4] [Black-Box On-Policy Distillation of Large Language Models](https://arxiv.org/abs/2511.10643)
*Tianzhu Ye,Li Dong,Zewen Chi,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: 生成对抗蒸馏（GAD）提出了一种通过对抗训练实现黑盒蒸馏的方法，学生模型通过判别器的反馈学习生成与教师模型相似的文本，实验表明其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒蒸馏方法仅依赖教师模型的文本输出，缺乏内部参数或逻辑的支持，限制了学生模型的学习效果。GAD旨在解决这一问题，提供更高效的蒸馏方式。

Method: GAD将学生LLM视为生成器，并通过训练判别器区分学生与教师模型的输出，形成对抗训练框架。判别器作为奖励模型，提供动态反馈。

Result: 实验显示，GAD在性能上显著优于序列级知识蒸馏方法。例如，Qwen2.5-14B-Instruct（学生模型）与教师模型GPT-5-Chat在评估中表现相当。

Conclusion: GAD为黑盒LLM蒸馏提供了高效的新范式，实验验证了其优越性和潜力。

Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.

</details>


### [5] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: ParoQuant是一种权重后训练量化方法，通过结合硬件高效的独立旋转和通道缩放，减少异常值影响，提升推理效率和精度。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型后训练量化中因异常值导致的量化误差和精度下降问题。

Method: 提出Pairwise Rotation Quantization（ParoQuant），结合独立旋转和通道缩放，优化量化组动态范围。

Result: 在推理任务中平均比AWQ提升2.4%准确率，且运行时开销小于10%。

Conclusion: ParoQuant为推理型大型语言模型的高效准确部署提供新途径。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: 该论文探讨了仿真测试中发现的自动驾驶失败场景是否能在真实世界中复现的问题，提出了一种验证仿真失败场景的有效方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决仿真与真实世界之间存在的差距（sim-to-real gap），以确保仿真测试中的失败场景在真实世界中具有实际意义。

Method: 通过形式化定义标记时间序列传感器数据与抽象场景的匹配关系，并设计了一种查询算法，能够在标记数据集中快速准确地识别匹配特定场景的子集。

Result: 实验结果表明，提出的算法在查询场景时比最先进的商业视觉大语言模型更准确且速度更快，并能随着时间序列数据的时长扩展。

Conclusion: 该研究为验证仿真失败场景在真实世界中的有效性提供了一种高效且可扩展的方法。

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>
