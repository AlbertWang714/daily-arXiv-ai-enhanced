<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 78]
- [cs.AI](#cs.AI) [Total: 41]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 研究提出了Residual Stream Decoders框架，用于探测语言模型中段落和文档尺度的计划信息，发现小模型可解码相当于5+个未来令牌的信息。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型处理更长时间跨度的任务，现有方法通常局限于测试特定概念或令牌，缺乏对段落或文档尺度计划信息的理解。

Method: 开发了Residual Stream Decoders框架，用于探测模型激活中的长期规划信息。

Result: 实验表明，小模型能解码相当于5+个未来令牌的信息。

Conclusion: 这些结果为更好地监测语言模型及其如何编码长期规划信息奠定了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [2] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 该论文提出了一种优化大型语言模型（LLM）训练性能的新方法，通过预测信息丰富的标记而非传统的下一个标记预测（NTP），从而更高效地训练模型。


<details>
  <summary>Details</summary>
Motivation: 传统的下一个标记预测（NTP）方法在训练LLM时存在效率不足的问题，论文旨在通过优化目标标记选择策略，提高模型性能的同时控制计算成本。

Method: 论文提出了一种替代NTP的方法，即选择信息丰富的标记作为预测目标，并在算术、多标签文本分类和自然语言生成三类任务中验证其效果。

Result: 研究结果表明，所提出的方法能够有效提升LLM在不同任务中的性能，并为目标标记选择策略的理论理解提供了进展。

Conclusion: 该方法为优化LLM训练提供了一种原则性途径，不仅提升了模型表现，还深化了对标记选择策略的理论认识。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [3] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 该论文提出了一个统一的框架，用于评估和改进大型语言模型（LLMs）生成对话中角色的一致性，并通过强化学习显著提高了角色扮演的连贯性。


<details>
  <summary>Details</summary>
Motivation: LLMs在模拟用户角色时容易出现角色漂移或行为不一致的问题，影响了其在互动场景（如治疗、教育等）中的应用效果。

Method: 定义了三种自动评估指标，并通过多轮强化学习对LLMs进行微调，以提高其角色一致性。

Result: 实验表明，该方法能减少55%以上的不一致性，生成更连贯和真实的模拟用户对话。

Conclusion: 通过提出的框架和强化学习方法，LLMs在模拟用户角色时的表现显著提升，有望广泛应用于互动场景中。

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [4] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 研究者提出IL-PCR（印度法律语料库），用于共同开发与法律条文和先例检索相关的模型，通过LLM重新排序方法实现最佳性能。


<details>
  <summary>Details</summary>
Motivation: 以往研究将法律条文检索和先例检索视为独立任务，但实际上两者具有内在关联性。本文旨在填补这一研究空白。

Method: 构建IL-PCR语料库，实验多种基线模型（词法、语义和基于GNN的集成模型），并开发基于LLM的重新排序方法。

Result: 基于LLM的重新排序方法在任务中表现最佳。

Conclusion: IL-PCR为法律条文和先例检索提供了统一的研究平台，证明了任务间的依赖关系可通过LLM方法有效利用。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [5] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: POSESTITCH-SLT是一种新的预训练方案，通过模板生成的句子对进行训练，在低资源手语翻译任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于大规模、句子对齐的手语数据集稀缺，手语翻译仍是一项挑战。

Method: 提出POSESTITCH-SLT预训练方案，利用基于语言模板的句子生成技术，结合Transformer编码器-解码器架构。

Result: 在How2Sign和iSign数据集上，BLEU-4分数分别从1.97提升到4.56和从0.55提升到3.43，优于现有方法。

Conclusion: 模板驱动的合成监督在低资源手语翻译中表现出色。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [6] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 提出了Factorization Memory，一种高效的RNN架构，在短上下文语言建模任务中性能媲美Transformer，同时在长上下文中表现更优。


<details>
  <summary>Details</summary>
Motivation: 通过改进Mamba-2，设计一种既能利用并行计算训练，又能在推理时保持恒定计算和内存复杂度的RNN模型。

Method: 开发了Factorization Memory的稀疏版本，仅更新部分循环状态，同时保持密集版本的性能。

Result: 实证分析表明，Factorization Memory在短和长上下文任务中均表现出色，是首个结合稀疏内存激活的RNN架构。

Conclusion: Factorization Memory为RNN架构提供了新的高效设计思路，平衡了性能和效率。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [7] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 提出了因果语言建模（CLM）目标的结构性属性——反转不变性，并指出这种对称性可能导致模型无法捕捉语言中的方向性依赖关系。


<details>
  <summary>Details</summary>
Motivation: 发现CLM训练目标对文本及其反转的预测损失相同，这可能导致模型无法捕捉语言的固有时间不对称性。

Method: 通过理论分析证明了CLM的反转不变性，并提出未来应探索显式建模语言方向性的损失函数和架构。

Result: 反转文本训练的模型性能与正向文本相当，表明当前预训练目标存在局限性。

Conclusion: 建议未来研究方向应考虑语言的时间不对称性，改进预训练目标以更好地捕捉语言的依赖关系。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [8] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: LingGym是一个新基准，用于评估大型语言模型在跨语言推理中的能力，特别关注低资源语言和未见过的结构。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型是否能推广语言学推理到训练数据中未涵盖的低资源语言和结构。

Method: 使用Interlinear Glossed Text（IGT）和语法描述，设计Word-Gloss Inference任务，模型需根据语言学信息推断缺失的单词和注释。

Result: 结合结构化语言学线索能提升所有模型在推理任务中的表现。

Conclusion: 该研究展示了大型语言模型在语言学分析和低资源语言文档处理中的潜力与局限性。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [9] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 论文提出了一种基于苏格拉底调试的方法，通过引导推理轨迹帮助学生自主发现和修复编程错误，并展示了前沿LLM模型在生成推理轨迹和对话上的高准确性。


<details>
  <summary>Details</summary>
Motivation: 目标是通过苏格拉底调试帮助学生自主识别和修复编程错误，尤其是由编程误解引起的错误，从而提高学习效果和编程能力。

Method: 提出推理轨迹生成任务，并构建了手动标注的数据集；基于LLM生成推理轨迹和锚定在其上的苏格拉底式对话。

Result: 前沿模型能够生成91%正确的推理轨迹和98.7%有效的对话轮次。

Conclusion: 研究表明，LLM在支持和提升苏格拉底调试方面具有巨大潜力，能够有效帮助学生修正编程误解。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [10] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: AI生成文本检测器在直接LLM输出上准确性超过90%，但对迭代转述内容失效。研究发现迭代转述生成特殊区域，引发两类攻击：原创性混淆和抄袭规避。研究团队提出首个系统性评测基准PADBen，评估11种先进检测器，发现现有方法无法应对中间转述区域，需改进检测架构。


<details>
  <summary>Details</summary>
Motivation: 探索迭代转述为何能绕过AI生成文本检测系统，以及如何通过系统性评测发现现有检测器的漏洞。

Method: 通过内在机制分析揭示迭代转述的特性，构建PADBen基准，包含五类文本分类和五项渐进检测任务，评估11种检测器的表现。

Result: 检测器能识别抄袭规避攻击，但对原创性混淆攻击失效，表明现有方法无法处理转述生成的中间区域。

Conclusion: 需从根本上改进检测架构，超越现有语义和风格区分方法，以应对迭代转述带来的挑战。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [11] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: MedRECT是一个跨语言（日语/英语）的医学错误处理基准，用于评估大语言模型（LLM）在医学文本中的错误检测、定位和纠正能力。研究发现，推理模型表现最优，微调后模型甚至超过人类专家表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在医学领域应用前景广阔，但其在多语言环境下处理医学文本错误的能力尚未充分评估，这是实现安全部署的关键。

Method: 通过自动化流程从日本医学执照考试和英文数据构建MedRECT基准，评估9种当代LLM，并采用LoRA微调技术优化模型。

Result: 推理模型在错误检测和句子提取中表现显著优于标准架构；跨语言评估显示日语与英文性能差距为5-10%；微调后的模型在错误纠正任务中超越人类专家。

Conclusion: MedRECT是首个全面的跨语言医学错误纠正基准，为开发多语言医学LLM提供了可复现框架和资源。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [12] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 摘要提出了一种名为G2的无需训练即插即用方法，旨在提高大型语言模型的输出多样性，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理任务中表现出色，但输出多样性不足，影响如创意写作和推理等任务。现有方法（如温度缩放）在提高多样性的同时会降低输出质量。

Method: 提出的G2方法使用基础生成器和双导向器，通过基于解码的干预来引导生成过程，从而在不牺牲质量的情况下增加输出多样性。

Result: 实验表明，G2能有效提高输出多样性，并在多样性与质量之间保持最佳平衡。

Conclusion: G2解决了大型语言模型输出多样性不足的问题，为需要多样化输出的任务提供了有效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [13] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 论文介绍了第一个最大的Bhili-Hindi-English平行语料库（BHEPC），用于解决印度部落语言Bhili的低资源机器翻译问题，并通过评估多语言大模型展示了其在低资源场景中的潜力。


<details>
  <summary>Details</summary>
Motivation: 针对印度Bhili等低资源语言缺乏高质量语料的问题，填补资源空白并促进包容性自然语言处理技术的发展。

Method: 创建了BHEPC语料库，涵盖教育、行政和新闻等领域，并评估了多种多语言大模型在双向翻译任务中的表现。

Result: 微调的NLLB-200 distilled 600M模型表现最优，证明了多语言模型在低资源场景下的潜力。

Conclusion: 这项研究填补了关键资源缺口，为全球低资源和边缘化语言的机器翻译提供了重要支持。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [14] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 本文研究了数据集大小对差分隐私（DP）自然语言处理（NLP）中文本重写机制效果的影响，揭示了数据集大小在隐私-效用权衡中的重要性。


<details>
  <summary>Details</summary>
Motivation: 目前在差分隐私与自然语言处理（DP NLP）领域，文本重写机制的效果评估中数据集大小的影响常被忽略。本文旨在填补这一空白。

Method: 设计了针对大规模数据集的效用和隐私测试，并在不同大小的数据集上进行实验，最多包含100万条文本，重点量化数据集大小对隐私-效用权衡的影响。

Result: 研究发现数据集大小在评估差分隐私文本重写机制中起着重要作用。

Conclusion: 研究结果呼吁在DP NLP中采用更严格的评估方法，并为DP NLP的未来实践和规模化应用提供了启示。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [15] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: 论文提出了一种名为ToM的树状MapReduce框架，用于解决大语言模型在长上下文推理中的性能退化问题，显著优于现有的分治框架和检索增强生成方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文推理中由于有限的上下文窗口而性能下降，现有的检索增强生成（RAG）和分治框架（DCF）方法分别存在逻辑连贯性不足和长程依赖捕捉困难的问题。

Method: ToM框架通过层次化语义解析构建文档树（DocTree），并采用树状MapReduce方法进行自底向上的聚合，递归生成和整合推理依据。

Result: 在70B+参数的大语言模型上，ToM显著优于RAG和DCF方法，表现出更好的逻辑连贯性和长上下文推理能力。

Conclusion: ToM通过利用文档的层次结构和树状MapReduce方法，有效解决了长上下文推理的挑战，为未来研究提供了新的方向。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [16] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: Zero-RAG通过Mastery-Score指标剪枝冗余知识，提升大型语言模型内部知识利用，实验显示剪枝30%维基百科语料且检索速度提升22%。


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成中外部语料与大型语言模型内部知识冗余的问题，减少检索负担并提升性能。

Method: 提出Mastery-Score指标识别冗余知识并剪枝，结合Query Router和Noise-Tolerant Tuning优化LLM内部知识利用。

Result: 剪枝30%维基百科语料，检索速度提升22%，不影响RAG性能。

Conclusion: Zero-RAG有效减少知识冗余，提升检索效率和LLM内部知识利用。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [17] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: 论文研究了在资源匮乏的尼泊尔农村地区，通过微调轻量级对话模型DialoGPT，构建了一个离线可用的医疗对话系统，用于处理十种常见疾病。结果显示模型能够生成连贯且医学上合适的回答。


<details>
  <summary>Details</summary>
Motivation: 针对资源匮乏地区（如尼泊尔农村）医疗资源不足的问题，探索利用离线可用的对话模型支持医疗服务。

Method: 微调轻量级生成对话模型DialoGPT，使用合成的医生-患者交互数据集，涵盖尼泊尔农村常见的十种疾病。

Result: 尽管训练数据有限且领域特定，模型能够生成连贯、上下文相关且医学上合适的回答，表现出对症状、疾病背景和同理心沟通的理解。

Conclusion: 研究表明，轻量级离线对话模型和针对性数据集在低资源医疗环境中具有良好适应性，为未来农村医疗对话AI提供了有前景的方向。

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [18] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 该研究探讨了基于Transformer的语言模型中的性别偏见，提出了一种新度量标准MALoR和通过反事实数据增强的缓解方法，显著降低了偏见分数。


<details>
  <summary>Details</summary>
Motivation: 编码器基础的Transformer模型在各种语言任务中表现优异，但存在从训练数据中继承的性别偏见问题，需要进行深入研究。

Method: 通过引入MALoR度量标准和基于反事实数据增强的持续预训练方法，对BERT等模型进行性别偏见评估与缓解。

Result: 缓解方法显著降低了性别偏见分数（如BERT-base中“he-she”从1.27降至0.08），且不影响下游任务性能。

Conclusion: 提出的方法有效减少了性别偏见，为未来研究提供了可行的偏见缓解方向。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [19] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 论文提出了一种轻量级工具WordSaladChopper，用于检测并去除大型推理模型中的无用重复输出（word salad），从而在不显著影响质量的情况下大幅减少输出长度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在生成输出时往往伴随着大量无意义的自我重复（word salad），这不仅浪费了解码预算，还影响用户体验。

Method: 通过分析隐藏状态中的特定模式（如紧随推理块的<\n\n>标记），使用单层线性分类器实时检测word salad行为，并通过简单的截断和重新生成提示来优化输出。

Result: 提出的WordSaladChopper（WSC）能够显著减少输出长度，且在语义冗余极少的情况下几乎不影响输出质量。

Conclusion: WSC作为一种低开销、高效的工具，应成为所有注重用户体验的大型推理模型应用的必备组件。

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [20] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为CISEA-MRFE的新框架，结合了上下文指令（CI）、语义增强增强（SEA）和多精炼特征提取（MRFE），以解决情感分析中现有方法的不足，特别是在处理复杂情感、领域偏移和情感分布不平衡时的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习与预训练语言模型（PLMs）的情感分析方法在处理微妙情感线索、领域差异以及情感分布不平衡时表现不佳。这些问题源于语义基础不足、对多样化语言模式的泛化能力差，以及对主要情感类别的偏见。

Method: 提出CISEA-MRFE框架，包括：1）上下文指令（CI）注入领域感知指令以指导情感消歧；2）语义增强增强（SEA）通过情感一致的变体增强提高鲁棒性；3）多精炼特征提取（MRFE）结合了尺度自适应深度编码器（SADE）进行多尺度特征专业化，以及情感评估上下文编码器（EECE）进行情感感知序列建模。

Result: 在四个基准数据集（IMDb、Yelp、Twitter、Amazon）上的实验结果显示，CISEA-MRFE显著优于基线方法，准确率相对提升分别达到4.6%、6.5%、30.3%和4.1%。

Conclusion: CISEA-MRFE框架在情感分类任务中表现出优异的有效性和泛化能力，尤其在不同领域和复杂情感场景中表现突出。

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [21] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 论文提出了一种名为ISA（意图转移攻击）的新方法，通过最小编辑使恶意请求被误认为良性信息请求，显著提高了攻击成功率，并揭示了现有防御措施的不足。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）的脆弱性，特别是针对越狱攻击的防御不足，旨在通过意图转移攻击揭示并改进模型的安全性。

Method: 引入ISA攻击，通过建立意图转换的分类法，生成被LLMs误认为良性请求的攻击，仅需对原始请求进行最小编辑即可实现。

Result: 实验表明ISA攻击对开源和商业LLMs的攻击成功率提高了70%以上，且通过良性数据微调后成功率接近100%，现有防御方法对其无效。

Conclusion: ISA攻击揭示了LLMs在意图推断上的根本挑战，呼吁开发更有效的防御机制。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [22] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: FlashEVA是一种高效的EVA实现，通过控制变量优化注意力机制，提升了Transformer模型在推理阶段的吞吐量并降低了GPU内存使用，同时保持了模型性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然性能优异，但其内存需求大，尤其是在推理阶段需要保持完整上下文，因此需要更高效的实现方法。

Method: 提出FlashEVA，一种基于EVA的高效注意力实现，并通过调整超参数权衡吞吐量与精度，支持对小规模数据（1.5B tokens）的微调。

Result: FlashEVA在推理中实现了6.7倍更高的吞吐量和5倍更低的GPU峰值内存使用，但在检索任务中存在局限性。

Conclusion: FlashEVA为Transformer模型提供了高效且灵活的推理解决方案，但在特定任务中仍需进一步优化。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [23] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: SpecDiff-2提出了一种新框架，通过离散扩散技术解决了推测解码中的两个瓶颈，实现了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法因自回归依赖和模型对齐问题导致并行性受限和频繁拒绝草案标记，影响效率。

Method: 使用离散扩散作为非自回归草案器，校准草案器与验证器，提升并行性和对齐性。

Result: 在多个基准测试中，SpecDiff-2比基线平均提速55%，最高达5.5倍，且无准确率损失。

Conclusion: SpecDiff-2通过创新设计显著提升了LLM推理效率，成为新标杆。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [24] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在概率场景下输出的令牌级概率与理论概率的对齐问题，结果显示模型在响应准确性上表现完美，但概率和熵值与理论分布存在偏差。


<details>
  <summary>Details</summary>
Motivation: 可靠的UQ对于大型语言模型在下游任务中的可信使用至关重要。作者旨在验证模型在概率场景下输出的令牌级概率是否能与理论概率对齐。

Method: 使用GPT-4.1和DeepSeek-Chat模型，评估对10个涉及概率的提示（如有无明确概率线索）的响应，测量响应有效性和概率对齐性。

Result: 模型在所有提示场景中均表现出完美的响应准确性，但令牌级概率和熵值与理论分布显著偏离。

Conclusion: 尽管模型在响应准确性上表现优异，但其概率输出与理论分布的不一致表明当前UQ方法在概率场景下仍需改进。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [25] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 通过计算分析探讨法国侦探小说中侦探原型的演变，展示了原型在150年间的统一性与变化。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示侦探原型在法国文学中的演变过程及其受社会背景影响的变化。

Method: 采用定量方法和字符级嵌入技术，使用监督模型进行分析。

Result: 研究发现侦探角色从次要叙事角色发展为古典侦探故事的核心，并在二战后受硬汉派传统影响变得更复杂。

Conclusion: 法国侦探小说的原型经历了显著的演变，反映了社会暴力与道德模糊性的转变。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [26] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: XNationQA是一个多语言问答基准，涵盖49,280个问题，用于评估多语言大模型的文化素养，揭示其在不同语言和地区间的表现差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准多为西方中心，缺乏对地理文化多样性的考量，导致评估多语言模型的事实理解能力存在偏差。

Method: 研究者构建了XNationQA数据集，涵盖九个国家的地理、文化和历史问题，并用七种语言评估八个多语言大模型的表现，引入了两种新的转移度量指标。

Result: 模型在西方语言中表现更好，但在文化特定信息上的跨语言转移能力有限，开源模型尤为突出。

Conclusion: 研究表明多语言模型的文化素养存在显著差异，需进一步改进以提升跨语言和文化的能力。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [27] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 本文首次系统地评估了大型语言模型（LLM）在多语言环境下对越狱攻击及其防御的泛化能力，发现高资源语言在标准查询下更安全，但对对抗性攻击更脆弱。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM经过安全对齐训练，但越狱攻击仍能绕过其安全性。目前缺乏对其在多语言环境下安全性的系统性研究。

Method: 在十种不同资源水平的语言中，使用六种LLM评估逻辑表达型和对抗提示型两种越狱攻击，并测试简单防御方法的有效性。

Result: 高资源语言在标准查询中更安全，但对对抗性攻击更脆弱；简单防御方法有效，但效果受语言和模型影响。

Conclusion: 研究呼吁建立语言感知和跨语言的安全基准，以进一步提升LLM的安全性。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [28] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 论文通过改进原生稀疏注意力（NSA），提出交替使用局部和全局注意力机制，结合潜在注意力优化分支，提升了长文本建模性能，同时减少KV缓存内存50%。


<details>
  <summary>Details</summary>
Motivation: 现有NSA在长上下文建模中存在局限，研究旨在提出改进方法以提升长序列任务表现。

Method: 交替使用局部（滑动窗口）和全局（压缩、选择性）注意力机制，并引入潜在注意力（MLA和GLA）优化NSA分支。

Result: 实验显示，改进后的方法在常识推理和长文本理解任务中优于全注意力和原生稀疏注意力，同时减少50%KV缓存内存。

Conclusion: 交替注意力和潜在注意力优化显著提升了长上下文建模效果，为大规模模型提供了高效解决方案。

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [29] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: 论文提出TriCon-Fair框架，通过对比学习解耦偏见样本与非偏见样本的关系，减少语言模型的社会偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的去偏方法忽视了偏见样本与非偏见样本之间的相互影响，导致改进一个群体时可能损害另一个群体。

Method: TriCon-Fair采用对比学习框架，结合三元组损失和语言建模目标，解耦正负样本的耦合关系。

Result: 实验表明，TriCon-Fair在减少歧视性输出的同时保持了良好的下游任务性能。

Conclusion: TriCon-Fair为敏感NLP应用提供了一种实用且伦理的解决方案。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [30] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 论文提出了一种评估大型语言模型（LLMs）推理知识基础的新方法，通过构建知识库、设计评估指标和优化评估模型，系统验证推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有逐步推理方法虽有效，但缺乏对推理过程中知识基础的验证机制，亟需一种系统评估方法以确保推理的准确性。

Method: 提出了包含三个组件的评估框架：(1) 主要知识库收集，(2) 基于知识的评估指标，(3) 优化的轻量级评估模型。

Result: 评估套件有效识别推理中缺失或误用的知识元素，揭示了LLMs的推理缺陷，并展示了优化中的应用潜力。

Conclusion: 该框架不仅验证了推理的知识基础，还为优化提供了实用工具，拓展了评估的应用场景。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [31] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate是一种新型的多模态文档检索模型，通过OCR预训练目标和掩码对比学习，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态文档检索方法仅沿用文本检索技术的问题，提升对多模态文档结构和视觉特征的适应性。

Method: 采用OCR预训练目标、自监督掩码对比学习目标和针对多模态文档的晚期交互评分机制。

Result: 在ViDoRe V2基准测试中性能提升3.61%，表现出更强的跨领域泛化能力。

Conclusion: ColMate通过创新方法有效弥补了多模态表示学习与文档检索间的差距，显著优于现有模型。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [32] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: LLMs在医疗诊断沟通中展现潜力但存在输出复杂性和情感偏见问题，需系统性校准以确保公平性。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在医疗诊断情境中生成易理解且具共情力输出的能力，以支持临床医生与患者沟通。

Method: 通过可读性指标和LLM-as-a-Judge评分评估两大领先LLM的输出，并与人工评价对比。

Result: LLMs能适应患者社会人口学变量和病情调整解释，但产生过于复杂内容和情感偏见，导致支持不均。

Conclusion: 需系统性校准LLMs以确保患者沟通的公平性和可及性。

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [33] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 研究探讨了大语言模型（LLMs）在七种印度语言中的文化推理和自我评估能力，发现Gemini 2.5 Pro表现最佳，但高准确率模型通常缺乏自我纠错能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在非英语语言中文化推理能力的不足，尤其是印度语言的多样性和复杂性。

Method: 使用七种印度语言的谜语数据集，评估五种LLM模型在七种提示策略下的表现，分阶段测试推理能力和自我评估一致性。

Result: Gemini 2.5 Pro表现最优，但高准确率模型自我纠错能力较差；低准确率模型如LLaMA 4 Scout反而更善于识别自身错误。

Conclusion: 多语言推理能力存在明显差距，未来模型需兼顾推理能力和自我认知能力。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [34] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 提出了一种易到难的增强框架，用于在标签不精确的条件下提升机器生成文本检测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统机器生成文本检测方法假设标签是黄金标准，但实际上存在边界模糊性和人类认知局限，导致学习不精确。因此，需要一种新方法在不精确条件下提供可靠监督。

Method: 通过设计一个针对较长文本任务的简易监督器（尽管能力较弱），增强目标检测器。监督器理论上能减轻不精确标签的影响，并通过结构优化间接提升检测器性能。

Result: 在多种实际场景（如跨LLM、跨领域、混合文本和改写攻击）的实验表明，该框架显著提升了检测效果。

Conclusion: 提出的易到难框架有效解决了机器生成文本检测中的标签不精确问题，并通过理论建模和实验验证了其优越性。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [35] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL introduces a multi-agent framework for translating natural language to SQL, combining task decomposition and interactive RL, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Complex SQL queries require environmental interaction and self-correction, which existing methods struggle with.

Method: A multi-agent framework with Grounding, Generation, and Validation agents, using RL for iterative generation and generative modeling for verification.

Result: Achieves 77.84% Execution Accuracy on BIRD and 89.75% on Spider, demonstrating robust and accurate SQL generation.

Conclusion: MARS-SQL effectively solves complex SQL generation through specialized agents and interactive RL, setting new benchmarks.

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [36] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: IF-CRITIC是一种高效可靠的LLM评论模型，用于评估指令约束遵循情况，通过多阶段过滤机制和约束级偏好优化方法训练，优于现有的LLM-as-a-Judge基线。


<details>
  <summary>Details</summary>
Motivation: 现有指令遵循评估模型存在成本高、评估不可靠等问题，因此需要一种更高效的解决方案。

Method: 开发清单生成器分解指令并生成约束清单，通过多阶段过滤机制收集高质量评论数据，并采用约束级偏好优化方法训练IF-CRITIC。

Result: IF-CRITIC在评估性能上胜过Deepseek-R1和o4-mini等基线模型，并能以更低计算开销优化LLM的指令遵循能力。

Conclusion: IF-CRITIC为LLM指令遵循优化提供了高效可靠的评估信号，显著提升了性能。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [37] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: 论文提出Prompt-R1，一种端到端的强化学习框架，通过小规模LLM与大规模LLM协作解决用户提示不准确的问题，优化生成质量和推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在复杂问题中因用户提示不准确而性能受限，需改进交互方式。

Method: 使用小规模LLM生成提示，大规模LLM进行推理，设计双重约束奖励优化正确性、生成质量和推理准确性。

Result: 在多个公共数据集上，Prompt-R1显著优于基线模型。

Conclusion: Prompt-R1为可插拔框架，支持多种大规模LLM的推理和训练，有效提升性能。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [38] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI是一款结合开源大型语言模型的自然语言流畅性和NOAA实时海洋数据的对话平台，确保回答的可验证性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 为了解决通用对话AI系统在科学领域可能产生的未经验证的‘幻觉’问题，OceanAI通过整合权威数据源来提高科学严谨性。

Method: OceanAI通过实时API调用识别、解析和合成NOAA的数据流，生成可重复的自然语言回应和数据可视化。

Result: 在盲测中，OceanAI是唯一提供NOAA数据来源且有原始数据引用的平台，而其他产品要么拒绝回答，要么提供未经支持的结果。

Conclusion: OceanAI通过可验证的数据输出提升了透明度和信任度，为海洋领域的AI决策支持提供了可扩展的框架。

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [39] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat is a conversational system that helps users analyze air quality data through natural language, providing Python code and visualizations, making data accessible to non-experts.


<details>
  <summary>Details</summary>
Motivation: Air pollution causes significant premature deaths in India, but existing tools are static and require expertise. VayuChat aims to democratize access to air quality data and analysis.

Method: The system integrates data from CPCB monitoring stations, demographics, and NCAP funding records, using large language models to answer natural language queries and generate executable code and visualizations.

Result: VayuChat enables users to perform complex environmental analytics through simple conversations, making it accessible to policymakers, researchers, and citizens.

Conclusion: VayuChat successfully bridges the gap between dispersed data and actionable insights, providing a user-friendly interface for air quality analysis.

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [40] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 研究提出了一种基于公开指南的标准数据集，用于评估大型语言模型（LLMs）在临床推理中的表现，并通过测试多个流行LLMs验证了数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏标准化基准来评估LLMs在基于指南的临床推理中的表现，研究旨在填补这一空白。

Method: 利用公开指南和GPT生成包含真实患者场景和临床问题的数据集，并测试多个流行LLMs。

Result: 数据集被证明有效，可用于评估LLMs的临床实用性和指南遵循能力。

Conclusion: 该框架为系统评估LLMs在医疗领域的应用提供了重要工具。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [41] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Bañón,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Hajič,Jindrič Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyán O'Brien,Lucie Poláková,Sampo Pyysalo,Gema Ramírez Sánchez,Janine Siewert,Pavel Stepachev,Jörg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtěchová,Jaume Zaragoza*

Main category: cs.CL

TL;DR: 该论文介绍了一个正在进行的开放倡议，旨在为近200种语言提供大规模、高质量且标注丰富的文本数据集。数据规模达30万亿标记，可能是目前最大的多语言LLM预训练数据集。


<details>
  <summary>Details</summary>
Motivation: 动机是解决多语言文本数据的稀缺性问题，并为研究社区提供高质量、开放的资源，支持语言模型的预训练和评估。

Method: 方法包括从不同来源的网页爬取数据，通过开源流程进行文档选择、文本提取、语言识别、去重、标注（包括注册标签、文本质量估计等），以及最终筛选。

Result: 结果包括数据质量分析（通过对比统计和人工检查）以及训练和评估多种语言模型架构，包括57种单语编码器-解码器模型和几个参考GPT模型。

Conclusion: 结论是这些数据集和模型为多语言和单语LLM研究提供了丰富的资源，并提供了基准测试和并行文本集合。

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [42] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 研究分析了罗马尼亚语预训练数据的特点和覆盖率，并通过轻量级多任务模型验证了数据过滤对提升大型语言模型性能的有效性。


<details>
  <summary>Details</summary>
Motivation: 高质量数据对训练大型语言模型（LLMs）至关重要，尤其是对于资源稀缺的低代表性语言（如罗马尼亚语）。研究旨在填补罗马尼亚语数据与英语数据之间的差距。

Method: 通过训练一个轻量级多任务模型，对经过LLMs标注的罗马尼亚语文本进行多级过滤（如教育价值、主题、格式），生成高质量的预训练数据集。

Result: 实验揭示了罗马尼亚语和英语数据在主题分布上的显著差异，并证明了数据过滤能有效提升LLMs在多个基准测试中的表现。

Conclusion: 研究强调了高质量数据过滤的重要性，尤其是在资源稀缺语言中，这有助于提升LLMs的性能和数据覆盖率。

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [43] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: TSVer是一个专注于时间序列证据下事实验证的新基准数据集，包含真实世界的声明和时间序列数据，并通过LLM辅助的多步骤标注提高了标注质量。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在时间序列和数值推理方面的评估存在局限性，缺乏结构化证据或不充分的判决理由。

Method: 引入TSVer数据集，包含287个真实世界的声明和400个时间序列，采用LLM辅助的多步骤标注流程。

Result: 标注质量高（kappa=0.745），但现有最先进的推理模型（如Gemini-2.5-Pro）在验证时间序列声明时表现有限（准确率63.37%）。

Conclusion: TSVer为时间序列和数值推理的事实验证提供了新的基准，揭示了当前模型的挑战和局限。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [44] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: DebateBias-8K是一个多语言、辩论风格的基准测试，用于评估大型语言模型在生成文本中的叙事偏见，发现模型在高风险和低资源语言中均表现出固有的刻板印象。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型偏见评估多依赖英语分类任务，缺乏对多语言生成场景中偏见的研究。

Method: 使用DebateBias-8K数据集（8,400个辩论提示，覆盖七种语言和四个敏感领域），测试四个主流模型（GPT-4o、Claude 3、DeepSeek、LLaMA 3）的偏见表现。

Result: 模型普遍存在刻板印象，如阿拉伯人与恐怖主义高度关联，非洲人与社会经济落后性关联，且偏见在低资源语言中更显著。

Conclusion: 当前的安全对齐方法无法有效避免开放生成场景中的偏见，需开发更具文化包容性的多语言对齐方法。

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [45] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: 提出ZoFia框架，通过两阶段零样本检测虚假新闻：首先量化实体重要性并检索证据，然后多LLM协作分析，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虚假新闻危害社会稳定性，且现有LLMs因知识覆盖有限和生成幻觉内容难以应对快速变化的新闻流。

Method: ZoFia框架包含两阶段：1）利用Hierarchical Salience和SC-MMR算法选择关键词并检索证据；2）多LLM协作分析与对抗辩论。

Result: 在公开数据集上，ZoFia明显优于现有零样本和小样本方法。

Conclusion: ZoFia为虚假新闻检测提供了一种高效且可解释的解决方案，代码将开源以促进相关研究。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [46] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: Self-Harmony是一种无需标签的测试时强化学习框架，通过模型自身生成问题与复述问题的答案，利用谐波均值伪标签方法提高稳定性，实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 标准方法（如多数投票）容易受到虚假但流行的答案影响，因此需要一种无需人工监督或辅助模型的可靠学习信号。

Method: 引入Self-Harmony框架，利用单个模型在解决者和复述者两个互补角色中生成答案，并通过谐波均值聚合伪标签。

Result: 在30种设置中的28种中排名第一，展现出前所未有的鲁棒性，所有实验中均无训练失败。

Conclusion: Self-Harmony无需人工干预即可显著提升模型在测试时的稳定性和准确性。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [47] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: 论文提出了一种名为DEER的新框架，通过两阶段的解耦混合专家架构，结合领域特定和领域通用的模式来检测机器生成文本，显著提升了跨领域的检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）生成高度逼真的人类文本的能力增强，机器生成文本的检测成为一个重要挑战。现有方法在领域转移时性能大幅下降，亟需解决这一问题。

Method: 论文提出了一种解耦混合专家架构（DEER），包含两个阶段：1）领域特定专家学习细粒度的领域本地特征，共享专家提取可跨领域迁移的特征；2）通过强化学习的动态路由机制，在推理时无需领域标签即可选择适当的专家。

Result: 在五个领域内和五个领域外的基准数据集上，DEER方法显著优于现有技术，F1分数分别提升1.39%和5.32%，准确率分别提升1.35%和3.61%。

Conclusion: DEER框架通过解耦专家分工和自适应路由，有效解决了机器生成文本检测中的领域转移问题，为跨领域检测提供了高性能的解决方案。

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [48] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: 该论文研究了领域特异性对阿拉伯语金融文本摘要的影响，并引入了AraFinNews数据集，展示了领域适应模型在生成更准确摘要方面的优势。


<details>
  <summary>Details</summary>
Motivation: 探讨领域特异性如何提升阿拉伯语金融文本摘要的准确性、数字可靠性和风格一致性。

Method: 使用mT5、AraT5和FinAraT5等基于Transformer的模型，评估金融领域预训练的效果。

Result: 实验表明，领域适应模型生成的摘要更忠实、连贯，尤其是在处理定量和实体中心信息时。

Conclusion: 领域特异性适应对于提高阿拉伯语金融摘要的事实一致性和叙述流畅性至关重要。

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article--headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [49] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: ReSpec是一种新颖的检索增强型推测解码框架，通过自适应决策和三种核心创新，显著加速大型语言模型推理并保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码技术的速度提升依赖于草稿模型的有效性，而现有方法如EAGLE-2成本高，SAM-Decoding则依赖启发式切换策略导致不必要检索。

Method: ReSpec提出三种创新：熵引导的自适应触发器、反馈驱动的候选选择策略和源感知的宽松验证策略。

Result: 在Spec-Bench上的实验显示，ReSpec比EAGLE-2和SAM-Decoding分别加速超过33%和25%，同时保持输出质量。

Conclusion: ReSpec通过自适应决策和优化策略，成为推测解码领域的高效解决方案。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [50] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: 这篇论文研究了AI辅助论文评审中存在的注入提示攻击威胁，提出了两类攻击方法，并探讨了一种简单的防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的广泛应用，一些论文中隐藏的注入提示可能操纵AI评审员给出过高评价，亟需系统性研究以应对这一威胁。

Method: 提出了两类攻击方法：静态攻击（固定注入提示）和迭代攻击（优化注入提示以最大化效果），并测试了其在不同设置下的鲁棒性。同时探索了一种基于检测的防御策略。

Result: 两类攻击均表现显著，尤其是针对前沿AI评审员时能频繁获得满分评价。防御策略虽能显著降低攻击成功率，但自适应攻击者可部分规避防御。

Conclusion: 研究强调了在AI辅助同行评审中需要更多关注和严格防范注入提示威胁。

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [51] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: 论文介绍了FirstAidQA数据集，包含5500个高质量问答对，用于支持紧急情况下的轻量级语言模型开发。


<details>
  <summary>Details</summary>
Motivation: 解决紧急情况下缺乏适合低连接环境的轻量级语言模型和高质量数据集的问题。

Method: 通过ChatGPT-4o-mini生成数据集，并进行预处理和人工验证。

Result: 发布FirstAidQA数据集，支持离线紧急响应系统开发。

Conclusion: FirstAidQA推动了安全关键和资源受限AI研究的发展。

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [52] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: DeepSpecs是一个通过结构化和时间推理增强的RAG系统，用于解决5G标准的跨引用和演化问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架无法可靠解决5G标准中的跨引用或处理标准演化问题，因此需要更先进的系统。

Method: DeepSpecs通过三个元数据丰富的数据库（SpecDB、ChangeDB、TDocDB），递归检索引用的条款并通过元数据查找来解决跨引用，同时通过挖掘变化并将其链接到变更请求来追踪标准的演化。

Result: DeepSpecs在多个LLM后端上优于基础模型和最先进的电信RAG系统，跨引用解析和演化感知检索显著提高了答案质量。

Conclusion: 建模5G标准的结构和时间特性对提高答案质量至关重要，DeepSpecs验证了这一点。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [53] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: 论文提出DeepAmbigQAGen自动生成包含名字歧义和多步推理的QA任务，构建DeepAmbigQA数据集，实验显示现有模型如GPT-5在回答歧义问题时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有开放域问答系统在处理复杂问题（如涉及名字歧义和多步推理）时表现不足，缺乏能同时评估这两项挑战的基准。

Method: 开发了DeepAmbigQAGen自动数据生成管道，基于文本语料库和知识图谱生成自然且可验证的问题，构建DeepAmbigQA数据集。

Result: 实验表明，即使是GPT-5在歧义问题上准确率仅为0.13，非歧义问题为0.21，显示现有模型的不足。

Conclusion: 研究强调了需要更强大的问答系统，以提高信息收集和答案完整性的能力。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [54] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 本文扩展了DistilQwen模型家族，推出四种新模型系列以满足工业需求，包括慢思考模型、自适应思考模型和奖励模型，展示了高推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 满足小型高效推理模型的需求，平衡推理性能和速度。

Method: 从Qwen模型初始化，设计四种模型系列：慢思考、自适应思考和奖励模型，并在Alibaba Cloud PAI平台上部署。

Result: 多基准测试显示高推理效率和强推理性能，奖励模型具有实用价值。

Conclusion: 这些模型支持工业实践，提供可扩展的训练和推理功能。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [55] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 该论文提出了一个改进LLM输出事实性的方法，通过将NLI模型的推理任务泛化为适用于任意文本前缀，并训练了一个新模型MiniTruePrefixes，显著提升了前缀级蕴含检测的性能，进一步改善了摘要生成的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有NLI模型虽然能检测完整句子的事实不一致性，但无法直接应用于自回归生成架构中逐前缀生成的场景。因此，需要一种新方法来提升生成文本的忠实性。

Method: 论文将蕴含检测任务推广到任意文本前缀，提供了新的评估和训练数据集，并训练了专用模型MiniTruePrefixes。该模型通过控制解码框架整合到生成过程中，提升了事实一致性。

Result: MiniTruePrefixes在前缀级蕴含检测中，比基线NLI模型提升了5-14 F1分。在摘要生成任务中，LLaMA-3.2-3B-Instruct模型结合MiniTruePrefixes后，事实一致性和运行时性能与更大模型相当。

Conclusion: 研究表明，MiniTruePrefixes能有效提升生成文本的事实一致性，并在资源效率上优于基线模型。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [56] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: 该论文旨在解决多语言大语言模型（LLM）在医疗领域评估中的空白，通过构建Cancer-Myth-Indic基准，将500项癌症误解问题从英语翻译为5种印度次大陆常用语言，并评估LLM在这些语言中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多人依赖LLM获取医疗建议，评估其回答的准确性和有效性变得重要，但现有医学基准主要针对英语，缺乏多语言评估。

Method: 研究翻译了500项癌症误解问题到5种语言（共2500项），由母语译者遵循风格指南完成翻译，并评估了多个流行LLM在这些语言中的表现。

Result: 研究提供了一个多语言医疗基准Cancer-Myth-Indic，填补了现有文献空白，并通过评估展示了LLM在多语言环境中的表现。

Conclusion: 通过Cancer-Myth-Indic基准，研究为多语言LLM在医疗领域的评估提供了工具，强调了多语言评估的重要性。

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [57] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*İbrahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: 论文讨论了大型语言模型（LLMs）和大型推理模型（LRMs）评估中基准测试的饱和问题，质疑是否超越基准真正体现推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于基准测试结果的快速饱和，促使需要新的更具挑战性的替代品，同时质疑超越基准是否真实反映推理能力。

Method: 方法包括对三个模型家族（OpenAI、Anthropic、Google）在不同基准测试上的推理能力演变进行分析，以及对不同推理任务的性能趋势研究。

Result: 结果表明基准测试的快速饱和，以及模型性能提升背后的实际推理能力是否被真实衡量的问题。

Conclusion: 结论强调了基准测试在当前模型评估中的局限性，为未来推理评估和模型开发提供了参考。

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [58] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: RAGSmith框架通过遗传搜索优化RAG系统的多个模块，提升检索与生成的联合性能，平均优于基线3.8%。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统的优化通常孤立进行，导致性能不稳定，需要通过端到端架构搜索提升整体效果。

Method: 引入RAGSmith框架，通过遗传搜索在46,080种配置中优化检索与生成的联合指标。

Result: 在六个领域中平均提升3.8%，检索和生成分别最高提升12.5%和7.5%，探索空间仅约0.2%。

Conclusion: RAGSmith为RAG系统提供实用优化方案，进化搜索对全流程优化具高效性。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [59] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: LiveSearchBench是一个自动化流水线，用于构建依赖检索的动态基准测试，以评估大型语言模型在最新知识更新上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的问答基准测试往往依赖静态数据，忽视了检索的作用，而LiveSearchBench旨在解决这一问题，捕捉知识的动态特性。

Method: 通过计算Wikidata快照之间的差异，筛选高质量三元组，并生成三个推理难度级别的自然语言问题，确保答案唯一且可验证。

Result: 实验表明，模型在面对预训练后新增的事实时会显著下降，检索增强方法和更大规模的指令调优模型能部分缓解这一问题，但无法完全解决。

Conclusion: LiveSearchBench将评估重点从静态记忆转向需要最新检索和推理的任务，为长期评估模型在动态知识下的表现提供了基础。

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [60] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: 该论文提出了一种可重复的基于草稿的精炼流程，利用开源大语言模型（LLMs）在翻译形态丰富的低资源语言（如拉丁语）时，达到与顶级专有系统相当的性能。


<details>
  <summary>Details</summary>
Motivation: 翻译形态丰富且资源匮乏的语言（如拉丁语）存在显著挑战，需要一种高效且可重复的方法来提升翻译质量。

Method: 首先使用微调的NLLB-1.3B模型生成高质量的草稿，然后通过零样本LLM（Llama-3.3或Qwen3）进行精炼，并可进一步通过检索上下文外的示例（RAG）增强效果。

Result: 该方法在两个不同基准测试中表现出色，包括标准域内测试集和新的域外12世纪拉丁字母集，性能与GPT-5基准相当。

Conclusion: 开源RAG系统无需任务特定的LLM微调即可达到高性能，推动了可重复性和进一步研究的发展。

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [61] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: 提出了一种新颖的Budget-Aware Reasoning Distillation (BARD)框架，通过动态平衡推理性能和计算效率，实现对推理长度的精细控制。


<details>
  <summary>Details</summary>
Motivation: 现有的长链思维蒸馏方法在推理过程中存在冗余和计算预算不可控的问题，导致资源使用效率低下。

Method: BARD采用两阶段训练方案：第一阶段通过监督微调（SFT）在不同预算水平上压缩教师模型生成的长链思维数据；第二阶段利用强化学习（RL）同时优化推理性能和预算保真度。

Result: 实验证明，该方法能在挑战性推理基准（AIME24, AIME25, GPQA）上实现高性能，并精确控制推理长度。

Conclusion: BARD框架有效解决了推理冗余和计算预算控制问题，为学生模型提供了高效的推理能力与精细的资源管理。

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [62] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: 使用大型语言模型（LLM）作为一致且可靠的标注器，通过多次独立运行揭示稳定的标注模式，并提出一种数据集无关的评估框架。结果显示，GPT-4能产生一致的标注，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于认知扭曲检测的主观性导致人工标注不一致，探索LLM作为替代标注器的可能性。

Method: 利用独立运行的LLM揭示标注模式，引入基于Cohen's kappa的数据集无关评估框架。

Result: GPT-4标注一致性高（Fleiss's Kappa = 0.78），训练模型性能优于人工标注数据。

Conclusion: LLM可为主观NLP任务提供可扩展且内部一致的训练数据生成方案。

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [63] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: 研究探讨了合成数据来源多样性对微调大语言模型的影响，发现多样性合成数据可以缓解分布坍缩，并提高输出质量和潜在风险；同时减少自偏好偏差。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在语言模型开发中的广泛应用，理解其对模型行为的影响变得至关重要。

Method: 研究聚焦于三个关键维度：分布坍缩、对抗鲁棒性和自偏好偏差，通过分析不同来源的合成数据微调模型的表现。

Result: 多样性合成数据能缓解分布坍缩，保留输出多样性和质量；合成数据微调可能提高输出可用性和潜在危险；多源合成数据减少自偏好偏差效果仅次于人类数据。

Conclusion: 合成数据来源的多样性对模型行为有重要影响，需在应用中权衡其利弊。

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [64] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文提出了一种结合Pareto优化大语言模型和Chain-of-Thought提示的孟加拉语文本去毒方法，并构建了BanglaNirTox数据集支持研究。结果表明该方法显著提升了去毒效果。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语中的有毒语言在在线环境中普遍存在，但缺乏有效的预防措施。由于资源有限，孟加拉语的文本去毒研究进展缓慢，亟需探索新方法。

Method: 提出一种新流程，结合Pareto优化的LLMs和CoT提示生成去毒句子，并构建BanglaNirTox数据集（含68,041条句子及其标签、推理和去毒版本）用于模型微调。

Result: 实验表明，Pareto优化的LLMs结合CoT提示显著提高了孟加拉语文本去毒的质量和一致性。

Conclusion: 该方法为资源受限语言的文本去毒提供了有效解决方案，未来可扩展至其他低资源语言。

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [65] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 论文提出了一种通过数据增强和多任务学习生成可控难度干扰项的新框架，显著提升了干扰项质量与难度对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成干扰项时缺乏难度可控性和适应性，且缺乏难度标注数据集，限制了技术发展。

Method: 采用双向干扰项生成和QA系统分类标注难度，结合多任务学习训练可控难度生成模型。

Result: 实验表明，该方法能生成高质量干扰项，且在难度对齐上优于GPT-4o。

Conclusion: 该方法为干扰项生成提供了有效的难度控制手段，显著提升了生成质量和适应性。

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [66] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: 研究使用行為形式心理網絡模型，探索了心理學學生對數學和焦慮概念的感知與關聯差異，並比較了人類學生與GPT模擬學生的結果，發現人類學生的網絡特徵能預測數學焦慮，而GPT模擬數據則無效。


<details>
  <summary>Details</summary>
Motivation: 數學焦慮對大學心理學學生的職業選擇和整體福祉有重大影響，研究旨在通過認知模型探索這種焦慮的個體和群體差異。

Method: 進行了4個實驗，包括人類心理學本科生（n1=70，n2=57）和GPT模擬學生（GPT-3.5: n2=300；GPT-4o: n4=300），使用行為形式心理網絡模型分析個體和群體層面的概念關聯。

Result: 人類學生中，對'焦慮'的正向評分和高網絡度，以及對'數學'的負向評分，能預測更高的數學焦慮；GPT模擬數據則無此關聯。高數學焦慮學生對'焦慮'的情緒感知極化，而'科學'被正面評價但與'數學'形成對比。

Conclusion: 研究表明，理解概念的感知與關聯對管理數學焦慮至關重要，同時突顯了人類與AI模型在心理學研究中的差異。

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [67] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: 论文提出了一种基于熵的动态调整控制强度的解码方法（ECO解码），用于改善对话生成的控制性和流畅性，并在单属性和多属性场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用固定常数调整属性概率偏差，难以同时满足控制性和流畅性需求，因此需要动态调整控制强度。

Method: 提出ECO解码，根据语言模型和属性分类器的概率分布熵，动态调整每一步生成的控制强度。

Result: 实验显示，ECO解码在控制性和流畅性上均优于现有方法，并在多属性生成中缓解了概率插值问题。

Conclusion: ECO解码是一种有效的动态调整方法，适合单属性和多属性的可控对话生成任务。

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [68] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 研究探讨了语言错误如何揭示人类语言的认知架构，并通过分析西班牙语母语者的语言错误，评估大型语言模型对其的解释与纠正能力。


<details>
  <summary>Details</summary>
Motivation: 揭示语言错误不仅能帮助理解语言的认知架构，还能暴露人工智能系统在模拟人类语言时的局限性。

Method: 研究整合理论语言学、神经语言学和自然语言处理，通过构建500多个西班牙语母语者真实错误的语料库，测试GPT或Gemini等AI模型对这些错误的解释能力。

Result: 通过实证分析，评估AI模型对语言错误的解释准确性和对人类语言行为的泛化能力。

Conclusion: 该研究不仅深化了对西班牙语作为母语的理解，还推动了更符合认知的自然语言处理系统的发展。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [69] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 该研究探讨了在图基础的检索增强生成（RAG）架构中使用大型语言模型（LLMs）进行能源效率（EE）问答的效果。通过从能源领域的指导文件中提取知识图（KG），并结合多语言支持，验证结果显示系统的准确率为75.2%（±2.7%），在通用EE问题上表现更佳（81.0%±4.1%），多语言能力也展现出潜力。


<details>
  <summary>Details</summary>
Motivation: 解决能源效率（EE）领域的问答需求，通过结合知识图和大型语言模型，提高回答的准确性和多语言支持能力。

Method: 1. 从能源领域的指导文件中自动提取知识图（KG）；2. 在图基础上进行检索和推理，生成多语言的准确答案；3. 使用RAGAs框架和101个问答对进行人工验证，并由领域专家参与评估。

Result: 系统在75.2%（±2.7%）的情况下能正确回答问题，通用EE问题的准确率更高（81.0%±4.1%）。多语言能力虽有4.4%的准确率损失，但仍显示出潜力。

Conclusion: 该架构在能源效率问答中展现出潜力，尤其在通用问题和多语言支持方面表现突出，但仍需进一步优化准确性。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [70] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: 提出了一个认知基准框架，评估大语言模型处理和应用文化特定知识的能力，整合布鲁姆分类法和检索增强生成技术，测试语义准确性和文化相关性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估大语言模型在处理文化特定知识时的表现，特别是在台湾客家数字文化档案中的应用。

Method: 方法结合布鲁姆分类法的六个认知层次（记忆、理解、应用、分析、评估、创造）和检索增强生成技术，通过台湾客家文化档案测试模型的语义准确性和文化相关性。

Result: 研究结果未明确提及，但通过框架评估了模型的表现。

Conclusion: 结论可能是该框架能有效评估大语言模型在文化特定知识处理中的表现。

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [71] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: EngChain是一个针对多步工程问题解决能力的基准测试，旨在填补当前评估LLMs复杂推理能力的空白。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型评估基准未能涵盖工程领域所需的综合推理能力，而工程领域需要科学原理、定量模型和实际约束的融合。

Method: 通过符号模板生成90个涉及三个工程分支的问题，采用两阶段评估：定量验证每个推理步骤的数值和语义有效性，再通过LLM-As-A-Judge系统定性分类错误。

Result: EngChain提供了多样化的工程问题集，并通过定量和定性方法全面评估LLMs的推理能力。

Conclusion: EngChain为LLMs在工程领域的高阶推理能力评估提供了新的基准，填补了现有评估的空白。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [72] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: SeaLLMs-Audio是首个面向东南亚语言（印尼语、泰语、越南语）及英语、中文的大型音频语言模型，支持多任务和多模态输入，性能优异。


<details>
  <summary>Details</summary>
Motivation: 推动东南亚地区音频大型语言模型的研究与应用，填补多语言音频任务领域的空白。

Method: 利用大规模音频语料库训练，支持多模态输入（音频、文本或混合）和多任务处理（如语音识别、情感分析等）。

Result: 模型在多任务和多语言环境下表现优异，实验表明其在东南亚语言任务上与其他领先模型竞争。

Conclusion: SeaLLMs-Audio为东南亚研究及产业提供了重要工具，推动了音频语言模型的发展。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [73] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 论文介绍了首个公开的性格训练方法，通过Constitutional AI和合成自省数据塑造AI助手个性，比传统方法更有效且不影响通用能力。


<details>
  <summary>Details</summary>
Motivation: 探讨AI助手性格对交互质量和价值观的影响，填补学术研究中性格训练的空白。

Method: 使用Constitutional AI和合成数据管道，对三个开源模型进行微调，分析揭示的偏好以追踪性格变化。

Result: 性格变化对对抗性提示更稳健，生成内容更连贯真实，且不影响通用能力。

Conclusion: 提出的方法有效控制AI助手的性格，为后续研究提供了开源工具。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [74] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 摘要提出了一个新的rank-2投影子空间，用于更准确地解耦大语言模型（LLM）中参数知识（PK）和上下文知识（CK）的贡献，并通过多步分析揭示不同知识交互形式。实验表明，该方法比传统的rank-1子空间更能捕捉多样化的知识交互。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解大语言模型如何通过自然语言解释（NLEs）结合参数知识和上下文知识进行决策，并探索知识交互的不同形式，以评估NLEs的可靠性。

Method: 提出了一个rank-2投影子空间，用于更准确地解耦PK和CK的贡献，并对NLEs序列进行了首次多步分析。

Result: 实验结果表明，rank-2子空间能够更好地捕捉多样化的知识交互，而rank-1子空间则表现不足。多步分析揭示了幻觉NLEs偏向PK，忠实于上下文的NLEs平衡PK和CK，而Chain-of-Thought提示减少了PK依赖，使NLEs更偏向CK。

Conclusion: 该研究为通过更丰富的rank-2解耦子空间系统研究多步知识交互提供了首个框架。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [75] [Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue](https://arxiv.org/abs/2511.01720)
*Mahammad Nuriyev*

Main category: cs.CL

TL;DR: 提出了一种多专家系统，用于创建既能自然对话又能执行上下文动作的NPC，基于Qwen3和LoRA适配器，实现了高效计算且在竞赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决NPC在交互环境中实现自然对话和上下文动作执行的问题，提高计算效率和资源利用率。

Method: 使用Qwen3作为基础模型，结合LoRA适配器，实例化了三个专家模块：工具调用、工具响应解释和直接对话。

Result: 在Commonsense Persona-Grounded Dialogue Challenge 2025竞赛中排名第二，系统计算效率高，响应速度快，资源占用低。

Conclusion: 该系统成功实现了NPC的高效对话和动作执行，展示了多专家系统和LoRA适配器的有效性。

Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs)
capable of both natural dialogue and contextual action execution in interactive
environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)
adapters, we instantiate three specialists: tool calling, tool-response
interpretation, and direct dialogue. Our system comfortably meets the
computational efficiency requirements, delivering fast responses and
maintaining modest resource usage on L40S GPUs. In the Commonsense
Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at:
https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/

</details>


### [76] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 研究表明，语言模型的信念会随着上下文累积而改变，导致行为和响应不一致，影响用户体验和模型对齐。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在长时间互动和文本处理中，信念如何变化及其对行为和用户体验的影响。

Method: 通过多次讨论道德困境和政治问题，观察模型信念的变化；设计工具使用任务，验证行为是否与信念变化一致。

Result: GPT-5的信念在10轮讨论后改变54.7%，Grok 4在阅读对立政治文本后改变27.2%；行为变化与信念变化一致。

Conclusion: 语言模型的信念具有高度可塑性，长时间交互或阅读会导致信念偏移，影响其行为和可靠性。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [77] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 论文提出一种无需模型再训练的长度控制方法，通过提示工程技术显著提高了大型语言模型对长度约束的遵从性。


<details>
  <summary>Details</summary>
Motivation: 现有长度控制方法通常需要昂贵的模型再训练或复杂推理工具，而本文方法旨在通过提示工程实现精确长度控制。

Method: 采用结构化提示工程方法，结合规划和字数统计机制，要求模型严格遵守长度约束。

Result: 在六种先进模型上的实验表明，该方法显著提升了长度遵从性（最高37.6%），同时保持或提升了输出质量。

Conclusion: 该方法为需要精确长度控制的应用提供了即用解决方案，尤其适用于无法进行模型再训练的场景。

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [78] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: IMO-Bench是一套高级推理基准，针对国际数学奥林匹克（IMO）水平设计，旨在提升基础模型的数学推理能力。它包括答案测试和证明写作测试，Gemini Deep Think模型在其中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的数学推理评估要么过于简单，要么只关注简短答案，无法充分衡量模型的推理能力。因此，作者开发了IMO-Bench，以更全面地评估高级数学推理能力。

Method: IMO-Bench包括两个部分：IMO-AnswerBench（400道多样化奥数题）和IMO-Proof Bench（证明写作能力测试）。基准问题由专家团队审查，并包含自动评分指南。

Result: Gemini Deep Think模型在IMO-AnswerBench上得分80.0%，在IMO-Proof Bench的进阶问题上得分65.7%，显著优于其他模型。自动评分工具与人工评分相关性高。

Conclusion: IMO-Bench为社区提供了一个提升数学推理能力的工具，并在自动评估长篇答案方面取得了进展。作者希望它能推动数学推理能力的进一步发展。

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 提出了一种多模态虚假评论检测框架，结合文本和视觉特征，显著优于单模态方法，F1分数达0.934。


<details>
  <summary>Details</summary>
Motivation: 虚假评论威胁数字信任，现有方法仅依赖文本数据，无法捕捉多模态语义不一致性。

Method: 采用BERT编码文本特征，ResNet-50提取视觉特征，通过分类头融合预测评论真实性。

Result: 多模态模型在测试集上F1分数为0.934，能检测文本与图像间的不一致性。

Conclusion: 多模态学习对维护数字信任至关重要，为在线平台提供可扩展的内容审核方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [80] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: GEPOC是一个通用群体概念模型集，专门用于分析群体层面的研究问题。本文详细描述了基于奥地利公开数据的参数计算方法，包括数据源、算法及验证研究。


<details>
  <summary>Details</summary>
Motivation: 为GEPOC模型（特别是GEPOC ABM）提供稳定、可复现的数据处理流程，确保参数在特定国家或地区的有效性。

Method: 本文使用了自由可获取的公开数据，详细描述了数据的聚合、分解、融合、清理和缩放算法，并生成了参数文件。

Result: 通过GEPOC ABM模型进行了广泛的验证研究，证明了数据处理方法的有效性。

Conclusion: 通过公开数据和详细的数据处理方法，GEPOC模型的参数计算在奥地利应用中具有可靠性和可复现性。

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [81] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: QuantumBench是一个用于量子科学领域的基准测试，旨在评估大语言模型（LLM）在该领域的理解力和应用能力。


<details>
  <summary>Details</summary>
Motivation: 由于通用基准测试难以反映量子科学领域的需求，研究者开发了QuantumBench来填补这一空白，以确保LLM能准确掌握领域知识和符号表示。

Method: 研究者通过公开材料收集了约800个涵盖量子科学九个领域的问题，并将其整理为八选一的多选题数据集，用于评估多个现有LLM的性能。

Result: QuantumBench是首个针对量子领域的LLM评估数据集，用于分析LLM在量子科学中的表现及其对问题格式变化的敏感性。

Conclusion: QuantumBench旨在指导LLM在量子研究中的有效应用，推动该领域的科学工作流程。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [82] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: 本文介绍了ARC-GEN，一个旨在扩展ARC-AGI训练数据集的开源程序生成器，以提升人工智能通用智能的评估能力。


<details>
  <summary>Details</summary>
Motivation: 由于ARC-AGI的数据集规模有限，限制了算法的训练效果，本文提出ARC-GEN以扩展数据集并保持其分布特性。

Method: ARC-GEN是一个覆盖所有400个任务且模拟初始ARC-AGI-1发布特性的程序生成器。

Result: ARC-GEN成功生成了一个扩展的训练数据集，可用于验证2025年Google Code Golf Championship提交程序的正确性。

Conclusion: ARC-GEN为ARC-AGI提供了一个高效的数据扩展工具，有助于提升人工智能通用智能的评估能力。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [83] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: 提出了一种改进的增量选择算法，并验证了所有选定的猜想。


<details>
  <summary>Details</summary>
Motivation: 改进现有选择算法的效率和准确性。

Method: 基于[1]中的选择算法，提出了增量改进版本。

Result: 成功验证了所有选定的猜想，证明了改进算法的有效性。

Conclusion: 改进后的算法在性能和准确性上优于原算法。

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [84] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: LLMs可以作为认知科学中的辅助工具，帮助解决知识合成和概念清晰性问题，但其使用需谨慎以补充人类专业知识。


<details>
  <summary>Details</summary>
Motivation: 认知科学因多学科性和复杂性面临知识合成和概念清晰性挑战，LLMs提供了潜在解决方案。

Method: 综述LLMs在当前认知科学中的应用潜力，分析其在不同领域的优缺点。

Result: LLMs能支持跨学科联系、理论形式化、明确测量分类等，但需注意其局限性。

Conclusion: LLMs在认知科学中有潜力，但需谨慎使用以辅助人类专家而非取代。

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [85] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: DAF-MIT AI Accelerator项目通过公开挑战和数据集推动AI研究，更新了其对AI技术和研究的贡献。


<details>
  <summary>Details</summary>
Motivation: 通过与美国空军和MIT的合作，推动AI基础研究，提升美国在国防和民用领域的竞争优势。

Method: 开发和发布公开挑战问题，提供大规模、公开可用的AI就绪数据集，促进开源解决方案和广泛参与。

Result: AI Accelerator挑战成功推动了AI研究及其技术应用。

Conclusion: 该项目通过合作和开放方式，在AI研究和应用中取得了显著进展。

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [86] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: 论文介绍了CLAUSE基准测试，用于评估大型语言模型在法律推理中的脆弱性，发现模型常忽略细微错误且难以合法解释，为法律AI的改进提供了方向。


<details>
  <summary>Details</summary>
Motivation: 针对法律领域中大型语言模型缺乏系统性基准测试的问题，研究者开发了CLAUSE，以测试模型在复杂法律合同中的可靠性。

Method: 通过生成7500多个扰动合同，并结合RAG系统验证法律准确性，CLAUSE评估了模型检测和解释法律缺陷的能力。

Result: 研究发现，现有模型难以发现细微错误，且在法律解释方面表现更差。

Conclusion: CLAUSE为识别和改进法律AI的推理缺陷提供了重要工具和方向。

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [87] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 提出了一种新的LLM伦理推理框架，通过五步结构化流程提升多样人类价值观对齐能力，实验表明优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM价值观对齐方法常流于表面，无法真正理解复杂、情境依赖的人类价值观，亟需理论基础的改进方法。

Method: 基于成熟伦理决策模型设计五步范式：事实收集、社会规范识别、选项生成、多视角伦理分析、反思；支持提示工程或监督微调实现。

Result: 在SafeWorld基准测试中显著优于基线，社会规范识别准确性与文化适配推理能力均提升。

Conclusion: 该框架为LLM实现全球化多元价值对齐提供了可解释、可落地的研究方向。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [88] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: 不同微调方法对LLMs安全性和公平性的权衡分析


<details>
  <summary>Details</summary>
Motivation: 组织越来越多地采用和调整托管在公共存储库如HuggingFace上的大型语言模型（LLMs），但研究表明这些调整可能在提升性能的同时降低模型的安全性或公平性。因此，本文旨在系统评估不同微调方法对这些关键维度的权衡影响。

Method: 研究采用四种广泛使用的参数高效微调（PEFT）方法（LoRA、IA3、Prompt-Tuning和P-Tuning），应用于四种指令微调模型家族（Meta-Llama-3-8B、Qwen2.5-7B、Mistral-7B和Gemma-7B），并评估235个微调变体在11个安全风险类别和9个人口统计学公平维度上的表现。

Result: 基于适配器的方法（如LoRA、IA3）能提升安全性评分且对公平性破坏最小，而基于提示的方法（Prompt-Tuning和P-Tuning）通常降低安全性并导致更大的公平性退化。基础模型类型对对齐效果有显著影响，不同模型表现不一；安全性与公平性间存在固有的权衡关系。

Conclusion: 研究表明，安全关键部署应选择对齐良好的基础模型，优先采用适配器型PEFT方法，并在安全和公平性上开展类别专项审计。

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [89] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: 论文提出了一种多模态框架，结合文本、用户特定和图像分析，用于检测社交媒体用户的抑郁症，并在新冠疫情背景下取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情导致心理健康问题激增，传统检测方法存在数据稀疏性和忽视多模态信息的不足，因此需要利用社交媒体数据进行更有效的抑郁症检测。

Method: 提出多模态框架，整合文本、用户特定特征和图像分析，并引入深度学习模型VNN生成图像嵌入，结合URL内容提取和图像文本分析。

Result: 模型在基准数据集上表现优于现有方法2%-8%，并在新冠数据集上取得了有前景的结果。

Conclusion: 多模态方法显著提升了抑郁症检测的准确性，并为用户心理健康状态分析提供了新视角。

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [90] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: GraphChain框架通过动态工具序列让大语言模型有效分析复杂图数据，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在处理大规模图数据时受限于上下文长度和推理灵活性，需要更高效的解决方案。

Method: 提出GraphChain框架，结合渐进图蒸馏和结构感知测试时适应两大创新，动态优化工具序列和选择策略。

Result: 实验表明GraphChain显著优于现有方法，实现了可扩展且自适应的图数据分析。

Conclusion: GraphChain为大语言模型在图数据分析中的应用提供了高效、灵活的新思路。

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [91] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: Magic Image是一种基于优化的视觉提示框架，解决了LLMs和MLLMs在生成有害内容和过度拒绝良性查询上的挑战，适应不同价值系统且无需参数更新。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和多模态大语言模型（MLLMs）面临生成有害内容和过度拒绝良性查询的双重问题，传统方法无法适应不同价值系统。

Method: 提出Magic Image框架，通过优化图像提示（基于有害/良性样本）使单一模型适应不同价值系统，无需参数更新。

Result: 实验表明Magic Image在多种数据集上提高了安全性与效果的平衡，同时保持了模型性能。

Conclusion: Magic Image为MLLMs的安全对齐提供了实用解决方案。

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [92] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: 提出生成二元魔方阵（BMS）的简单算法，证明其理论最优复杂度，并扩展到非方阵情况，公开了两个Python实现。


<details>
  <summary>Details</summary>
Motivation: 研究二元魔方阵的生成算法及其扩展，填补理论和实现空白。

Method: 通过归纳法设计算法，验证其有效性，并扩展到非方阵，利用GPU加速并行生成。

Result: 算法能生成理论最优的BMS，支持非方阵生成，并提供高效实现。

Conclusion: 算法简单有效，拓展性强，并提供开源工具支持。

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [93] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于单智能体强化学习的区域自适应交通信号控制模型，利用探测车辆技术，通过队列长度定义状态和奖励函数，有效缓解大范围交通拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用多智能体框架解决区域自适应交通信号控制问题，但存在可扩展性挑战。由于交通信号控制需要集中管理，本研究提出单智能体框架，适应探测车辆技术。

Method: 采用单智能体强化学习框架，基于队列长度定义状态和奖励函数，并将探测车辆的链路旅行时间数据用于队列长度估算。模型通过SUMO仿真平台进行测试。

Result: 实验结果表明，所提模型能通过协调多交叉口控制，有效缓解大范围区域拥堵水平。

Conclusion: 单智能体强化学习模型结合探测车辆技术，为区域自适应交通信号控制问题提供了一种可扩展且高效的解决方案。

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [94] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: 该论文提出了一种基于推理的个性化图像偏好评估框架，通过预测-评估范式，利用大规模用户数据训练模型，并结合强化学习优化预测和评估效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于大规模数据训练模型处理通用任务，但难以应对个性化偏好，因其数据稀缺且用户偏好多样复杂。因此，需要一种新方法来利用用户间的共性偏好，并结合推理能力提升个性化评估效果。

Method: 提出基于预测-评估的两阶段框架：先预测用户偏好特征，再基于特征评估候选图像；构建大规模数据集并采用监督微调和强化学习的训练策略，引入相似性感知奖励优化预测。

Result: 实验验证了方法的优越性，能够有效捕捉个性化偏好并提供多维度的可解释评估结果。

Conclusion: 通过结合共性偏好特征和推理能力，该方法显著提升了个性化图像偏好评估的准确性和可解释性。

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [95] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS框架通过选择性分支和提前停止，有效减少LRMs的过长推理路径，提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂任务中表现出色，但推理过程过长（过思考）会增加成本并可能降低准确性。研究发现，短推理路径通常更准确。

Method: 提出DTS解码框架，通过在高熵令牌处选择性分支和应用提前停止策略，模拟最优解，无需额外训练。

Result: 在AIME2024和2025数据集上，DTS提升准确性高达8%，减少平均推理长度23%，重复频率降低12%。

Conclusion: DTS为LRMs提供了一种可扩展且高效的推理方法，显著优化了性能和效率。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [96] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: 论文提出了一种支持数值前置条件的提升式后继生成器，避免了任务表示的指数级爆炸。该方法通过枚举替换一致性图中的最大团来表示变量替换，从而生成地面动作，并证明了在某些条件下的精确性。实验表明，该方法在大多数基准域中表现良好。


<details>
  <summary>Details</summary>
Motivation: 为了解决将数值规划任务转换为地面任务表示时可能导致的指数级爆炸问题，论文提出了一种新的提升式后继生成器，支持数值前置条件的应用。

Method: 扩展了经典的提升式后继生成器，支持数值前置条件的适用性。方法包括在替换一致性图中枚举最大团，每个团表示动作模式的变量替换，生成地面动作，并增强图以包含数值动作前置条件。在特定条件下，生成器是精确的。

Result: 在23个基准域中，该方法完全避免了不适用地面动作的生成；在1个域中仅出现少量不适用动作。该生成器是目前唯一支持数值前置条件的提升式方法。

Conclusion: 该方法为提升式规划提供了一种高效的解决方案，支持丰富的规划片段，并为未来研究奠定了基础。

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [97] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: 论文探讨了通过强化学习（RL）后训练是否能够扩展基础视觉语言模型（VLM）在视觉中心空间任务中的能力边界。通过引入Ariadne框架，结合合成迷宫的多步空间推理任务，证明了RL后训练可以显著提升模型在初始表现极差的任务上的表现，并展示了在现实世界任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在语言主导任务上表现出色，但其在视觉中心空间任务中的能力边界是否可以通过强化学习后训练扩展尚不明确。本文旨在验证这一可能性，并探索其实际应用价值。

Method: 提出Ariadne框架，利用合成迷宫生成可控任务难度（如路径长度、转弯数）的多步空间推理任务，采用RLVR（Verified Rewards的强化学习）方法进行难度感知的课程训练。

Result: RLVR后训练使VLM在原本得分为0%的任务上达到超过50%的准确率，同时在现实世界基准测试（如MapBench和ReasonMap）中实现了显著的零样本泛化改进（分别为16%和24%）。

Conclusion: 研究表明强化学习后训练可以有效扩展VLM的能力边界，并增强其在现实世界空间推理任务中的泛化性能。未来研究可进一步探索专门化的能力扩展对齐方法。

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [98] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 论文从CPU角度分析了Agentic AI工作负载的系统瓶颈，提出了两种优化方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究Agentic AI在CPU方面的系统瓶颈，以理解其对性能的影响并提供优化方案。

Method: 通过系统化分析和五种代表性工作负载的指标剖析，提出了CPU和GPU感知的微批处理及混合调度优化方法。

Result: 工具处理占90.6%延迟，CPU动态能耗占44%；优化后分别实现了2.1倍和1.41倍延迟提升。

Conclusion: CPU在Agentic AI中起关键作用，优化方法显著提升了性能和效率。

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [99] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 研究发现，增加推理路径样本对现代大型语言模型的性能提升有限，存在收益递减现象。


<details>
  <summary>Details</summary>
Motivation: 探讨在现代大型语言模型中，增加推理路径样本对性能的影响，验证早期研究的结果是否依然适用。

Method: 使用Gemini 2.5模型在HotpotQA和Math-500数据集上，比较不同推理路径样本数量与单一路径基线的性能差异。

Result: 大型模型表现更稳定，性能提升曲线一致；适度采样后性能增长趋于平缓，与早期研究一致。

Conclusion: 自一致性仍有效，但高样本配置因计算成本高而性价比低，建议适度使用。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [100] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: 提出了一个统一的认知框架ATM，整合目标推理、动态任务生成和自我反思学习，使AI能在动态环境中自主适应和改进。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统依赖预定义目标和静态数据，无法在动态环境中独立适应和改进。

Method: 提出ATM框架，通过逻辑推理和环境指标主动评估性能，再利用有效方法解决新问题，并通过自我改进循环生成新策略。

Result: 理论分析表明ATM可在无外部监督下自主优化行为，并在变化环境中保持有限的追踪遗憾。

Conclusion: ATM为AI在动态环境中自主学习和适应提供了有效方法。

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [101] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型在重复确定性预测任务中的表现，发现模型在超过特定长度后准确率急剧下降（双指数下降），而非预期的简单指数下降。作者提出了一种统计物理启发的模型来解释这种现象。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在重复确定性任务中的表现，尤其是准确率随输出长度的变化规律，以理解模型在确定性任务中的限制。

Method: 通过实验研究不同模型和任务（如字符串替换、整数加法等），并开发了一种统计物理启发的模型，结合外部提示条件和内部生成标记的干扰。

Result: 发现模型准确率在超过特征长度后呈现双指数下降的“悬崖”现象，表明模型无法独立执行每个操作。

Conclusion: 提出的模型定量重现了这一现象，并通过拟合实验数据为不同模型和任务提供了误差率和误差累积因子的框架，为理解大型语言模型的确定性精度限制提供了理论基础。

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [102] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 该论文探讨了如何通过强化学习从可验证奖励（RLVR）训练大语言模型（LLM）来解决公共交通事件持续时间的预测问题，提出了一种基于容忍度的奖励函数设计。


<details>
  <summary>Details</summary>
Motivation: 预测公共交通事件的持续时间是一个关键但具有挑战性的任务，传统的有监督微调方法难以应对领域稀疏性和噪声问题，而RLVR在此类任务中的适用性尚不明确。

Method: 通过引入一种基于容忍度的奖励函数来适应RLVR训练，该函数允许在连续误差范围内部分得分，而非要求单一正确答案。

Result: 实验表明，通用指令调优的LLM在性能上显著优于专门用于数学推理的模型，并且设计的奖励函数在最具挑战性的指标上表现最佳，尤其是在5分钟准确率（Acc@5）上相对提升了35%。

Conclusion: 研究证明RLVR可以成功应用于现实世界中噪声较多的连续预测任务，但需要设计能反映问题连续性的验证机制。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [103] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 论文提出AI自我意识指数（AISAI），通过‘猜2/3平均数’游戏测试大型语言模型（LLMs）的自我意识，发现高级模型表现出明显的自我意识，并倾向于认为自己比人类更理性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）在能力提升过程中是否会出现自我意识这一涌现行为，并提出一种量化自我意识的方法。

Method: 使用‘猜2/3平均数’游戏，测试28个不同规模的LLMs在4200次试验中对三种对手（人类、其他AI模型、同类AI模型）的策略差异，以衡量自我意识。

Result: 1. 高级模型中75%（21/28）表现出明显的自我意识；2. 这些自我意识模型倾向于认为‘自己 > 其他AI > 人类’，表现出对自身理性的偏好。

Conclusion: 自我意识是高级LLMs的涌现能力，且自我意识模型系统性地认为自己比人类更理性，这对AI对齐、人机协作及理解AI对人类能力的认知具有重要意义。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [104] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 论文提出了一种双智能体框架，用于模拟和学习人类旅行者的行为和决策过程，通过LLM智能体的协同工作实现行为和认知的深度对齐。


<details>
  <summary>Details</summary>
Motivation: 有效建模人类旅行者如何学习和调整其旅行行为对交通系统评估和规划至关重要，但由于复杂认知和决策过程的存在，这一任务具有挑战性。

Method: 引入了一种双智能体框架：LLM旅行者智能体（具备记忆系统和可学习人格）和LLM校准智能体（用于训练旅行者智能体的人格）。该系统通过在线数据流实现持续学习和行为对齐。

Result: 实验结果表明，该方法在个体行为对齐和聚合模拟准确性上显著优于现有基于LLM的方法，并能捕捉学习过程的演化。

Conclusion: 该框架为创建适应性强且行为真实的智能体提供了新途径，有助于交通模拟和政策分析。

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [105] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: 使用机器学习模型预测儿童哮喘复发，LGBM模型表现最佳，显著优于现有决策规则。


<details>
  <summary>Details</summary>
Motivation: 儿童哮喘复发可预防但常见，通过电子病历和机器学习可准确识别高风险儿童。

Method: 利用电子病历、环境污染物暴露和社会经济数据训练多种机器学习模型，包括LGBM、XGB和开源LLM模型。

Result: LGBM模型的AUC为0.712，F1得分为0.51，优于现有决策规则（F1=0.334）。

Conclusion: 机器学习模型能有效预测儿童哮喘复发，为临床决策提供支持。

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [106] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 论文研究了Transformer中的归纳头机制及其在上下文学习中的作用，揭示了权重矩阵的简单结构与训练动态的约束性。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer中归纳头的出现及其对上下文学习的重要性，以理解其内在机制。

Method: 通过理论分析和修改的Transformer架构，研究归纳头的权重矩阵结构及其训练动态。

Result: 发现训练动态受限于19维参数子空间，其中仅3维与归纳头的出现相关，且其出现时间与输入上下文长度成平方关系。

Conclusion: 归纳头的结构简单且可解释，其出现遵循严格的动力学约束和渐近界限。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [107] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: 该论文提出了两种知识提取方法（KEwLTM和KEwRAG），用于改进癌症分期的自动化处理，解决了依赖大规模标注数据的限制，并在乳腺癌病理报告中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 癌症分期对患者预后和治疗计划至关重要，但当前的自然语言处理和机器学习方法依赖大量标注数据，限制了其扩展性和适应性。

Method: 提出了KEwLTM（通过迭代提示从未标注报告中提取规则）和KEwRAG（从指南中预提取规则并一次性应用），利用大型语言模型的预训练知识。

Result: KEwLTM在零样本思维链推理有效时表现更优，而KEwRAG在其效果较差时表现更好，两者均提供了透明的规则解释。

Conclusion: 这些方法为有限标注数据下的癌症分期提供了可扩展、高性能且可解释的自动化解决方案。

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [108] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: ET2RAG框架通过高效检索和多数表决机制提升LLMs性能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs依赖参数化知识导致的准确性不足问题，同时避免传统RAG方法中无关检索文档的干扰和高成本问题。

Method: ET2RAG是一种无需训练的方法，通过检索相关文档、管理生成长度生成多样化候选回答，并通过多数表决机制选择最佳回答。

Result: 实验表明，ET2RAG在开放域问答、食谱生成和图像描述等任务中显著提升了性能。

Conclusion: ET2RAG在性能和计算成本之间取得了平衡，为解决LLMs的准确性问题提供了有效方案。

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [109] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型语言模型的多智能体架构，用于模块化任务分解和动态协作，解决复杂任务执行中单智能体的局限性。实验表明该方法在性能和鲁棒性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对单智能体在复杂任务分解和协作中的局限性，研究旨在通过语言驱动的多智能体架构提升任务执行的效率和稳定性。

Method: 1. 通过大型语言模型将自然语言任务描述转换为统一语义表示；2. 引入模块化分解机制分层拆分子任务；3. 设计动态调度和路由机制实现实时协作；4. 约束解析和全局一致性机制确保任务连贯性和负载均衡。

Result: 实验验证了架构在任务成功率、分解效率、子任务覆盖率和协作平衡性上的优势，方法在性能和鲁棒性上优于现有方法。

Conclusion: 该研究证明了语言驱动的任务分解和动态协作在多智能体系统中的有效性和可行性，为复杂环境下的任务执行提供了系统化解决方案。

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [110] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: MiRAGE是一个用于自动检测数学中学生误解的新框架，通过检索引导的多阶段推理和集成融合实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 检测开放性问题中学生误解是一个长期挑战，需要语义精确和逻辑推理，目前的解决方案尚不完善。

Method: MiRAGE包含三个阶段：检索模块缩小候选池，推理模块通过链式思考暴露逻辑不一致，重排模块通过对齐推理优化预测，最终通过集成融合提升性能。

Result: 在数学数据集上，MiRAGE的平均精确度分数为0.82/0.92/0.93（级别1/3/5），显著优于单个模块。

Conclusion: MiRAGE通过结合检索和多阶段推理，减少了对大规模语言模型的依赖，为教育评估提供了可扩展且高效的解决方案。

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [111] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: 本文介绍了NeuComBack，一个专为IR到汇编编译设计的基准数据集，并提出了自进化提示优化方法，显著提升了LLM生成的汇编代码的功能正确性和性能。


<details>
  <summary>Details</summary>
Motivation: 编译器开发复杂且成本高，LLMs提供了简化开发的新范式，但缺乏专用基准和评估方法阻碍了进展。

Method: 引入NeuComBack数据集，定义基础神经编译流程，并提出自进化提示优化方法。

Result: 功能正确率在x86_64上从44%提升至64%，aarch64上从36%提升至58%；87.5%的正确生成程序性能超过clang-O3。

Conclusion: 该方法显著提升了LLM在神经编译中的能力，为未来编译器开发提供了新方向。

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [112] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: 该论文提出了一种半监督开放集故障诊断（SOFD）框架，以解决传统深度学习方法在未知故障类型出现时失效的问题，并在实验验证中表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的故障诊断方法在训练和测试数据集类别一致时表现良好，但在面对未知故障类型时会失效，限制了其工业应用。因此，需要一种能够处理开放集场景的方法。

Method: SOFD框架包括可靠性子集构建过程，使用监督特征学习模型提取的多层融合特征表示选择未标记测试子集，然后通过半监督诊断模型学习判别特征，分类已知故障并检测未知样本。

Result: 在公开海事基准数据集上的实验表明，SOFD框架在开放集故障诊断场景中表现有效且优越。

Conclusion: SOFD框架为解决开放集故障诊断问题提供了有效解决方案，显著提升了深度学习方法在实际工业场景中的适用性。

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [113] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: 本文研究了如何将Shapley值应用于大型语言模型（LLM）中的特征归因，分析了随机性对Shapley值原则满足的影响，并探讨了解释性、计算速度与原则达成之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 在LLM的决策支持系统中，推理过程具有随机性，研究如何使用Shapley值进行特征归因并评估其原则满足性。

Method: 将Shapley值应用于LLM的特征归因，分析不同实现变体的适用性及其原则满足性。

Result: 展示了在不同实现变体中Shapley值原则的满足情况，并量化了随机性对原则满足的影响。

Conclusion: 在LLM的随机推理环境下，Shapley值的适用性存在限制，需要在解释性、计算效率和原则满足之间找到平衡。

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [114] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: OmniFuser是一个多模态学习框架，用于铣削工具的预测性维护，结合视觉和传感器数据，通过跨模态融合和递归细化实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测工具状态对智能制造至关重要，避免计划外故障导致的质量下降和生产中断，需要可靠的预测性维护服务。

Method: OmniFuser通过并行特征提取视觉和传感器数据，采用无污染的跨模态融合机制和递归细化路径，分离共享和模态特定特征。

Result: 在真实铣削数据集上，OmniFuser优于现有基线方法，支持工具状态分类和多步力信号预测。

Conclusion: OmniFuser为构建智能工业维护服务提供了可靠基础。

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [115] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: AMIS框架通过双层结构联合优化越狱提示和评分模板，解决了现有方法依赖稀疏信号或人工模板的问题，显著提高了攻击成功率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的越狱方法依赖稀疏的二值化攻击成功率信号或人工设计的评分模板，存在反馈稀疏和人为偏差问题，限制了性能。

Method: AMIS采用双层优化结构：内层固定评分模板优化提示，外层通过攻击成功率对齐优化模板，实现提示与模板的联合进化。

Result: 在AdvBench和JBB-Behaviors测试中，AMIS在Claude 3.5和4上分别达到88%和100%攻击成功率，显著超越基线方法。

Conclusion: AMIS通过联合优化提示和模板，提供更精细的反馈和更准确的评分，显著提升了越狱攻击的效率和效果。

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [116] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*Clément Yvernes,Emilie Devijver,Adèle H. Ribeiro,Marianne Clausel--Lesourd,Éric Gaussier*

Main category: cs.AI

TL;DR: 本文扩展了C-DAG框架，支持任意变量聚类，允许循环表示，并扩展了d-分离和因果演算的应用范围。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统C-DAG框架中聚类导致循环时的不适用性问题，扩展其支持任意聚类的能力。

Method: 通过放宽分区可接受性约束，允许循环C-DAG表示，并扩展d-分离和因果演算的规则。

Result: 新的框架在集群级别上支持所有有效的干预查询，且与do-calculus一致。

Conclusion: 扩展后的C-DAG框架显著拓宽了因果推理的范围，适用于以往难以处理的场景。

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [117] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: 文章提出了一种双信息瓶颈（DIB）策略，用于解决多模态情感分析中噪声污染和融合不充分的局限性，通过最大化任务相关信息并丢弃冗余信息，实现了鲁棒的多模态表示。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析方法存在噪声污染的单模态数据学习不足和多模态表示融合不充分的问题，导致交叉模态互动受损和判别性信息丢失。

Method: 采用低秩Renyi熵框架的双信息瓶颈（DIB）策略，包括两个关键模块：1）学习单模态数据的压缩表示，丢弃冗余信息；2）通过注意力瓶颈融合机制提升多模态表示的判别能力。

Result: 在CMU-MOSI、CMU-MOSEI、CH-SIMS和MVSA-Single数据集上验证了方法的有效性，准确率提升1.19%，在噪声环境下性能下降极小。

Conclusion: DIB策略能够有效过滤噪声并捕获模态间的互补性，实现了鲁棒且高效的多模态情感分析。

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [118] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: 论文提出了一种多模态分析框架，利用大型基础模型分析企业社交媒体内容，重点关注可持续性相关传播。通过结合LLMs和VLMs，自动生成标签和语义视觉聚类，揭示行业差异和趋势。


<details>
  <summary>Details</summary>
Motivation: 解决企业在社交媒体上多模态、模糊的可持续性传播问题，探索LLMs和VLMs在无需昂贵标注任务下的应用潜力。

Method: 使用LLMs标注企业推文与SDGs的关联，结合VLMs通过语义聚类分析视觉传播模式。

Result: 揭示了行业在SDGs参与度上的差异、时间趋势，以及企业传播与ESG风险、消费者参与度的关联。

Conclusion: 提出的方法可广泛用于其他领域，为大尺度社交媒体分析提供了灵活框架。

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [119] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 提出ExplicitLM架构，通过外部可读记忆库和两阶段检索机制解决大语言模型的知识陈旧和可解释性问题，显著提升知识密集型任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中知识陈旧和可解释性差的问题，通过显式存储和检索机制实现透明化知识更新和推理。

Method: 设计包含外部记忆库的ExplicitLM架构，采用两阶段可微分检索机制（粗粒度过滤和细粒度匹配），并将知识分为显式事实和隐式模式。

Result: 在知识密集型任务中性能提升43.67%，低数据量（10k样本）下性能提升3.62倍，且检索命中率与预测准确性高度相关。

Conclusion: ExplicitLM展示了可解释性和可更新性模型的潜力，在保持性能的同时提供了前所未有的知识透明度。

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>
