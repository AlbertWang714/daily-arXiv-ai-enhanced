<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.AI](#cs.AI) [Total: 27]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [StreetMath: Study of LLMs' Approximation Behaviors](https://arxiv.org/abs/2510.25776)
*Chiung-Yi Tseng,Somshubhra Roy,Maisha Thasin,Danyang Zhang,Blessing Effiong*

Main category: cs.CL

TL;DR: 论文介绍了StreetMath基准测试，用于评估大型语言模型在实际近似场景中的能力，并通过实验揭示了模型在近似任务中的表现与认知吝啬性差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是填补大型语言模型在非自回归解码模型中进行快速近似数学推理能力研究的空白。

Method: 研究方法包括引入StreetMath基准测试，对不同架构的模型进行广泛评估，并应用机制解释技术分析其内部计算状态。

Result: 结果显示，模型通常在近似任务中仍尝试计算精确值或调用外部工具，且近似和精确运算依赖于不同的神经组件。

Conclusion: 结论是大型语言模型在街头数学情境中未表现出与人类相同的认知吝啬性。

Abstract: There is a substantial body of literature examining the mathematical
reasoning capabilities of large language models (LLMs), particularly their
performance on precise arithmetic operations in autoregressive architectures.
However, their ability to perform approximate reasoning in informal, fast-paced
mathematical operations has received far less attention, especially among
non-autoregressive decoder models. Our work addresses this gap by introducing
StreetMath, a benchmark designed to evaluate models' approximation abilities
under real-world approximation scenarios. We conduct extensive evaluations
across different LLM architectures: Qwen3-4B-Instruct-2507,
Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and
Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to
probe their internal computational states. Our analysis reveals that LLMs
generally attempt to compute exact values or invoke external tools even in
tasks that call for approximation. Moreover, while models sometimes reach the
correct answer in early layers or steps, they still consume more tokens when
solving approximation tasks. Additional experiments indicate that exact and
approximate arithmetic operations rely on largely separate neural components.
Drawing upon research on cognitive psychology, we argue that LLMs do not
exhibit cognitive miserliness in the same way humans do in street math
settings. We open source our work https://github.com/ctseng777/StreetMath

</details>


### [2] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 本文提出了一种基于模糊逻辑和句法依赖解析的方法，用于对意见词的强度和方向进行细粒度分类，从而更准确地评估实体评论的情感倾向。


<details>
  <summary>Details</summary>
Motivation: 现有的基于词典的意见挖掘方法忽略了意见强度的差异性，无法区分极强、强、中等、极弱和弱的意见表达。

Method: 结合模糊逻辑算法将意见词分类为不同强度等级，并通过句法依赖解析关联到目标方面的相关词，计算实体在特定方面的评分。

Result: 该方法能够更细致地评估评论的情感强度和方向，为实体排名提供更准确的依据。

Conclusion: 提出的方法在意见挖掘中实现了对意见强度和方向的更细致分析，提升了情感分析的准确性。

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [3] [LASTIST: LArge-Scale Target-Independent STance dataset](https://arxiv.org/abs/2510.25783)
*DongJae Kim,Yaejin Lee,Minsu Park,Eunil Park*

Main category: cs.CL

TL;DR: 该研究提出了一个针对韩语的目标无关立场检测（LASTIST）数据集，填补了低资源语言在该领域的空白，包含56.3万条标注句子，并基于此训练了先进的深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 当前立场检测研究主要集中在目标相关的任务上，且大多基于英语数据集，韩语等低资源语言缺乏相关资源，因此需要开发一个大规模、目标无关的数据集。

Method: 通过收集韩国政党的新闻稿，构建了包含563,299条标注句子的LASTIST数据集，并训练了先进的深度学习和立场检测模型。

Result: LASTIST数据集支持多种立场检测任务，包括目标无关立场检测和时间演变立场检测，数据集已公开。

Conclusion: LASTIST数据集为韩语立场检测研究提供了宝贵资源，推动了低资源语言在该领域的发展。

Abstract: Stance detection has emerged as an area of research in the field of
artificial intelligence. However, most research is currently centered on the
target-dependent stance detection task, which is based on a person's stance in
favor of or against a specific target. Furthermore, most benchmark datasets are
based on English, making it difficult to develop models in low-resource
languages such as Korean, especially for an emerging field such as stance
detection. This study proposes the LArge-Scale Target-Independent STance
(LASTIST) dataset to fill this research gap. Collected from the press releases
of both parties on Korean political parties, the LASTIST dataset uses 563,299
labeled Korean sentences. We provide a detailed description of how we collected
and constructed the dataset and trained state-of-the-art deep learning and
stance detection models. Our LASTIST dataset is designed for various tasks in
stance detection, including target-independent stance detection and diachronic
evolution stance detection. We deploy our dataset on
https://anonymous.4open.science/r/LASTIST-3721/.

</details>


### [4] [zFLoRA: Zero-Latency Fused Low-Rank Adapters](https://arxiv.org/abs/2510.25784)
*Dhananjaya Gowda,Seoha Song,Harshith Goka,Junhyun Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为zFLoRA的零延迟低秩适配器，解决了现有适配器在推理时带来的显著计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在部署任务特定适配器时，尽管适配器参数量少（通常不到基础模型的1%），但其推理时的额外计算开销却不成比例地高（可达基础模型的2.5倍）。

Method: 提出了一种新型零延迟融合低秩适配器（zFLoRA），在基础模型上引入零或可忽略的延迟开销。

Result: 在1B、3B和7B大小的LLMs上，zFLoRA表现优于流行的监督微调基准方法，包括低秩适配器（LoRA）和全微调（FFT）。实验覆盖18个任务，包括常识推理、数学推理和摘要对话。在NPU（三星Galaxy S25+）和GPU（NVIDIA H100）平台上，zFLoRA均表现出零至可忽略的延迟开销。

Conclusion: zFLoRA是一种高效的适配器方案，显著减少了推理时的计算开销，同时保持了任务性能。

Abstract: Large language models (LLMs) are increasingly deployed with task-specific
adapters catering to multiple downstream applications. In such a scenario, the
additional compute associated with these apparently insignificant number of
adapter parameters (typically less than 1% of the base model) turns out to be
disproportionately significant during inference time (upto 2.5x times that of
the base model). In this paper, we propose a new zero-latency fused low-rank
adapter (zFLoRA) that introduces zero or negligible latency overhead on top of
the base model. Experimental results on LLMs of size 1B, 3B and 7B show that
zFLoRA compares favorably against the popular supervised fine-tuning benchmarks
including low-rank adapters (LoRA) as well as full fine-tuning (FFT).
Experiments are conducted on 18 different tasks across three different
categories namely commonsense reasoning, math reasoning and summary-dialogue.
Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA
H100) platforms show that the proposed zFLoRA adapters introduce zero to
negligible latency overhead.

</details>


### [5] [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
*Yaniv Nikankin,Dana Arad,Itay Itzhak,Anja Reusch,Adi Simhi,Gal Kesten-Pomeranz,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 提高机制可解释性中的电路发现方法，通过引导法、比率选择和整数线性规划优化，提升了性能和忠实度。


<details>
  <summary>Details</summary>
Motivation: 解决机制可解释性中电路发现的核心挑战，即确定模型执行任务的特定部分。

Method: 使用引导法识别一致归因得分的边；引入比率选择策略优先选择强正得分边；采用整数线性规划替代标准贪婪选择。

Result: 在多任务和模型中生成的电路更忠实，且表现优于先前方法。

Conclusion: 提出的三种改进显著提升了电路发现的性能和忠实度。

Abstract: One of the main challenges in mechanistic interpretability is circuit
discovery, determining which parts of a model perform a given task. We build on
the Mechanistic Interpretability Benchmark (MIB) and propose three key
improvements to circuit discovery. First, we use bootstrapping to identify
edges with consistent attribution scores. Second, we introduce a simple
ratio-based selection strategy to prioritize strong positive-scoring edges,
balancing performance and faithfulness. Third, we replace the standard greedy
selection with an integer linear programming formulation. Our methods yield
more faithful circuits and outperform prior approaches across multiple MIB
tasks and models. Our code is available at:
https://github.com/technion-cs-nlp/MIB-Shared-Task.

</details>


### [6] [LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection](https://arxiv.org/abs/2510.25799)
*Adam S. Jovine,Tinghan Ye,Francis Bahk,Jingjing Wang,David B. Shmoys,Peter I. Frazier*

Main category: cs.CL

TL;DR: 论文提出了LISTEN框架，利用大型语言模型作为零样本偏好预测器，通过迭代算法（LISTEN-U和LISTEN-T）解决多目标决策中的偏好形式化问题，实验表明两种方法在不同任务中各有优势。


<details>
  <summary>Details</summary>
Motivation: 人类专家在多目标决策中难以形式化复杂偏好，导致决策效率低下。为解决这一问题，作者引入LISTEN框架，利用自然语言指导LLM预测偏好。

Method: 提出LISTEN框架，包含两种迭代算法：LISTEN-U（基于参数化效用函数）和LISTEN-T（基于非参数锦标赛选择）。通过自然语言输入指导LLM预测偏好。

Result: 实验表明，LISTEN-U在偏好参数化对齐任务中表现优异，而LISTEN-T在鲁棒性上更胜一筹。两种方法有效减轻了传统偏好获取的认知负担。

Conclusion: LISTEN框架为多目标决策提供了一种自然语言驱动的解决方案，展示了LLM在偏好预测中的潜力。

Abstract: Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.

</details>


### [7] [Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data](https://arxiv.org/abs/2510.25804)
*Haoran Deng,Yingyu Lin,Zhenghao Lin,Xiao Liu,Yizhou Sun,Yi-An Ma,Yeyun Gong*

Main category: cs.CL

TL;DR: LongFilter是一种用于筛选长上下文预训练数据的框架，通过衡量长上下文与短上下文的信息增益，高效选择高质量数据，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有长文本数据大多缺乏有意义的长距离依赖，训练效率低，需精心筛选数据以提高模型性能。

Method: 提出LongFilter框架，通过对比模型在长上下文与短上下文下的预测差异，识别依赖长距离上下文的样本。

Result: 在LLaMA-3-8B模型上，将上下文长度从8K扩展到64K，LongFilter显著提升了HELMET、LongBench和RULER等基准测试的性能。

Conclusion: LongFilter能高效筛选高质量长上下文训练数据，优化长上下文语言模型的性能。

Abstract: Long-context language models unlock advanced capabilities in reasoning, code
generation, and document summarization by leveraging dependencies across
extended spans of text. However, a significant portion of readily available
long-text data lacks meaningful long-distance dependencies; most spans can be
predicted using only local context. Training on such data is inefficient,
making careful data selection crucial. Therefore, we introduce LongFilter, a
framework for curating training data tailored to long-context pretraining.
LongFilter measures the information gain provided by extended context by
contrasting model predictions under long-context versus short-context settings,
thereby identifying samples where long-range dependencies are essential.
Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show
that LongFilter efficiently selects high-quality data and yields substantial
improvements on benchmarks such as HELMET, LongBench, and RULER.

</details>


### [8] [Ideology-Based LLMs for Content Moderation](https://arxiv.org/abs/2510.25805)
*Stefano Civelli,Pietro Bernardelle,Nardiena A. Pratama,Gianluca Demartini*

Main category: cs.CL

TL;DR: 研究探讨了角色设定对不同大语言模型在有害内容分类中的一致性和公平性的影响，揭示了角色设定可能引入意识形态偏见的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在内容审核系统中的广泛应用，确保其公平性和中立性变得至关重要。研究旨在探索角色设定如何影响模型对有害内容的判断。

Method: 通过分析不同模型架构、规模及内容模态（语言与视觉）下角色设定对分类行为的影响，并进一步研究在政治任务中模型的意识形态一致性。

Result: 表面指标显示角色设定对分类准确性影响微小，但深入分析表明不同意识形态的角色在标记有害内容时存在显著差异，且大模型更易与同意识形态角色一致。政治任务中，角色更倾向于捍卫自身观点并弱化对立观点的危害性。

Conclusion: 角色设定可能在看似中立的大语言模型输出中引入意识形态偏见，引发对AI系统强化党派倾向的担忧。

Abstract: Large language models (LLMs) are increasingly used in content moderation
systems, where ensuring fairness and neutrality is essential. In this study, we
examine how persona adoption influences the consistency and fairness of harmful
content classification across different LLM architectures, model sizes, and
content modalities (language vs. vision). At first glance, headline performance
metrics suggest that personas have little impact on overall classification
accuracy. However, a closer analysis reveals important behavioral shifts.
Personas with different ideological leanings display distinct propensities to
label content as harmful, showing that the lens through which a model "views"
input can subtly shape its judgments. Further agreement analyses highlight that
models, particularly larger ones, tend to align more closely with personas from
the same political ideology, strengthening within-ideology consistency while
widening divergence across ideological groups. To show this effect more
directly, we conducted an additional study on a politically targeted task,
which confirmed that personas not only behave more coherently within their own
ideology but also exhibit a tendency to defend their perspective while
downplaying harmfulness in opposing views. Together, these findings highlight
how persona conditioning can introduce subtle ideological biases into LLM
outputs, raising concerns about the use of AI systems that may reinforce
partisan perspectives under the guise of neutrality.

</details>


### [9] [Beyond Long Context: When Semantics Matter More than Tokens](https://arxiv.org/abs/2510.25816)
*Tarun Kumar Chawdhury,Jon D. Duke*

Main category: cs.CL

TL;DR: 论文提出了CLEAR方法，通过实体感知检索在临床问答任务中提升效率和准确性，效果优于传统方法，尤其是在处理长文档时。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）中的临床文档通常以base64编码附件存储，导致语义问答困难，传统向量数据库方法难以捕捉临床关系的细微之处。

Method: CLEAR方法利用实体感知检索，显著减少了token使用量，同时在实验中与零样本大上下文推理和传统分块检索增强生成方法进行比较。

Result: CLEAR在12个临床笔记上取得了58.3%的胜率，平均语义相似度为0.878，token使用量减少了78%。长文档（超过65,000 token）的胜率高达75%。

Conclusion: 实体感知检索在临床自然语言处理中显著提升了效率和准确性，且评估框架为语义精度和计算效率要求高的临床问答系统提供了可复用的透明基准。

Abstract: Electronic Health Records (EHR) store clinical documentation as base64
encoded attachments in FHIR DocumentReference resources, which makes semantic
question answering difficult. Traditional vector database methods often miss
nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)
method, introduced by Lopez et al. 2025, uses entity aware retrieval and
achieved improved performance with an F1 score of 0.90 versus 0.86 for
embedding based retrieval, while using over 70 percent fewer tokens. We
developed a Clinical Notes QA Evaluation Platform to validate CLEAR against
zero shot large context inference and traditional chunk based retrieval
augmented generation. The platform was tested on 12 clinical notes ranging from
10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a
58.3 percent win rate, an average semantic similarity of 0.878, and used 78
percent fewer tokens than wide context processing. The largest performance
gains occurred on long notes, with a 75 percent win rate for documents
exceeding 65,000 tokens. These findings confirm that entity aware retrieval
improves both efficiency and accuracy in clinical natural language processing.
The evaluation framework provides a reusable and transparent benchmark for
assessing clinical question answering systems where semantic precision and
computational efficiency are critical.

</details>


### [10] [A Survey on Efficient Large Language Model Training: From Data-centric Perspectives](https://arxiv.org/abs/2510.25817)
*Junyu Luo,Bohan Wu,Xiao Luo,Zhiping Xiao,Yiqiao Jin,Rong-Cheng Tu,Nan Yin,Yifan Wang,Jingyang Yuan,Wei Ju,Ming Zhang*

Main category: cs.CL

TL;DR: 这篇论文是第一篇从数据为中心的角度系统调查数据高效的大型语言模型（LLM）后训练的综述，提出了一种分类方法，并总结了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练范式面临数据挑战，包括人工标注的高成本和数据规模边际效益递减，因此实现数据高效的后训练成为关键研究问题。

Method: 论文提出了一种数据高效LLM后训练的分类法，涵盖数据选择、数据质量提升、合成数据生成、数据蒸馏与压缩，以及自演化数据生态系统。

Result: 总结了每类中的代表性方法，并指出了未来的研究方向，旨在激发进一步探索大规模模型训练中数据利用潜力。

Conclusion: 通过分析数据高效LLM后训练的挑战，强调了开放性问题并提出了潜在的研究途径，为未来研究提供了方向。

Abstract: Post-training of Large Language Models (LLMs) is crucial for unlocking their
task generalization potential and domain-specific capabilities. However, the
current LLM post-training paradigm faces significant data challenges, including
the high costs of manual annotation and diminishing marginal returns on data
scales. Therefore, achieving data-efficient post-training has become a key
research question. In this paper, we present the first systematic survey of
data-efficient LLM post-training from a data-centric perspective. We propose a
taxonomy of data-efficient LLM post-training methods, covering data selection,
data quality enhancement, synthetic data generation, data distillation and
compression, and self-evolving data ecosystems. We summarize representative
approaches in each category and outline future research directions. By
examining the challenges in data-efficient LLM post-training, we highlight open
problems and propose potential research avenues. We hope our work inspires
further exploration into maximizing the potential of data utilization in
large-scale model training. Paper List:
https://github.com/luo-junyu/Awesome-Data-Efficient-LLM

</details>


### [11] [Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation](https://arxiv.org/abs/2510.25904)
*Frederico Belcavello,Ely Matos,Arthur Lorenzi,Lisandra Bonoto,Lívia Ruiz,Luiz Fernando Pereira,Victor Herbst,Yulla Navarro,Helen de Andrade Abreu,Lívia Dutra,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 论文探讨了基于LLM的应用在语言资源创建中的表现，尤其是半自动标注在语义标注中的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM工具在语言研究中潜力巨大，但目前缺乏对其性能和在标注数据集中影响的全面评估，尤其是在视角化的NLP方法下。

Method: 通过比较手动、自动和半自动标注设置下的标注时间、覆盖范围和多样性，评估LLM语义角色标注器的表现。

Result: 半自动标注在框架多样性上表现优于纯人工标注，而在标注覆盖率上与之相当；纯自动标注在所有指标上表现较差，但标注时间最短。

Conclusion: 研究表明，半自动标注结合了人工与自动标注的优势，是语义标注任务中更高效的选择。

Abstract: The use of LLM-based applications as a means to accelerate and/or substitute
human labor in the creation of language resources and dataset is a reality.
Nonetheless, despite the potential of such tools for linguistic research,
comprehensive evaluation of their performance and impact on the creation of
annotated datasets, especially under a perspectivized approach to NLP, is still
missing. This paper contributes to reduction of this gap by reporting on an
extensive evaluation of the (semi-)automatization of FrameNet-like semantic
annotation by the use of an LLM-based semantic role labeler. The methodology
employed compares annotation time, coverage and diversity in three experimental
settings: manual, automatic and semi-automatic annotation. Results show that
the hybrid, semi-automatic annotation setting leads to increased frame
diversity and similar annotation coverage, when compared to the human-only
setting, while the automatic setting performs considerably worse in all
metrics, except for annotation time.

</details>


### [12] [Revisiting Multilingual Data Mixtures in Language Model Pretraining](https://arxiv.org/abs/2510.25947)
*Negar Foroutan,Paul Teiletche,Ayush Kumar Tarun,Antoine Bosselut*

Main category: cs.CL

TL;DR: 研究发现，在多语言预训练中，适当平衡的混合数据不会损害模型性能，英语作为枢纽语言能跨语言家族带来好处，且多语言诅咒现象不明显。


<details>
  <summary>Details</summary>
Motivation: 探讨多语言数据混合对大型语言模型(LLM)预训练的影响，挑战关于语言覆盖与性能之间的潜在权衡的常见假设。

Method: 训练1.1B和3B参数的LLM，使用25到400种语言的多样化多语言语料库，对比不同语言组合的效果。

Result: 合理的多语言数据混合不会降低性能，英语作为枢纽语言具有普遍优势，多语言诅咒在模型规模扩大时不明显。

Conclusion: 多语言数据在适当平衡下可以提升模型能力，而不会牺牲性能，这对低资源语言的模型开发具有重要意义。

Abstract: The impact of different multilingual data mixtures in pretraining large
language models (LLMs) has been a topic of ongoing debate, often raising
concerns about potential trade-offs between language coverage and model
performance (i.e., the curse of multilinguality). In this work, we investigate
these assumptions by training 1.1B and 3B parameter LLMs on diverse
multilingual corpora, varying the number of languages from 25 to 400. Our study
challenges common beliefs surrounding multilingual training. First, we find
that combining English and multilingual data does not necessarily degrade the
in-language performance of either group, provided that languages have a
sufficient number of tokens included in the pretraining corpus. Second, we
observe that using English as a pivot language (i.e., a high-resource language
that serves as a catalyst for multilingual generalization) yields benefits
across language families, and contrary to expectations, selecting a pivot
language from within a specific family does not consistently improve
performance for languages within that family. Lastly, we do not observe a
significant "curse of multilinguality" as the number of training languages
increases in models at this scale. Our findings suggest that multilingual data,
when balanced appropriately, can enhance language model capabilities without
compromising performance, even in low-resource settings

</details>


### [13] [Semantic Label Drift in Cross-Cultural Translation](https://arxiv.org/abs/2510.25967)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 研究探讨机器翻译中文化差异对语义标签保留的影响，发现现代大语言模型在翻译中引发标签漂移，文化因素是关键。


<details>
  <summary>Details</summary>
Motivation: 机器翻译广泛用于资源稀缺语言，但文化差异对语义标签保留的影响未被充分研究，可能导致下游应用的误解和文化冲突。

Method: 通过在文化敏感和中性领域的实验，分析现代大语言模型与传统统计机器翻译工具在标签漂移上的表现。

Result: 发现现代大语言模型在翻译中引发标签漂移，文化相似性是标签保留的关键因素。

Conclusion: 忽视文化因素会损害标签保真度并增加下游应用风险，需在机器翻译中纳入文化考量。

Abstract: Machine Translation (MT) is widely employed to address resource scarcity in
low-resource languages by generating synthetic data from high-resource
counterparts. While sentiment preservation in translation has long been
studied, a critical but underexplored factor is the role of cultural alignment
between source and target languages. In this paper, we hypothesize that
semantic labels are drifted or altered during MT due to cultural divergence.
Through a series of experiments across culturally sensitive and neutral
domains, we establish three key findings: (1) MT systems, including modern
Large Language Models (LLMs), induce label drift during translation,
particularly in culturally sensitive domains; (2) unlike earlier statistical MT
tools, LLMs encode cultural knowledge, and leveraging this knowledge can
amplify label drift; and (3) cultural similarity or dissimilarity between
source and target languages is a crucial determinant of label preservation. Our
findings highlight that neglecting cultural factors in MT not only undermines
label fidelity but also risks misinterpretation and cultural conflict in
downstream applications.

</details>


### [14] [SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation](https://arxiv.org/abs/2510.25975)
*Sina Bagheri Nezhad,Yao Li,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 论文提出了一种名为SymCode的神经符号框架，通过将数学问题解决转化为可验证的代码生成任务，显著提升了大型语言模型在复杂数学推理中的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂数学推理中表现不佳，生成的内容缺乏验证且算术不准确，现有方法虽能提供思路但缺乏确定性验证机制。

Method: 引入SymCode框架，利用SymPy库将数学问题解决转化为可验证的代码生成任务，替代传统基于文本的生成方法。

Result: 在MATH-500和OlympiadBench等挑战性基准测试中，SymCode显著提升了准确率（提高13.6个百分点），同时提高了模型的token效率和故障透明度。

Conclusion: 通过将LLM推理建立在确定性符号引擎上，SymCode为形式化领域中更准确、可信的AI迈出了关键一步。

Abstract: Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.

</details>


### [15] [NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium](https://arxiv.org/abs/2510.25977)
*Dinghong Song,Jierui Xu,Weichu Yang,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 本文设计了一种针对Trainium AI加速器的高性能矩阵乘法（matmul）方法，通过定制化技术显著提升了LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: Trainium AI加速器的异构架构虽为LLM训练和推理提供了高效解决方案，但其特殊的脉动阵列架构和数据布局要求使其难以充分发挥性能，因此需要优化的matmul方法。

Method: 通过内核融合和新型缓存策略，减少数据在软件管理内存层级间的移动，最大化SRAM带宽并避免昂贵的矩阵转置。

Result: 在九个数据集和四个最新LLM上的评估显示，matmul内核平均加速1.35倍（最高2.22倍），端到端LLM推理平均加速1.66倍（最高2.49倍）。

Conclusion: 提出的方法显著提升了Trainium上的矩阵乘法和LLM推理性能，优于AWS现有实现。

Abstract: AI accelerators, customized to AI workloads, provide cost-effective and
high-performance solutions for training and inference. Trainium, an AI
accelerator recently developed by Amazon Web Services (AWS), provides an
attractive option for LLM training and inference through its heterogeneous
architecture. However, leveraging Trainium architecture for high performance
can be challenging because of its systolic array architecture and special
requirement on data layout. In this paper, we design high-performance matrix
multiplication (matmul), a critical compute kernel, for LLM inference on
Trainium. We introduce a series of techniques customized to Trainium based on
kernel fusion and novel caching strategies to reduce data movement across the
software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive
matrix transpose. Evaluating with nine datasets and four recent LLMs, we show
that our system largely outperforms the state-of-the-art matmul implemented by
AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x
speedup (up to 2.22x), which translates to an average 1.66x speedup (up to
2.49x) for end-to-end LLM inference.

</details>


### [16] [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)
*Dinghong Song,Yuan Feng,Yiwei Wang,Shangye Chen,Cyril Guyot,Filip Blagojevic,Hyeran Jeon,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 论文提出AttnCache框架，通过重用相似的注意力图来加速预填充阶段的LLM推理，减少自注意力的计算开销。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，许多任务仅依赖预填充阶段，而自注意力计算因其二次复杂度成为性能瓶颈。研究发现不同句子的注意力图类似，因此提出重用相似注意力图的想法。

Method: 构建注意力图记忆数据库，利用高效缓存和相似性搜索技术，在推理时重用预缓存的注意力图，减少自注意力计算。

Result: 实验显示，AttnCache在CPU上实现1.2倍端到端和2倍注意力加速，GPU上实现1.6倍端到端和3倍注意力加速，且准确率损失可忽略。

Conclusion: AttnCache有效加速预填充阶段的LLM推理，显著减少计算开销，适用于多种实际应用场景。

Abstract: Large Language Models (LLMs) are widely used in generative applications such
as chatting, code generation, and reasoning. However, many realworld workloads
such as classification, question answering, recommendation, and text embedding
rely solely on the prefill stage of inference, where the model encodes input
sequences without performing autoregressive decoding. In these prefill only
scenarios, the self-attention computation becomes the primary performance
bottleneck due to its quadratic complexity with respect to sequence length. In
this paper, we observe that semantically different sentences often produce
similar attention maps across layers and heads. Building on this insight, we
propose AttnCache, a framework that accelerates the prefill stage of LLM
inference by retrieving and reusing similar attention maps. Based on an
attention map memorization database, AttnCache employs efficient caching and
similarity search techniques to identify and reuse pre-cached attention maps
during inference, thereby reducing the computational overhead of
self-attention. Experimental results show that AttnCache achieves an average of
1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x
attention speedup on GPU, with negligible accuracy degradation.

</details>


### [17] [PORTool: Tool-Use LLM Training with Rewarded Tree](https://arxiv.org/abs/2510.26020)
*Feijie Wu,Weiwu Zhu,Yuxiang Zhang,Soumya Chatterjee,Jiarong Zhu,Fan Mo,Rodin Luo,Jing Gao*

Main category: cs.CL

TL;DR: 论文提出了PORTool，一种强化学习方法，旨在解决当前工具使用大型语言模型（LLM）在动态环境中表现受限的问题。通过生成多样化的轨迹并为其分配奖励，PORTool显著提高了模型的最终准确性和工具调用步骤的效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于静态数据集训练的LLM在动态工具调用环境中表现受限，无法充分探索多种可能的解决方案。本文旨在通过强化学习方法提升模型在这些环境中的表现。

Method: PORTool采用强化学习方法，通过生成多个轨迹并形成树状结构，为每个步骤分配奖励，计算分叉相对优势和轨迹相对优势，以训练LLM。

Result: 实验表明，PORTool显著提高了模型的最终准确性和工具调用步骤的效率，并通过消融研究验证了步骤奖励的必要性和设计鲁棒性。

Conclusion: PORTool通过强化学习方法有效提升了LLM在动态工具调用环境中的性能，为解决类似问题提供了新的思路。

Abstract: Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.

</details>


### [18] [Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs](https://arxiv.org/abs/2510.26024)
*HyoJung Han,Sweta Agrawal,Eleftheria Briakou*

Main category: cs.CL

TL;DR: 论文研究了跨语言对齐（CLA）中文化消失的问题，提出了一种新的评估框架和解决方案。


<details>
  <summary>Details</summary>
Motivation: 跨语言对齐可能导致文化消失，即丧失对不同语言查询的文化情境响应能力，论文旨在分析和解决这一问题。

Method: 引入了一个评估框架'transfer-localization plane'，并提出了'Surgical Steering'方法，通过不同层的激活控制来优化目标。

Result: 研究发现CLA方法在改进事实传递的同时牺牲了文化定位，新方法在两者之间取得了更好的平衡。

Conclusion: 通过分层激活控制可以更好地平衡跨语言对齐中的事实传递和文化定位问题。

Abstract: Cross-lingual alignment (CLA) aims to align multilingual representations,
enabling Large Language Models (LLMs) to seamlessly transfer knowledge across
languages. While intuitive, we hypothesize, this pursuit of representational
convergence can inadvertently cause "cultural erasure", the functional loss of
providing culturally-situated responses that should diverge based on the query
language. In this work, we systematically analyze this trade-off by introducing
a holistic evaluation framework, the transfer-localization plane, which
quantifies both desirable knowledge transfer and undesirable cultural erasure.
Using this framework, we re-evaluate recent CLA approaches and find that they
consistently improve factual transfer at the direct cost of cultural
localization across all six languages studied. Our investigation into the
internal representations of these models reveals a key insight: universal
factual transfer and culturally-specific knowledge are optimally steerable at
different model layers. Based on this finding, we propose Surgical Steering, a
novel inference-time method that disentangles these two objectives. By applying
targeted activation steering to distinct layers, our approach achieves a better
balance between the two competing dimensions, effectively overcoming the
limitations of current alignment techniques.

</details>


### [19] [Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings](https://arxiv.org/abs/2510.26032)
*Felipe Larios,Mariana Borras-Osorio,Yuqi Wu,Ana Gabriela Claros,David Toro-Tobon,Esteban Cabezas,Ricardo Loor-Torres,Maria Mateo Chavez,Kerly Guevara Maldonado,Luis Vilatuna Andrango,Maria Lizarazo Jimenez,Ivan Mateo Alzamora,Misk Al Zahidy,Marcelo Montero,Ana Cristina Proano,Cristian Soto Jacome,Jungwei W. Fan,Oscar J. Ponce-Ponte,Megan E. Branda,Naykky Singh Ospina,Juan P. Brito*

Main category: cs.CL

TL;DR: 摘要研究了甲状腺意外发现（ITFs）的普遍性、特征及临床后果，利用NLP技术分析了影像报告，发现ITFs与甲状腺癌的过度诊断相关。


<details>
  <summary>Details</summary>
Motivation: 随着影像技术的广泛应用，甲状腺意外发现越来越常见，但其临床影响尚未明确。本研究旨在通过NLP技术揭示ITFs的流行病学特征及其后续影响。

Method: 研究采用回顾性队列设计，使用基于Transformer的NLP技术分析影像报告，提取ITFs及相关特征，并通过逻辑回归分析影响因素。

Result: 在115,683例患者中，7.8%发现ITFs，多见于女性、老年人和高BMI者。ITFs显著增加甲状腺结节、活检、手术及癌症诊断的风险。

Conclusion: ITFs与甲状腺癌过度诊断相关，呼吁标准化报告和有选择性的随访以减少不必要的医疗干预。

Abstract: Importance Incidental thyroid findings (ITFs) are increasingly detected on
imaging performed for non-thyroid indications. Their prevalence, features, and
clinical consequences remain undefined. Objective To develop, validate, and
deploy a natural language processing (NLP) pipeline to identify ITFs in
radiology reports and assess their prevalence, features, and clinical outcomes.
Design, Setting, and Participants Retrospective cohort of adults without prior
thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from
July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline
identified ITFs and extracted nodule characteristics from image reports from
multiple modalities and body regions. Main Outcomes and Measures Prevalence of
ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer
diagnosis. Logistic regression identified demographic and imaging-related
factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%
women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more
likely in women, older adults, those with higher BMI, and when imaging was
ordered by oncology or internal medicine. Compared with chest CT, ITFs were
more likely via neck CT, PET, and nuclear medicine scans. Nodule
characteristics were poorly documented, with size reported in 44% and other
features in fewer than 15% (e.g. calcifications). Compared with patients
without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,
biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were
papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were
common and strongly associated with cascades leading to the detection of small,
low-risk cancers. These findings underscore the role of ITFs in thyroid cancer
overdiagnosis and the need for standardized reporting and more selective
follow-up.

</details>


### [20] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

TL;DR: QCoder Benchmark是一个评估大型语言模型在量子编程任务中表现的框架，支持通过量子模拟器环境提供反馈，并对比人类编写的代码。实验显示，高级模型的准确率较低，而基于推理的模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 量子编程领域的大型语言模型应用尚未充分探索，尤其是在与硬件交互的任务中，因此需要一个专门的评估框架。

Method: 引入QCoder Benchmark，结合量子模拟器环境和人类编写的代码，评估模型在量子编程任务中的表现。

Result: GPT-4o模型的准确率为18.97%，而基于推理的模型o3达到78%，超过人类编写的代码的平均成功率（39.98%）。

Conclusion: 量子编程任务对语言模型提出了挑战，基于推理的模型表现更优，提出了未来的研究方向。

Abstract: Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


### [21] [Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking](https://arxiv.org/abs/2510.26122)
*Feng Ju,Zeyu Qin,Rui Min,Zhitao He,Lingpeng Kong,Yi R. Fung*

Main category: cs.CL

TL;DR: 论文提出了一种“一个问题，多个解决方案”（1PNS）的训练范式，通过引入推理路径分歧（RPD）度量来提升大语言模型的推理多样性，实验结果显示1PNS显著提升了输出多样性及推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统“一个问题，一个解决方案”（1P1S）训练范式导致模型输出多样性低，限制了推理能力的提升，因此需要新的方法来增加推理路径的多样性。

Method: 提出1PNS训练范式，引入RPD度量来评估多步推理链之间的语义差异，并利用RPD选择最大多样性的解决方案集进行模型微调。

Result: 实验证明，1PNS训练显著提升了输出多样性和pass@k表现，pass@16平均提升2.80%，在AIME24上提升4.99%。

Conclusion: 1PNS训练范式通过增加推理多样性，进一步放大了测试时间扩展（TTS）的效果，为提升大语言模型的推理能力提供了新思路。

Abstract: While Test-Time Scaling (TTS) has proven effective in improving the reasoning
ability of large language models (LLMs), low diversity in model outputs often
becomes a bottleneck; this is partly caused by the common "one problem, one
solution" (1P1S) training practice, which provides a single canonical answer
and can push models toward a narrow set of reasoning paths. To address this, we
propose a "one problem, multiple solutions" (1PNS) training paradigm that
exposes the model to a variety of valid reasoning trajectories and thus
increases inference diversity. A core challenge for 1PNS is reliably measuring
semantic differences between multi-step chains of thought, so we introduce
Reasoning Path Divergence (RPD), a step-level metric that aligns and scores
Long Chain-of-Thought solutions to capture differences in intermediate
reasoning. Using RPD, we curate maximally diverse solution sets per problem and
fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields
more varied outputs and higher pass@k, with an average +2.80% gain in pass@16
over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that
1PNS further amplifies the effectiveness of TTS. Our code is available at
https://github.com/fengjujf/Reasoning-Path-Divergence .

</details>


### [22] [On the Influence of Discourse Relations in Persuasive Texts](https://arxiv.org/abs/2510.26124)
*Nawar Turk,Sevag Kaspar,Leila Kosseim*

Main category: cs.CL

TL;DR: 论文研究了说服技巧（PTs）与语篇关系（DRs）之间的关系，借助大语言模型（LLMs）和提示工程，构建了一个结合PTs和DRs标注的银数据集，并发现六种语篇关系在说服性文本中起关键作用。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏同时标注说服技巧和语篇关系的数据集，因此作者希望通过LLMs构建这样的数据集，并探索两者之间的关系，以帮助检测在线宣传和虚假信息。

Method: 作者利用SemEval 2023 Task 3的数据集标注了19种说服技巧，并通过LLMs和提示工程开发了40种DR分类器，最后通过多数投票策略生成了5个银数据集。

Result: 统计分析表明，六种语篇关系（Cause, Purpose, Contrast, Cause+Belief, Concession, Condition）在说服性文本中起关键作用，尤其是与特定说服技巧（如Loaded Language）相关联。

Conclusion: 这项研究不仅有助于理解有效沟通的机制，还为检测在线宣传和虚假信息提供了新的视角。

Abstract: This paper investigates the relationship between Persuasion Techniques (PTs)
and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and
prompt engineering. Since no dataset annotated with both PTs and DRs exists, we
took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point
and developed LLM-based classifiers to label each instance of the dataset with
one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10
different prompts, resulting in 40 unique DR classifiers. Ensemble models using
different majority-pooling strategies were used to create 5 silver datasets of
instances labelled with both persuasion techniques and level-2 PDTB senses. The
silver dataset sizes vary from 1,281 instances to 204 instances, depending on
the majority pooling technique used. Statistical analysis of these silver
datasets shows that six discourse relations (namely Cause, Purpose, Contrast,
Cause+Belief, Concession, and Condition) play a crucial role in persuasive
texts, especially in the use of Loaded Language, Exaggeration/Minimisation,
Repetition and to cast Doubt. This insight can contribute to detecting online
propaganda and misinformation, as well as to our general understanding of
effective communication.

</details>


### [23] [Similarity-Distance-Magnitude Language Models](https://arxiv.org/abs/2510.26183)
*Allen Schmaltz*

Main category: cs.CL

TL;DR: 论文介绍了SDM语言模型，通过微调实现序列预测，优化生成内容在高概率区域的占比，并通过SDM激活层进行二进制分类。实验表明，现有预训练模型可轻松转换为SDM模型，且统计效率优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 旨在优化语言模型的生成内容质量，通过SDM激活层减少生成内容的不确定性，提高模型的统计效率。

Method: 使用监督微调方法，结合对比输入编码方案和在线生成的硬负样本，通过SDM激活层估计基变换以优化损失函数。

Result: SDM模型在统计效率上优于现有基准方法，减少了生成中的弃权情况。

Conclusion: SDM语言模型通过微调和SDM激活层的应用，有效提升了序列预测的质量和效率。

Abstract: We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which
are sequence prediction models fine-tuned to maximize the proportion of
generations in the well-calibrated, high-probability region partitioned by a
final-layer SDM activation layer used for binary classification of
instruction-following. We demonstrate that existing pre-trained decoder-only
Transformer LMs can be readily converted into SDM LMs via supervised
fine-tuning, using the final-layer SDM activation layer during training to
estimate a change-of-base for a supervised next-token loss over a contrastive
input encoding scheme, with additional hard negative examples generated online
during training. This results in reduced abstentions (i.e., improved
statistical efficiency) compared to strong supervised baselines.

</details>


### [24] [Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation](https://arxiv.org/abs/2510.26200)
*Woojin Kim,Jaeyoung Do*

Main category: cs.CL

TL;DR: 论文提出Token Timestep Allocation (TTA)方法，通过为每个token分配时间步来解决扩散语言模型(DLMs)的更新遗忘问题，从而提升生成文本的可控性和流畅性。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型(DLMs)在细粒度修正中存在更新遗忘问题，导致语义编辑丢失和生成文本的流畅性与连贯性下降。

Method: 提出Token Timestep Allocation (TTA)，通过为每个token分配时间步实现软语义排序：关键token早期冻结，不确定token持续修正。支持固定策略或自适应策略。

Result: TTA在情感控制和去毒任务中表现优异，提高了准确性（20%以上）和流畅性（困惑度降低近一半），同时降低了毒性。

Conclusion: 通过时间步分配的软排序是解决更新遗忘问题、实现稳定可控扩散文本生成的关键。

Abstract: While diffusion language models (DLMs) enable fine-grained refinement, their
practical controllability remains fragile. We identify and formally
characterize a central failure mode called update forgetting, in which uniform
and context agnostic updates induce token level fluctuations across timesteps,
erasing earlier semantic edits and disrupting the cumulative refinement
process, thereby degrading fluency and coherence. As this failure originates in
uniform and context agnostic updates, effective control demands explicit token
ordering. We propose Token Timestep Allocation (TTA), which realizes soft and
semantic token ordering via per token timestep schedules: critical tokens are
frozen early, while uncertain tokens receive continued refinement. This
timestep based ordering can be instantiated as either a fixed policy or an
adaptive policy driven by task signals, thereby supporting a broad spectrum of
refinement strategies. Because it operates purely at inference time, it applies
uniformly across various DLMs and naturally extends to diverse supervision
sources. Empirically, TTA improves controllability and fluency: on sentiment
control, it yields more than 20 percent higher accuracy and nearly halves
perplexity using less than one fifth the steps; in detoxification, it lowers
maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).
Together, these results demonstrate that softened ordering via timestep
allocation is the critical lever for mitigating update forgetting and achieving
stable and controllable diffusion text generation.

</details>


### [25] [Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning](https://arxiv.org/abs/2510.26205)
*Qi Luo,Xiaonan Li,Tingshuo Fan,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了GlobalQA基准，用于评估全局RAG能力，发现现有方法表现不佳，并提出GlobalRAG框架显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估基准主要关注局部RAG能力，而忽略了全局RAG的需求，例如跨文档的信息聚合与分析。

Method: 提出GlobalRAG框架，包括分块检索、LLM驱动的智能过滤器以及聚合模块。

Result: GlobalRAG在Qwen2.5-14B模型上实现了6.63 F1，显著优于基线方法的1.51 F1。

Conclusion: GlobalRAG有效解决了全局RAG任务中的挑战，并在实验中表现出色。

Abstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.

</details>


### [26] [Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs](https://arxiv.org/abs/2510.26253)
*Takuma Sato,Seiya Kawano,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 研究通过提供语用理论作为提示，提升语言模型在理解隐含含义任务中的表现，最高提升9.6%。


<details>
  <summary>Details</summary>
Motivation: 隐含含义理解在人类交流中至关重要，语言模型需具备类似能力。

Method: 提出一种方法，将Gricean语用学和关联理论等语用理论作为提示，引导模型逐步推理。

Result: 实验显示，相比基线，该方法使模型性能提升最高达9.6%；仅提及理论名称也能带来1-3%的提升。

Conclusion: 语用理论提示是一种有效的上下文学习方法，可显著提升语言模型在隐含含义任务中的表现。

Abstract: The ability to accurately interpret implied meanings plays a crucial role in
human communication and language use, and language models are also expected to
possess this capability. This study demonstrates that providing language models
with pragmatic theories as prompts is an effective in-context learning approach
for tasks to understand implied meanings. Specifically, we propose an approach
in which an overview of pragmatic theories, such as Gricean pragmatics and
Relevance Theory, is presented as a prompt to the language model, guiding it
through a step-by-step reasoning process to derive a final interpretation.
Experimental results showed that, compared to the baseline, which prompts
intermediate reasoning without presenting pragmatic theories (0-shot
Chain-of-Thought), our methods enabled language models to achieve up to 9.6\%
higher scores on pragmatic reasoning tasks. Furthermore, we show that even
without explaining the details of pragmatic theories, merely mentioning their
names in the prompt leads to a certain performance improvement (around 1-3%) in
larger models compared to the baseline.

</details>


### [27] [Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages](https://arxiv.org/abs/2510.26254)
*Mérilin Sousa Silva,Sina Ahmadi*

Main category: cs.CL

TL;DR: 调查预训练语言模型是否能像双语者一样区分借词与原生词，结果发现模型表现不佳，显示出对借词的偏好。


<details>
  <summary>Details</summary>
Motivation: 研究预训练语言模型是否具备识别借词的能力，特别是在双语环境中，这对于开发支持少数民族语言的NLP工具和语言保护具有重要意义。

Method: 评估了多种预训练语言模型在10种语言中的表现，通过明确的指令和上下文信息测试其区分借词和原生词的能力。

Result: 结果表明，模型在区分借词与原生词方面表现不佳，显示出对借词的偏好。

Conclusion: 这项工作揭示了现代NLP系统在借词识别上的局限性，对开发支持少数民族语言的工具和保护受主导语言词汇压力影响的语言社区具有启示意义。

Abstract: Throughout language history, words are borrowed from one language to another
and gradually become integrated into the recipient's lexicon. Speakers can
often differentiate these loanwords from native vocabulary, particularly in
bilingual communities where a dominant language continuously imposes lexical
items on a minority language. This paper investigates whether pretrained
language models, including large language models, possess similar capabilities
for loanword identification. We evaluate multiple models across 10 languages.
Despite explicit instructions and contextual information, our results show that
models perform poorly in distinguishing loanwords from native ones. These
findings corroborate previous evidence that modern NLP systems exhibit a bias
toward loanwords rather than native equivalents. Our work has implications for
developing NLP tools for minority languages and supporting language
preservation in communities under lexical pressure from dominant languages.

</details>


### [28] [Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual](https://arxiv.org/abs/2510.26271)
*Sukrit Sriratanawilai,Jhayahgrit Thongwat,Romrawin Chumpu,Patomporn Payoungkhamdee,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 该论文研究了在视觉语言模型（VLMs）中知识蒸馏（KD）对多语言性能的影响，发现不同蒸馏方法在模型压缩后的跨语言表示一致性和任务稳定性上表现不均。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决视觉语言模型在多语言任务中性能不均的问题，特别是模型尺寸减小时表现更差的现象。

Method: 方法是对五种知识蒸馏方法进行实证研究，分析它们在CLIP和SigLIP2模型上对多语言检索和视觉问答任务的影响。

Result: 研究发现某些蒸馏配置在模型尺寸减半时仍能保持或提升多语言检索的鲁棒性，而其他配置则无法保持跨任务的稳定性。

Conclusion: 结论是知识蒸馏的设计对多语言任务性能有关键影响，单纯的聚合准确性指标无法全面反映其效果。

Abstract: Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.

</details>


### [29] [Do LLMs Signal When They're Right? Evidence from Neuron Agreement](https://arxiv.org/abs/2510.26277)
*Kang Chen,Yaoning Wang,Kai Xiong,Zhuoka Feng,Wenhe Sun,Haotian Chen,Yixin Cao*

Main category: cs.CL

TL;DR: 论文提出了Neuron Agreement Decoding (NAD)方法，利用神经元激活信号进行无监督解码，减少了计算开销并提高了模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖外部输出信号（如词元概率或自评估）来评分，但这些信号可能在校准后表现不佳。论文通过分析神经元激活行为发现其能提供更丰富的内部动态信息。

Method: NAD方法通过神经元激活稀疏性和跨样本激活一致性选择候选答案，无需依赖文本输出或标注数据。

Result: 在数学和科学基准测试中，NAD与多数投票法表现相当；在开放编码任务中优于Avg@64方法,并能提前99%停止生成任务。

Conclusion: NAD证明了内部信号能提供可靠、可扩展且高效的解码引导，显著降低了计算资源消耗。

Abstract: Large language models (LLMs) commonly boost reasoning via
sample-evaluate-ensemble decoders, achieving label free gains without ground
truth. However, prevailing strategies score candidates using only external
outputs such as token probabilities, entropies, or self evaluations, and these
signals can be poorly calibrated after post training. We instead analyze
internal behavior based on neuron activations and uncover three findings: (1)
external signals are low dimensional projections of richer internal dynamics;
(2) correct responses activate substantially fewer unique neurons than
incorrect ones throughout generation; and (3) activations from correct
responses exhibit stronger cross sample agreement, whereas incorrect ones
diverge. Motivated by these observations, we propose Neuron Agreement Decoding
(NAD), an unsupervised best-of-N method that selects candidates using
activation sparsity and cross sample neuron agreement, operating solely on
internal signals and without requiring comparable textual outputs. NAD enables
early correctness prediction within the first 32 generated tokens and supports
aggressive early stopping. Across math and science benchmarks with verifiable
answers, NAD matches majority voting; on open ended coding benchmarks where
majority voting is inapplicable, NAD consistently outperforms Avg@64. By
pruning unpromising trajectories early, NAD reduces token usage by 99% with
minimal loss in generation quality, showing that internal signals provide
reliable, scalable, and efficient guidance for label free ensemble decoding.

</details>


### [30] [Unravelling the Mechanisms of Manipulating Numbers in Language Models](https://arxiv.org/abs/2510.26285)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Bertram Højer,Michal Spiegel,Raúl Vázquez,Aman Sinha,Josef Kuchař,Philipp Mondorf*

Main category: cs.CL

TL;DR: 研究探索了大型语言模型如何处理数字，发现尽管存在错误输出，模型对数字的表征具有系统性和普遍性，并提出了改进模型架构的潜力。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明，尽管不同的大型语言模型在处理数字时会产生错误输出，但其对数字的嵌入表示却相似且准确。本文旨在解释这一矛盾现象，探讨模型如何处理数字并量化其机制的准确性下限。

Method: 研究分析了语言模型对数字的表征，包括隐藏状态和不同输入上下文中的表现，并设计了通用探针以追踪错误来源。

Result: 发现语言模型对数字的表征具有系统性、高准确性和跨模型的普遍性，可以通过特定层追踪错误来源。

Conclusion: 研究为理解预训练语言模型处理数字的方式提供了基础，并展示了改进模型架构的可能性。

Abstract: Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.

</details>


### [31] [Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games](https://arxiv.org/abs/2510.26298)
*Jingran Zhang,Ning Li,Justin Cui*

Main category: cs.CL

TL;DR: OpenAI的ChatGPT Atlas展示了在动态网页环境中进行交互的能力，但在实时游戏中的表现有显著限制。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索Atlas在动态交互网页环境中的能力，尤其是与传统信息检索任务的对比。

Method: 通过浏览器游戏（如T-Rex Runner、Sudoku等）测试Atlas的表现，并用游戏分数作为定量指标。

Result: Atlas在逻辑推理任务（如数独）中表现优异，但在需要精确时序和控制的实时游戏中表现较差。

Conclusion: Atlas在分析处理能力强，但在需要实时交互的动态环境中仍有明显局限性。

Abstract: OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,
enabling the model to analyze webpages, process user intents, and execute
cursor and keyboard inputs directly within the browser. While its capacity for
information retrieval tasks has been demonstrated, its performance in dynamic,
interactive environments remains less explored. In this study, we conduct an
early evaluation of Atlas's web interaction capabilities using browser-based
games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,
and Stein.world. We employ in-game performance scores as quantitative metrics
to assess performance across different task types. Our results show that Atlas
performs strongly in logical reasoning tasks like Sudoku, completing puzzles
significantly faster than human baselines, but struggles substantially in
real-time games requiring precise timing and motor control, often failing to
progress beyond initial obstacles. These findings suggest that while Atlas
demonstrates capable analytical processing, there remain notable limitations in
dynamic web environments requiring real-time interaction. The website of our
project can be found at https://atlas-game-eval.github.io.

</details>


### [32] [SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling](https://arxiv.org/abs/2510.26322)
*Fares Fawzi,Vinitra Swamy,Dominik Glandorf,Tanya Nazaretsky,Tanja Käser*

Main category: cs.CL

TL;DR: SCRIBE是一个用于教育场景的框架，通过小型开源模型结合工具增强推理，生成个性化反馈，解决了隐私、计算资源和教学有效性等问题。


<details>
  <summary>Details</summary>
Motivation: 在教育环境中使用语言模型时，面临隐私、计算资源和教学有效性三大挑战，急需小规模、本地运行且能生成可靠反馈的模型。

Method: 提出了SCRIBE框架，结合领域专用工具和自反思推理管道，通过两阶段LoRA微调技术训练了3B和8B参数的小型模型。

Result: 8B-SCRIBE在相关性和可操作性上表现优于或接近更大模型，学生评价与GPT-4o和Llama-3.3 70B相当。

Conclusion: SCRIBE为资源有限且注重隐私的教育应用提供了可行解决方案。

Abstract: Language models can be used to provide interactive, personalized student
feedback in educational settings. However, real-world deployment faces three
key challenges: privacy concerns, limited computational resources, and the need
for pedagogically valid responses. These constraints require small, open-source
models that can run locally and reliably ground their outputs in correct
information. We introduce SCRIBE, a framework for multi-hop, tool-augmented
reasoning designed to generate valid responses to student questions about
feedback reports. SCRIBE combines domain-specific tools with a self-reflective
inference pipeline that supports iterative reasoning, tool use, and error
recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA
fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned
GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models
achieve comparable or superior quality to much larger models in key dimensions
such as relevance and actionability, while being perceived on par with GPT-4o
and Llama-3.3 70B by students. These findings demonstrate the viability of
SCRIBE for low-resource, privacy-sensitive educational applications.

</details>


### [33] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: ACER方法通过生成系统化课程和数据，将通用大模型转化为特定领域专家，提升其在经济学等专业领域的表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型在专业领域理解上的不足，提出了ACER方法以系统化提升其专业能力。

Method: ACER生成教科书风格的课程，结合Bloom分类法创建问答对，并通过持续预训练和交叉课程安排进行优化。

Result: 实验显示ACER在专业领域（如微观经济学）的准确率提升5%，且在非目标领域也有正向知识迁移。

Conclusion: ACER是一种可扩展且有效的方法，能够显著缩小大语言模型在专业领域的差距。

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [34] [MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data](https://arxiv.org/abs/2510.26345)
*Mykhailo Poliakov,Nadiya Shvai*

Main category: cs.CL

TL;DR: 论文研究了一种名为MisSynth的管道，结合检索增强生成（RAG）生成合成谬误样本，用于微调大型语言模型（LLM），显著提升了识别科学谬误的准确性。


<details>
  <summary>Details</summary>
Motivation: 健康相关的错误信息普遍存在且潜在危害大，但其识别难度高，尤其是当这些信息曲解科学发现时。研究旨在探索如何通过合成数据和轻量级微调技术提升LLM识别谬误的能力。

Method: 提出MisSynth管道，采用RAG生成合成谬误样本，并用这些样本微调LLM。实验使用了MISSCI数据集和框架。

Result: 微调后的模型性能显著提升，如LLaMA 3.1 8B模型的F1分数比基线提高了35%。

Conclusion: 通过合成谬误数据增强有限标注资源，可以显著提升LLM在零样本分类任务中的表现，即使计算资源有限。

Abstract: Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.

</details>


### [35] [The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration](https://arxiv.org/abs/2510.26352)
*Kotaro Furuya,Yuichi Kitagawa*

Main category: cs.CL

TL;DR: 论文提出了一种基于语言模型图的自动团队构建框架，通过语义一致性和社区检测发现协同合作的LLM团队，无需先验知识。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM团队需要协同组合才能超越单一模型能力，但现有方法因模型不透明而难以实现最优组合。

Method: 构建语言模型图，通过对话语义一致性映射模型关系，并用社区检测识别协同群体。

Result: 实验表明，该方法能发现功能一致的协同团队，性能优于随机基线，接近人工标注团队。

Conclusion: 为自动设计多智能体LLM团队提供了新方法。

Abstract: While a multi-agent approach based on large language models (LLMs) represents
a promising strategy to surpass the capabilities of single models, its success
is critically dependent on synergistic team composition. However, forming
optimal teams is a significant challenge, as the inherent opacity of most
models obscures the internal characteristics necessary for effective
collaboration. In this paper, we propose an interaction-centric framework for
automatic team composition that does not require any prior knowledge including
their internal architectures, training data, or task performances. Our method
constructs a "language model graph" that maps relationships between models from
the semantic coherence of pairwise conversations, and then applies community
detection to identify synergistic model clusters. Our experiments with diverse
LLMs demonstrate that the proposed method discovers functionally coherent
groups that reflect their latent specializations. Priming conversations with
specific topics identified synergistic teams which outperform random baselines
on downstream benchmarks and achieve comparable accuracy to that of
manually-curated teams based on known model specializations. Our findings
provide a new basis for the automated design of collaborative multi-agent LLM
teams.

</details>


### [36] [On the Role of Context for Discourse Relation Classification in Scientific Writing](https://arxiv.org/abs/2510.26354)
*Stephen Wan,Wei Liu,Michael Strube*

Main category: cs.CL

TL;DR: 研究了预训练语言模型（PLM）和大语言模型（LLM）在科学文献中的话语关系分类（DRC）任务中的表现，发现话语结构定义的上下文对任务有帮助，并分析了哪些科学话语关系类型最受益。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在科研工作流中的广泛应用，研究者希望通过话语层面信息为AI生成的科学主张提供证据支持，话语结构推断是这一目标的第一步。

Method: 研究通过实验评估PLM和LLM在科学文献DRC任务中的表现，特别关注上下文（话语结构）的作用。

Result: 实验表明，上下文（话语结构定义的）通常对DRC任务有帮助，某些特定类型的科学话语关系受益更多。

Conclusion: 初步研究表明，PLM和LLM在科学文献DRC任务中有潜力，且上下文是关键因素之一。

Abstract: With the increasing use of generative Artificial Intelligence (AI) methods to
support science workflows, we are interested in the use of discourse-level
information to find supporting evidence for AI generated scientific claims. A
first step towards this objective is to examine the task of inferring discourse
structure in scientific writing.
  In this work, we present a preliminary investigation of pretrained language
model (PLM) and Large Language Model (LLM) approaches for Discourse Relation
Classification (DRC), focusing on scientific publications, an under-studied
genre for this task. We examine how context can help with the DRC task, with
our experiments showing that context, as defined by discourse structure, is
generally helpful. We also present an analysis of which scientific discourse
relation types might benefit most from context.

</details>


### [37] [OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education](https://arxiv.org/abs/2510.26422)
*Min Zhang,Hao Chen,Hao Chen,Wenqi Zhang,Didi Zhu,Xin Lin,Bo Jiang,Aimin Zhou,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: 本文介绍了OmniEduBench，一个全面评估中文教育能力的基准数据集，涵盖知识与素养两个维度，发现当前大型语言模型在教育应用中仍有显著改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型及其基准主要关注知识维度，忽视了素养能力的评估，且缺乏多样性，尤其是在中文教育领域。

Method: 构建了一个包含24.602K高质量问答对的中文教育基准OmniEduBench，分为知识与素养两个维度，进一步细分6类，覆盖61个学科和11种题型。

Result: 实验显示，知识维度中仅Gemini-2.5 Pro超过60%准确率，而素养维度中表现最好的QWQ模型仍落后人类近30%。

Conclusion: 结果揭示了大型语言模型在教育应用中的挑战和巨大改进空间。

Abstract: With the rapid development of large language models (LLMs), various LLM-based
works have been widely applied in educational fields. However, most existing
LLMs and their benchmarks focus primarily on the knowledge dimension, largely
neglecting the evaluation of cultivation capabilities that are essential for
real-world educational scenarios. Additionally, current benchmarks are often
limited to a single subject or question type, lacking sufficient diversity.
This issue is particularly prominent within the Chinese context. To address
this gap, we introduce OmniEduBench, a comprehensive Chinese educational
benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.
The data is meticulously divided into two core dimensions: the knowledge
dimension and the cultivation dimension, which contain 18.121K and 6.481K
entries, respectively. Each dimension is further subdivided into 6 fine-grained
categories, covering a total of 61 different subjects (41 in the knowledge and
20 in the cultivation). Furthermore, the dataset features a rich variety of
question formats, including 11 common exam question types, providing a solid
foundation for comprehensively evaluating LLMs' capabilities in education.
Extensive experiments on 11 mainstream open-source and closed-source LLMs
reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro
surpassed 60\% accuracy, while in the cultivation dimension, the
best-performing model, QWQ, still trailed human intelligence by nearly 30\%.
These results highlight the substantial room for improvement and underscore the
challenges of applying LLMs in education.

</details>


### [38] [1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models](https://arxiv.org/abs/2510.26446)
*Zeliang Zong,Kai Zhang,Zheyang Li,Wenming Tan,Ye Ren,Yiyan Zhai,Jilin Hu*

Main category: cs.CL

TL;DR: SSLC方法结合稀疏优化和低秩逼近，显著压缩大型语言模型并保持性能，实验表明其优于独立方法。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在带宽和计算资源上的高需求问题，探索稀疏优化与低秩逼近的协同效应。

Method: 提出SSLC方法，将低秩逼近和稀疏优化统一为迭代优化问题并求解。

Result: 在LLaMA和Qwen2.5模型上，SSLC压缩50%且性能无损，速度提升1.63倍。

Conclusion: SSLC为高效部署大型语言模型提供实用解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
language comprehension and generation; however, their widespread adoption is
constrained by substantial bandwidth and computational demands. While pruning
and low-rank approximation have each demonstrated promising performance
individually, their synergy for LLMs remains underexplored. We introduce
\underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank
\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths
of both techniques: low-rank approximation compresses the model by retaining
its essential structure with minimal information loss, whereas sparse
optimization eliminates non-essential weights, preserving those crucial for
generalization. Based on theoretical analysis, we first formulate the low-rank
approximation and sparse optimization as a unified problem and solve it by
iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models
(7B-70B) show that SSLC, without any additional training steps, consistently
surpasses standalone methods, achieving state-of-the-arts results. Notably,
SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least
1.63$\times$ speedup, offering a practical solution for efficient LLM
deployment.

</details>


### [39] [Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs](https://arxiv.org/abs/2510.26512)
*Dipak Meher,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: 论文提出CORE-KG框架，通过核心解析模块和结构化提示改善法律文本的知识图谱构建效果，并通过消融实验量化了各组件的贡献。


<details>
  <summary>Details</summary>
Motivation: 解决法律文本中知识图谱构建的噪声和节点重复问题，提升自动化分析的准确性。

Method: 结合类型感知的核心解析模块和领域指导的结构化提示，优化LLM的知识图谱生成。

Result: 核心解析模块减少28.32%的节点重复和4.32%的噪声节点；结构化提示减少4.34%的节点重复和73.33%的噪声节点。

Conclusion: CORE-KG框架显著提升了法律文本知识图谱的构建效果，为复杂文本的结构化提取提供了实证指导。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer critical insights but are often unstructured,
lexically dense, and filled with ambiguous or shifting references, which pose
significant challenges for automated knowledge graph (KG) construction. While
recent LLM-based approaches improve over static templates, they still generate
noisy, fragmented graphs with duplicate nodes due to the absence of guided
extraction and coreference resolution. The recently proposed CORE-KG framework
addresses these limitations by integrating a type-aware coreference module and
domain-guided structured prompts, significantly reducing node duplication and
legal noise. In this work, we present a systematic ablation study of CORE-KG to
quantify the individual contributions of its two key components. Our results
show that removing coreference resolution results in a 28.32% increase in node
duplication and a 4.32% increase in noisy nodes, while removing structured
prompts leads to a 4.34% increase in node duplication and a 73.33% increase in
noisy nodes. These findings offer empirical insights for designing robust
LLM-based pipelines for extracting structured representations from complex
legal texts.

</details>


### [40] [Hebrew Diacritics Restoration using Visual Representation](https://arxiv.org/abs/2510.26521)
*Yair Elboher,Yuval Pinter*

Main category: cs.CL

TL;DR: DIVRIT是一种新颖的希伯来语变音符号恢复系统，将其视为零样本分类问题，并通过视觉语言模型处理未变音文本，实现了高准确率。


<details>
  <summary>Details</summary>
Motivation: 希伯来语变音符号恢复对准确发音和消除文本歧义至关重要，现有机器学习方法虽已取得进展，但仍需进一步优化。

Method: DIVRIT将任务视为零样本分类问题，使用希伯来视觉语言模型处理未变音文本，动态生成候选集并选择最合适的变音模式。

Result: 系统在‘oracle’设置中表现出高准确率，且通过架构优化和训练方法改进显著提升了泛化能力。

Conclusion: 视觉表示在希伯来语变音符号恢复中具有潜力，DIVRIT展现了自动化解决方案的前景。

Abstract: Diacritics restoration in Hebrew is a fundamental task for ensuring accurate
word pronunciation and disambiguating textual meaning. Despite the language's
high degree of ambiguity when unvocalized, recent machine learning approaches
have significantly advanced performance on this task.
  In this work, we present DIVRIT, a novel system for Hebrew diacritization
that frames the task as a zero-shot classification problem. Our approach
operates at the word level, selecting the most appropriate diacritization
pattern for each undiacritized word from a dynamically generated candidate set,
conditioned on the surrounding textual context. A key innovation of DIVRIT is
its use of a Hebrew Visual Language Model, which processes undiacritized text
as an image, allowing diacritic information to be embedded directly within the
input's vector representation.
  Through a comprehensive evaluation across various configurations, we
demonstrate that the system effectively performs diacritization without relying
on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting
where the correct diacritized form is guaranteed to be among the provided
candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic
architectural enhancements and optimized training methodologies yield
significant improvements in the system's overall generalization capabilities.
These findings highlight the promising potential of visual representations for
accurate and automated Hebrew diacritization.

</details>


### [41] [Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models](https://arxiv.org/abs/2510.26577)
*Yinrong Hong,Zhiquan Tan,Kai Hu*

Main category: cs.CL

TL;DR: CAST是一种动态树解码方法，考虑了GPU配置和批量大小等推理成本，显著提升了大型语言模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型因自回归设计和规模庞大导致的推理延迟问题。

Method: 提出CAST动态树解码方法，综合考虑GPU设备和批量大小等系统变量，动态优化树结构。

Result: 在六种任务和六种不同LLM上测试，CAST比传统方法快5.2倍，性能优于现有技术5%至20%。

Conclusion: CAST通过动态优化树结构，显著提升了推理效率和性能。

Abstract: Large Language Models (LLMs) face significant inference latency challenges
stemming from their autoregressive design and large size. To address this,
speculative decoding emerges as a solution, enabling the simultaneous
generation and validation of multiple tokens. While recent approaches like
EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,
they often neglect the impact of crucial system variables such as GPU devices
and batch sizes.
  Therefore, we introduce a new dynamic tree decoding approach called CAST that
takes into account inference costs, including factors such as GPU
configurations and batch sizes, to dynamically refine the tree structure.
Through comprehensive experimentation across six diverse tasks and utilizing
six distinct LLMs, our methodology demonstrates remarkable results, achieving
speeds up to 5.2 times faster than conventional decoding methods. Moreover, it
generally outperforms existing state-of-the-art techniques from 5% to 20%.

</details>


### [42] [Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model](https://arxiv.org/abs/2510.26622)
*Biao Zhang,Yong Cheng,Siamak Shakeri,Xinyi Wang,Min Ma,Orhan Firat*

Main category: cs.CL

TL;DR: 论文重新探讨了编码器-解码器架构的大语言模型（RedLLM），对比了其与主流解码器架构（DecLLM）在不同规模下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的研究主要集中于解码器架构，缺乏对编码器-解码器架构的全面比较，本研究旨在填补这一空白。

Method: 通过增强编码器-解码器架构（RedLLM），并与解码器架构（DecLLM）进行比较，实验范围从1.5亿到80亿参数规模，使用RedPajama V1进行预训练，FLAN进行指令调优。

Result: RedLLM展现出与DecLLM相当的扩展性和上下文长度外推能力，且在推理效率上更优。经过指令调优后，RedLLM在多任务中表现相当甚至更好。

Conclusion: 本研究结果表明编码器-解码器架构仍具潜力，值得进一步探索，以开发更强大高效的大型语言模型。

Abstract: Recent large language model (LLM) research has undergone an architectural
shift from encoder-decoder modeling to nowadays the dominant decoder-only
modeling. This rapid transition, however, comes without a rigorous comparative
analysis especially \textit{from the scaling perspective}, raising concerns
that the potential of encoder-decoder models may have been overlooked. To fill
this gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent
recipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison
between RedLLM, pretrained with prefix language modeling (LM), and DecLLM,
pretrained with causal LM, at different model scales, ranging from $\sim$150M
to $\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for
instruction tuning, our experiments show that RedLLM produces compelling
scaling properties and surprisingly strong performance. While DecLLM is overall
more compute-optimal during pretraining, RedLLM demonstrates comparable scaling
and context length extrapolation capabilities. After instruction tuning, RedLLM
achieves comparable and even better results on various downstream tasks while
enjoying substantially better inference efficiency. We hope our findings could
inspire more efforts on re-examining RedLLM, unlocking its potential for
developing powerful and efficient LLMs.

</details>


### [43] [Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models](https://arxiv.org/abs/2510.26683)
*Mingchen Tu,Zhiqiang Liu,Juan Li,Liangyurui Liu,Junjie Wang,Lei Liang,Wen Zhang*

Main category: cs.CL

TL;DR: Evontree框架利用少量高质量本体规则，系统性提取、验证和增强大语言模型（LLM）中的领域知识，无需大量外部数据。


<details>
  <summary>Details</summary>
Motivation: 在数据敏感领域（如医疗），缺乏高质量领域特定训练数据阻碍了LLM的适配。本体规则蕴含领域智慧，可作为知识管理的基础。

Method: Evontree从原始模型中提取领域本体，使用核心本体规则检测不一致性，并通过自蒸馏微调强化知识。

Result: 在医疗问答基准测试中，Evontree优于未修改模型和其他监督基线，准确率最高提升3.7%。

Conclusion: Evontree在低资源领域适配中表现出高效、稳健的效果。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities
across multiple domains by leveraging massive pre-training and curated
fine-tuning data. However, in data-sensitive fields such as healthcare, the
lack of high-quality, domain-specific training corpus hinders LLMs' adaptation
for specialized applications. Meanwhile, domain experts have distilled domain
wisdom into ontology rules, which formalize relationships among concepts and
ensure the integrity of knowledge management repositories. Viewing LLMs as
implicit repositories of human knowledge, we propose Evontree, a novel
framework that leverages a small set of high-quality ontology rules to
systematically extract, validate, and enhance domain knowledge within LLMs,
without requiring extensive external datasets. Specifically, Evontree extracts
domain ontology from raw models, detects inconsistencies using two core
ontology rules, and reinforces the refined knowledge via self-distilled
fine-tuning. Extensive experiments on medical QA benchmarks with
Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both
unmodified models and leading supervised baselines, achieving up to a 3.7%
improvement in accuracy. These results confirm the effectiveness, efficiency,
and robustness of our approach for low-resource domain adaptation of LLMs.

</details>


### [44] [Kimi Linear: An Expressive, Efficient Attention Architecture](https://arxiv.org/abs/2510.26692)
*Kimi Team,Yu Zhang,Zongyu Lin,Xingcheng Yao,Jiaxi Hu,Fanqing Meng,Chengyin Liu,Xin Men,Songlin Yang,Zhiyuan Li,Wentao Li,Enzhe Lu,Weizhou Liu,Yanru Chen,Weixin Xu,Longhui Yu,Yejie Wang,Yu Fan,Longguang Zhong,Enming Yuan,Dehao Zhang,Yizhi Zhang,T. Y. Liu,Haiming Wang,Shengjun Fang,Weiran He,Shaowei Liu,Yiwei Li,Jianlin Su,Jiezhong Qiu,Bo Pang,Junjie Yan,Zhejun Jiang,Weixiao Huang,Bohong Yin,Jiacheng You,Chu Wei,Zhengtao Wang,Chao Hong,Yutian Chen,Guanduo Chen,Yucheng Wang,Huabin Zheng,Feng Wang,Yibo Liu,Mengnan Dong,Zheng Zhang,Siyuan Pan,Wenhao Wu,Yuhao Wu,Longyu Guan,Jiawen Tao,Guohong Fu,Xinran Xu,Yuzhi Wang,Guokun Lai,Yuxin Wu,Xinyu Zhou,Zhilin Yang,Yulun Du*

Main category: cs.CL

TL;DR: Kimi Linear是一种混合线性注意力架构，首次在公平比较下超越全注意力模型，适用于短上下文、长上下文和强化学习任务。核心是Kimi Delta Attention（KDA），通过改进的Gated DeltaNet和块状算法实现高效计算。实验显示其在性能和解码吞吐量上显著优于全注意力模型，并开源了相关实现和模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种高效且高性能的线性注意力架构，以替代传统的全注意力模型，尤其在长上下文和强化学习任务中表现更优。

Method: 提出KDA模块，结合Gated DeltaNet和精细门控机制，采用块状算法和DPLR过渡矩阵优化计算效率。模型通过KDA和Multi-Head Latent Attention的混合层构建。

Result: Kimi Linear在3B激活参数的预训练模型上表现优异，降低KV缓存使用75%，解码吞吐量提升6倍，显著优于全注意力模型。

Conclusion: Kimi Linear是一种性能优越、高效的线性注意力架构，可作为全注意力模型的替代方案，尤其适合长输入输出任务。

Abstract: We introduce Kimi Linear, a hybrid linear attention architecture that, for
the first time, outperforms full attention under fair comparisons across
various scenarios -- including short-context, long-context, and reinforcement
learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an
expressive linear attention module that extends Gated DeltaNet with a
finer-grained gating mechanism, enabling more effective use of limited
finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware
efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)
transition matrices, which substantially reduces computation compared to the
general DPLR formulation while remaining more consistent with the classical
delta rule.
  We pretrain a Kimi Linear model with 3B activated parameters and 48B total
parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention
(MLA). Our experiments show that with an identical training recipe, Kimi Linear
outperforms full MLA with a sizeable margin across all evaluated tasks, while
reducing KV cache usage by up to 75% and achieving up to 6 times decoding
throughput for a 1M context. These results demonstrate that Kimi Linear can be
a drop-in replacement for full attention architectures with superior
performance and efficiency, including tasks with longer input and output
lengths.
  To support further research, we open-source the KDA kernel and vLLM
implementations, and release the pre-trained and instruction-tuned model
checkpoints.

</details>


### [45] [The End of Manual Decoding: Towards Truly End-to-End Language Models](https://arxiv.org/abs/2510.26697)
*Zhichao Wang,Dongyang Ma,Xinting Huang,Deng Cai,Tian Lan,Jiahao Xu,Haitao Mi,Xiaoying Tang,Yan Wang*

Main category: cs.CL

TL;DR: AutoDeco提出了一种新型架构，通过动态预测上下文相关的解码参数（如温度和top-p），实现了真正的端到端生成。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的解码过程依赖非可微调的超参数，需要手动调整。AutoDeco旨在通过学习控制解码策略来解决这一问题。

Method: 通过增强标准Transformer架构，添加轻量级头部，动态预测每个步骤的temperature和top-p值，实现参数化的token级解码。

Result: 在八个基准测试中，AutoDeco显著优于默认解码策略，性能接近oracle-tuned基线，并能根据自然语言指令调整解码策略。

Conclusion: AutoDeco不仅提升了解码性能，还开启了可指导和交互式LLM解码的新范式。

Abstract: The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a
non-differentiable decoding process that requires laborious, hand-tuning of
hyperparameters like temperature and top-p. This paper introduces AutoDeco, a
novel architecture that enables truly "end-to-end" generation by learning to
control its own decoding strategy. We augment the standard transformer with
lightweight heads that, at each step, dynamically predict context-specific
temperature and top-p values alongside the next-token logits. This approach
transforms decoding into a parametric, token-level process, allowing the model
to self-regulate its sampling strategy within a single forward pass.
  Through extensive experiments on eight benchmarks, we demonstrate that
AutoDeco not only significantly outperforms default decoding strategies but
also achieves performance comparable to an oracle-tuned baseline derived from
"hacking the test set"-a practical upper bound for any static method.
Crucially, we uncover an emergent capability for instruction-based decoding
control: the model learns to interpret natural language commands (e.g.,
"generate with low randomness") and adjusts its predicted temperature and top-p
on a token-by-token basis, opening a new paradigm for steerable and interactive
LLM decoding.

</details>


### [46] [Value Drifts: Tracing Value Alignment During LLM Post-Training](https://arxiv.org/abs/2510.26707)
*Mehar Bhatia,Shravan Nayak,Gaurav Kamath,Marius Mosbach,Karolina Stańczak,Vered Shwartz,Siva Reddy*

Main category: cs.CL

TL;DR: 研究了LLMs在训练过程中如何以及何时对齐人类价值观，重点关注后训练阶段的价值偏移。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在社会中日益重要，研究其与人类价值观的对齐变得至关重要。此前的研究多关注已训练好的模型，忽略了训练动态。

Method: 通过实验分析Llama-3和Qwen-3模型在不同后训练阶段的价值对齐表现，使用SFT和偏好优化数据集及算法。

Result: SFT阶段基本确定了模型的价值取向，后续偏好优化很难重新对齐；不同偏好优化算法会影响价值对齐结果。

Conclusion: 研究为后训练中的数据选择和算法优化提供了实用建议，以提高LLMs与人类价值观的对齐。

Abstract: As LLMs occupy an increasingly important role in society, they are more and
more confronted with questions that require them not only to draw on their
general knowledge but also to align with certain human value systems.
Therefore, studying the alignment of LLMs with human values has become a
crucial field of inquiry. Prior work, however, mostly focuses on evaluating the
alignment of fully trained models, overlooking the training dynamics by which
models learn to express human values. In this work, we investigate how and at
which stage value alignment arises during the course of a model's
post-training. Our analysis disentangles the effects of post-training
algorithms and datasets, measuring both the magnitude and time of value drifts
during training. Experimenting with Llama-3 and Qwen-3 models of different
sizes and popular supervised fine-tuning (SFT) and preference optimization
datasets and algorithms, we find that the SFT phase generally establishes a
model's values, and subsequent preference optimization rarely re-aligns these
values. Furthermore, using a synthetic preference dataset that enables
controlled manipulation of values, we find that different preference
optimization algorithms lead to different value alignment outcomes, even when
preference data is held constant. Our findings provide actionable insights into
how values are learned during post-training and help to inform data curation,
as well as the selection of models and algorithms for preference optimization
to improve model alignment to human values.

</details>


### [47] [AMO-Bench: Large Language Models Still Struggle in High School Math Competitions](https://arxiv.org/abs/2510.26768)
*Shengnan An,Xunliang Cai,Xuezhi Cao,Xiaoyu Li,Yehao Lin,Junlin Liu,Xinxuan Lv,Dan Ma,Xuanlin Wang,Ziwen Wang,Shuang Zhou*

Main category: cs.CL

TL;DR: AMO-Bench是一个高级数学推理基准，包含50个人工设计的问题，难度达到国际数学奥林匹克竞赛（IMO）水平或更高，用于评估大型语言模型（LLMs）的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的数学竞赛基准对评估顶级LLMs的效果逐渐减弱，因为模型在这些任务上的表现趋于饱和。AMO-Bench通过引入更具挑战性的原创问题来解决这一问题。

Method: AMO-Bench的问题均由专家交叉验证，确保达到IMO难度标准，且所有问题均为原创，避免数据记忆导致的性能泄漏。每个问题只需最终答案，支持自动评分。

Result: 在26个LLMs上的实验显示，即使表现最好的模型在AMO-Bench上的准确率也仅为52.4%，大多数模型低于40%。分析表明增加测试计算资源可以提升性能。

Conclusion: AMO-Bench揭示了当前LLMs在数学推理上的不足，并为未来的研究方向提供了基准。

Abstract: We present AMO-Bench, an Advanced Mathematical reasoning benchmark with
Olympiad level or even higher difficulty, comprising 50 human-crafted problems.
Existing benchmarks have widely leveraged high school math competitions for
evaluating mathematical reasoning capabilities of large language models (LLMs).
However, many existing math competitions are becoming less effective for
assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To
address this, AMO-Bench introduces more rigorous challenges by ensuring all 50
problems are (1) cross-validated by experts to meet at least the International
Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original
problems to prevent potential performance leakages from data memorization.
Moreover, each problem in AMO-Bench requires only a final answer rather than a
proof, enabling automatic and robust grading for evaluation. Experimental
results across 26 LLMs on AMO-Bench show that even the best-performing model
achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.
Beyond these poor performances, our further analysis reveals a promising
scaling trend with increasing test-time compute on AMO-Bench. These results
highlight the significant room for improving the mathematical reasoning in
current LLMs. We release AMO-Bench to facilitate further research into
advancing the reasoning abilities of language models.
https://amo-bench.github.io/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [48] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 该论文探索如何将SHAP（SHapley Additive exPlanations）应用于国际象棋分析，以揭示棋局中各个棋子的具体贡献，从而提高引擎评估的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前国际象棋引擎提供的评估通常是模糊的，无法直观展示棋局中各个棋子的具体贡献，作者希望通过SHAP方法解决这一问题。

Method: 通过将棋子视为特征并系统地移除它们，计算每个棋子的加法贡献，以解释引擎的输出。这一方法结合了传统国际象棋教学中的思路与现代可解释AI技术。

Result: 该方法能够以人类可理解的方式解释引擎的评估，并提供了新的可视化、训练和引擎比较可能性。

Conclusion: 论文提出的方法为可解释的国际象棋AI研究提供了新的方向，同时作者发布了相关代码和数据以促进未来研究。

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [49] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: 论文提出了一个两级框架（ITI和CEP），解释了压缩过程如何导致因果结构的发现，而非表面统计模式，并指出智能是结构化环境中生存压力的必然结果。


<details>
  <summary>Details</summary>
Motivation: 现有框架虽认为压缩对智能至关重要，但未明确解释为何压缩会促进因果结构而非表面模式的发现。本文旨在填补这一理论空白。

Method: 引入信息论驱动（ITI）和压缩效率原则（CEP）两级框架，结合信息论、物理和进化约束，阐明从生存压力到因果模型发现的机械性过程。

Result: 框架预测了压缩效率与泛化能力、因果模型与相关性模型的区别等，并统一解释了生物和人工系统中的智能现象。

Conclusion: 该框架为智能的认知和功能维度提供了统一解释，无需依赖意识或主观经验的假设。

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [50] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: 论文提出了一种通过聚合多角度评判的输出，以建模多样化的、基于角色的偏好框架，旨在解决LLM评判难以校准、易受评分标准影响及存在偏见的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM评判难以校准，存在评分标准敏感、偏见和不稳定性问题，而解决这一问题对推动可靠奖励模型和高效路由系统等关键应用至关重要。

Method: 论文提出了一个框架，通过学习聚合多个基于评分标准的评判输出来建模多样化、角色化的偏好，并对比了两种实现：广义加法模型(GAM)和多层感知机(MLP)。

Result: 该方法通过案例研究验证了其在对人类和LLM评判偏见的鲁棒性，并展示了优于基线模型的性能。

Conclusion: 通过角色化方法大规模合成偏好标签及两种聚合器的实现，论文为校准LLM评判提供了一种新思路。

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [51] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: SciTrust 2.0是一个评估大型语言模型（LLMs）在科学应用中可信度的框架，重点关注真实性、对抗鲁棒性、科学安全和科学伦理。通过对七种LLMs的评估，发现通用行业模型在各方面表现优于科学专用模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLMs在高风险科学应用中的可信度问题，研究开发了一个全面的评估框架，以提升AI系统的信任度。

Method: 采用多维度评估框架（真实性、对抗鲁棒性、科学安全和伦理），结合新的开放性真实性基准和伦理基准，并使用多种评估指标（如准确性和语义相似性）。

Result: 通用行业模型（如GPT-o4-mini）在各维度表现最佳，而科学专用模型在逻辑和伦理推理能力上存在显著缺陷，安全性评估中表现出高风险漏洞。

Conclusion: SciTrust 2.0为开发更可信的AI系统提供了基础，推动了科学背景下模型安全性和伦理的研究。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [52] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: Humans-Junior是一个3.8B规模的模型，其性能在FACTS Grounding公共子集上与GPT-4o相当（在±5%的等效范围内）。费用方面，Humans-Junior的云端API价格约为GPT-4o的1/19，且自托管或边缘部署可大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 提出一个小规模但高性价比的语言模型，能够在特定任务上与前沿大模型如GPT-4o性能相当，同时显著降低成本。

Method: 结合了"Exoskeleton Reasoning"支架和基于协议合规的行为微调，而非直接训练领域知识。这种方法显著提升了性能并降低了方差。

Result: 在Q1-Q500测试中，Humans-Junior得分为72.7%，与GPT-4o的73.5%差异仅为0.8%，证明其在±5%范围内等效。云端价格仅为GPT-4o的1/19。

Conclusion: 小模型通过优化方法可以在特定任务上媲美大模型性能，同时大幅降低成本，展示了高效AI的潜力。

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [53] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: 该论文提出了一种注意力感知的逆向规划方法，用于从行为中推断人类的认知偏见，并将其应用于实际驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 人类的认知偏见会影响其目标导向行为，与人类交互的自主系统需要理解这种偏见。

Method: 结合深度强化学习和计算认知建模，提出注意力感知的逆向规划方法。

Result: 该方法能够从行为中推断认知偏见，并在真实驾驶场景中验证了其可扩展性。

Conclusion: 注意力感知的逆向规划为理解人类认知偏见提供了一种有效工具，适用于实际交互场景。

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [54] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: 本文介绍了autosurvey2，一种通过检索增强合成和结构化评估自动生成学术综述的多阶段流程系统，实验证明其在结构和主题相关性上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 由于研究文献快速增长，尤其是在大语言模型领域，手动生成全面且最新的综述论文变得困难。

Method: autosurvey2采用多阶段流程，包括并行章节生成、迭代精炼和实时检索最新文献，结合多LLM评估框架确保质量和准确性。

Result: 实验表明，autosurvey2在结构连贯性和主题相关性上表现优于现有检索基数和自动化基线，同时保持较高的引用保真度。

Conclusion: 通过整合检索、推理和自动化评估，autosurvey2为生成长篇学术综述提供可扩展且可复现的解决方案，并为未来自动化学术写作研究奠定基础。

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [55] [Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization](https://arxiv.org/abs/2510.26023)
*Zhipeng Bao,Qianwen Li*

Main category: cs.AI

TL;DR: 论文提出了StuckSolver，一种基于大型语言模型的恢复框架，帮助自动驾驶车辆通过自主推理或乘客引导决策解决卡滞问题，无需修改内部架构。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在某些交通场景中容易卡滞，现有恢复方案如远程干预和手动接管不够高效或普及，亟需一种新的解决方案。

Method: StuckSolver作为插件模块，利用标准传感器数据流检测卡滞状态，结合环境上下文生成恢复指令，无需修改现有自动驾驶架构。

Result: 在Bench2Drive基准和自定义场景中，StuckSolver通过自主推理达到接近最优性能，结合乘客引导后表现进一步提升。

Conclusion: StuckSolver提供了一种高效且可扩展的解决方案，显著提升了自动驾驶车辆在复杂环境中的恢复能力。

Abstract: Despite significant advancements in recent decades, autonomous vehicles (AVs)
continue to face challenges in navigating certain traffic scenarios where human
drivers excel. In such situations, AVs often become immobilized, disrupting
overall traffic flow. Current recovery solutions, such as remote intervention
(which is costly and inefficient) and manual takeover (which excludes
non-drivers and limits AV accessibility), are inadequate. This paper introduces
StuckSolver, a novel Large Language Model (LLM) driven recovery framework that
enables AVs to resolve immobilization scenarios through self-reasoning and/or
passenger-guided decision-making. StuckSolver is designed as a plug-in add-on
module that operates on top of the AV's existing perception-planning-control
stack, requiring no modification to its internal architecture. Instead, it
interfaces with standard sensor data streams to detect immobilization states,
interpret environmental context, and generate high-level recovery commands that
can be executed by the AV's native planner. We evaluate StuckSolver on the
Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results
show that StuckSolver achieves near-state-of-the-art performance through
autonomous self-reasoning alone and exhibits further improvements when
passenger guidance is incorporated.

</details>


### [56] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: 探讨AI的问责性，确保其服务于消费者、选民和决策者的需求。


<details>
  <summary>Details</summary>
Motivation: 当前AI缺乏问责性，无法被质疑或讨论，影响其服务的公正性和透明度。

Method: 将一般问责性定义应用于AI，分析其问责与无问责的情形，并探索改进方法。

Result: 通过讨论和探索，提出提高AI问责性的可能途径。

Conclusion: 实现AI问责性是确保其公正服务的关键方向。

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [57] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: 论文介绍了Lean4PHYS框架，包含LeanPhysBench基准测试和PhysLib库，用于大学物理问题的形式化推理，并报告了模型在基准测试上的表现和PhysLib的效果。


<details>
  <summary>Details</summary>
Motivation: 为大学物理问题提供一个形式化推理的框架和基准测试，填补Lean4中物理领域基准测试的空白。

Method: 开发了LeanPhysBench基准测试和PhysLib库，并测试了多种数学证明工具和闭源模型的性能。

Result: 最佳模型DeepSeek-Prover-V2-7B和Claude-Sonnet-4的表现分别为16%和35%，PhysLib能平均提升模型性能11.75%。

Conclusion: LeanPhysBench具有挑战性，PhysLib有效提升模型性能，这是Lean4中首个物理领域的基准测试研究。

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [58] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 论文提出了一种量化框架，分析大语言模型（LLM）推理的经济性，揭示了成本递减、规模收益递减和最优成本效益区间，为模型部署和市场定价提供了经济依据。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型推理成本的经济性，以评估其商业可行性和广泛应用的潜力。

Method: 提出“推理经济学”框架，将LLM推理视为计算驱动的智能生产活动，分析边际成本、规模经济性和输出质量，基于WiNEval-3.0数据构建了“LLM推理生产边界”。

Result: 揭示了三大原则：边际成本递减、规模收益递减和最优成本效益区间。

Conclusion: 为模型部署决策提供了经济依据，并为未来AI推理资源的市场定价和优化奠定了实证基础。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [59] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种名为‘Reasoning Curriculum’的两阶段课程，先在数学领域通过RL训练增强推理能力，再通过混合领域RL进行迁移和巩固。该方法在多个领域和模型上表现一致提升。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在大型语言模型中的应用主要集中在数学和代码领域，缺乏对其他领域推理能力的探索。本文旨在通过一种简单通用的课程设计，提升模型在多领域的推理能力。

Method: 采用两阶段课程设计：1）在数学领域进行RL训练以开发推理技能；2）在混合领域进行联合RL以迁移和巩固这些技能。该方法无需特殊奖励模型，仅依赖标准的可验证性检查。

Result: 在Qwen3-4B和Llama-3.1-8B模型上，该方法在多领域测试中表现出一致的性能提升。分析表明，两阶段课程设计均必要，且数学优先的策略能增强解决复杂问题的认知行为。

Conclusion: Reasoning Curriculum为提升通用推理能力提供了一种简洁易用且无需额外工具的解决方案。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [60] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 论文提出了QASU基准，用于评估大语言模型在问卷数据分析中的表现，展示了如何通过选择合适的数据格式和提示策略提升准确性。


<details>
  <summary>Details</summary>
Motivation: 当前调查问卷数据的大规模处理和结构化分析对大语言模型提出了挑战，而现有的工具（如Qualtrics、SPSS等）缺乏与LLM的深度集成，导致缺乏优化建议。

Method: 引入QASU基准，测试大语言模型在六种结构化能力（如答案查找、受访者计数等）上的表现，对比六种数据序列化格式和多种提示策略的效果。

Result: 实验表明，选择合适的数据格式和提示策略可提升准确性多达8.8%，针对特定任务通过自增强提示可额外提升3-4%的准确性。

Conclusion: QASU基准为LLM在问卷分析中的研究和应用提供了一个简单而实用的基础，有助于推动该领域的进步。

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [61] [GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance](https://arxiv.org/abs/2510.26309)
*Jiseong Chung,Ronny Ko,Wonchul Yoo,Makoto Onizuka,Sungmok Kim,Tae-Wan Kim,Won-Yong Shin*

Main category: cs.AI

TL;DR: GraphCompliance框架通过将法规文本和运行时上下文分别表示为策略图和上下文图并进行对齐，提高了合规性评估的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: Web规模的合规性评估面临监管文本结构化与运行时上下文自然语言处理的挑战，需要一种方法将语义信息与法规的结构化元素对齐。

Method: 引入GraphCompliance框架，将法规文本编码为策略图，运行时上下文编码为上下文图，通过法官大语言模型（LLM）进行对齐推理。

Result: 在300个GDPR场景的实验中，GraphCompliance在多项评估任务中表现优于纯LLM和RAG基线，F1分数提高了4.1-7.2个百分点。

Conclusion: 结构化表示与法官LLM的结合是实现规范推理的互补方法，能有效减少合规性评估的工作负担。

Abstract: Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.

</details>


### [62] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 论文提出了一种名为IPA-UCT的新方法，通过弱化状态抽象条件来提高蒙特卡洛树搜索（MCTS）的样本效率，从而在多种测试领域和迭代预算下优于现有方法OGA-UCT及其衍生技术。


<details>
  <summary>Details</summary>
Motivation: 现有的状态-动作对抽象方法（如OGA-UCT）在噪声或大动作空间环境中难以找到状态抽象，限制了MCTS的性能提升。论文旨在通过放宽状态抽象条件来解决这一问题。

Method: 提出了一种称为IPA-UCT的技术，基于弱化的状态抽象条件（IPA框架），在保持较高精度的同时找到更多的抽象。同时，将IPA和ASAP统一为一个更通用的框架p-ASAP，进一步扩展为ASASAP框架。

Result: 实验验证表明，IPA-UCT在多种测试领域和迭代预算下显著优于OGA-UCT及其衍生方法。

Conclusion: IPA-UCT通过弱化状态抽象条件有效提升了MCTS的样本效率，同时揭示了一种更通用的抽象框架（p-ASAP和ASASAP）。

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [63] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: BOTS是一个基于贝叶斯推断的框架，用于在LLM强化微调中动态选择任务，通过结合显式和隐式证据提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统任务抽样方法效率低下，难以适应模型变化。BOTS旨在通过动态任务选择优化强化微调过程。

Method: BOTS使用贝叶斯推断维护任务难度的后验估计，结合显式（直接评估）和隐式（插值推断）证据，并通过Thompson采样平衡探索与利用。

Result: BOTS在多种领域和LLM规模下显著提升了数据效率和性能，优于基线方法。

Conclusion: BOTS为RFT中的动态任务选择提供了实用且可扩展的解决方案。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [64] [AI Mathematician as a Partner in Advancing Mathematical Discovery - A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 论文研究了AI Mathematician（AIM）系统作为数学研究合作伙伴的潜力，通过结合人类干预和机器计算，解决复杂数学问题。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在数学研究中的协作作用，而非仅仅是问题求解工具。

Method: 通过分解问题为可管理子目标、选择分析方法和验证结果，结合人类直觉与机器计算。

Result: 实现了协同推理，生成可靠、透明且可解释的证明。

Conclusion: 人类与AI的协同推理可以推动数学发现的前沿。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [65] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 论文提出了一种基于任务项内在性质的数据子集选择方法Scales++，以替代依赖模型失败模式的现有方法，显著降低了成本并保持了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 目前的大语言模型评估成本高昂，现有方法依赖模型失败模式，存在高成本、冷启动问题和未来模型失败模式假设的局限性。

Method: 提出了一种任务项中心的高效基准测试方法Scales++，根据任务项的认知需求选择数据子集。

Result: Scales++将初始选择成本降低了18倍以上，预测精度接近全数据集，在Open LLM Leaderboard上仅用0.5%数据子集即可达到2.9%平均绝对误差。

Conclusion: 任务项中心方法Scales++在不显著损失准确性的前提下，实现了更高效的模型评估，具备更好的冷启动性能和可解释性。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [66] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: Autograder+是一种新型自动评分系统，通过AI驱动的反馈和可视化功能，将传统评分转变为形成性学习体验。


<details>
  <summary>Details</summary>
Motivation: 编程教育的快速发展超出传统评估工具的能力，导致教师难以提供有意义的反馈。Autograder+旨在解决这一问题。

Method: 系统结合了大语言模型的自动反馈生成和学生代码可视化功能，并通过专家反馈进行微调。

Result: 在600份学生作业中，系统生成的反馈与教师评论高度一致，可视化功能也能有效分组解决方案。

Conclusion: Autograder+显著减轻教师负担，支持针对性教学并提升学习效果。

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [67] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: 论文提出了一种医学稀疏自动编码器（MedSAE），用于提高医学视觉模型中表征的可解释性，并通过新评估框架验证其效果。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的人工智能需要兼具准确性和可解释性的模型，当前缺乏针对医学视觉的机制解释性方法。

Method: 在MedCLIP的潜在空间中应用MedSAE，并结合相关性指标、熵分析和MedGEMMA自动命名神经元进行评估。

Result: 在CheXpert数据集上，MedSAE神经元比原始MedCLIP特征具有更高的单义性和可解释性。

Conclusion: 该方法为高性能医学AI与透明度搭建了桥梁，提供了可扩展的临床可靠表征方案。

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [68] [LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks](https://arxiv.org/abs/2510.26486)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human smuggling networks are complex and constantly evolving, making them
difficult to analyze comprehensively. Legal case documents offer rich factual
and procedural insights into these networks but are often long, unstructured,
and filled with ambiguous or shifting references, posing significant challenges
for automated knowledge graph (KG) construction. Existing methods either
overlook coreference resolution or fail to scale beyond short text spans,
leading to fragmented graphs and inconsistent entity linking. We propose
LINK-KG, a modular framework that integrates a three-stage, LLM-guided
coreference resolution pipeline with downstream KG extraction. At the core of
our approach is a type-specific Prompt Cache, which consistently tracks and
resolves references across document chunks, enabling clean and disambiguated
narratives for structured knowledge graph construction from both short and long
legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes
by 32.22% compared to baseline methods, resulting in cleaner and more coherent
graph structures. These improvements establish LINK-KG as a strong foundation
for analyzing complex criminal networks.

</details>


### [69] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 论文探讨了上下文工程的起源、定义、历史发展阶段及其在人工智能中的未来潜力。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，机器需要更好地理解和适应人类的情境和目的，因此提出上下文工程的概念。

Method: 通过对上下文工程的历史演变进行梳理，提出系统定义，并分析关键设计考虑。

Result: 论文为上下文工程提供了概念基础，并展望了其在未来AI系统中的重要性。

Conclusion: 上下文工程是AI系统发展的关键方向，需要社区共同努力推进。

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [70] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: AI与人类反馈结合提升AI输出的真实性验证质量，AI辅助需谨慎以避免过度依赖。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力提升和任务复杂度增加，确保AI输出的质量和安全性变得更具挑战性，特别是真实性验证。本文探讨如何利用AI提升人类监督质量。

Method: 通过结合AI评分和人类评分（基于AI评分者的置信度），研究AI辅助对人类验证准确性的影响。对比不同类型的AI辅助（如展示解释、信心、标签或仅展示搜索结果和证据）。

Result: 结合AI和人类评分优于单独依赖任一方；AI辅助（如提供搜索证据）能提升人类验证准确性，但展示过多AI信息（如解释、标签）会导致过度依赖。

Conclusion: 在‘放大监督’（结合人类和AI监督超越人类能力的AI系统）中，AI辅助设计需平衡信息透明度和避免过度依赖。

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [71] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: 该论文系统评估了大语言模型（LLMs）在规范性推理任务中的能力，揭示了其在特定类型规范性推理中的不一致性以及类似人类认知偏见的特征。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多种推理任务中表现优异，但其处理规范性推理的能力尚未被充分研究。论文旨在填补这一空白，并通过逻辑和模态视角评估LLMs的规范性推理能力。

Method: 作者引入了一个新数据集，覆盖规范性和认知性领域的广泛形式推理模式，并通过比较LLMs在这两类模态上的推理表现，结合非形式认知因素的影响进行分析。

Result: 结果显示，LLMs总体上遵循有效的推理模式，但在某些规范性推理类型中表现不一致，并显示出类似人类认知偏见的特征。

Conclusion: 这些发现表明，在LLMs中实现逻辑一致性仍面临挑战，并为提升其可靠性提供了方向。研究数据和代码已公开。

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [72] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: 研究发现多模态大语言模型（MLLMs）在处理视觉-语言数据时偏好文本输入，原因是视觉键向量与文本键空间分布不一致，导致视觉信息被低估。通过分析LLaVA和Qwen2.5-VL的键向量分布，证实了这一点。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型在处理视觉-语言数据时偏好文本输入的内在原因，而非仅归因于外部数据因素。

Method: 提取LLaVA和Qwen2.5-VL的键向量，使用t-SNE和Jensen-Shannon散度方法分析其分布结构。

Result: 视觉和文本键向量在注意力空间中占据明显不同的子空间，跨模态差异显著大于模态内差异。

Conclusion: 文本偏好源于注意力键空间内的内在不对齐，而非仅是外部数据因素的影响。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [73] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: 该论文对不同计算平台上的基础模型推理能力进行全面评估，提供了一个基础设施无关的基准测试，并挑战了传统的规模扩展假设。


<details>
  <summary>Details</summary>
Motivation: 评估当代基础模型在不同计算平台（高性能计算、云平台和大学集群）上的推理能力，以指导模型在教育、生产及研究中的应用。

Method: 通过三个阶段实验：（1）基线建立，（2）基础设施验证，（3）扩展评估，使用79个问题对15个基础模型进行评估。

Result: 研究发现训练数据质量比模型规模更重要，并验证了基础设施无关的基准的可重复性。

Conclusion: 论文提出了三基础设施方法和79问题基准，可用于长期跟踪基础模型的推理能力发展。

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [74] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）中列表处理任务的机制，发现模型通过少数注意力头（称为“过滤头”）编码了一种通用的过滤操作表示，类似于函数式编程中的“filter”功能。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何实现列表处理任务，特别是理解模型是否能够学习并泛化抽象的过滤操作。

Method: 使用因果中介分析（causal mediation analysis）对多样化的列表处理任务进行研究，识别出编码过滤谓词的注意力头（filter heads）。

Result: 发现这些过滤头的查询状态可以编码通用的过滤谓词表示，能跨不同格式、语言或任务应用。同时也观察到模型可能使用另一种策略（直接标记满足条件的项）进行过滤。

Conclusion: LLMs能够发展出可解释的抽象计算操作实现，其泛化方式与传统函数式编程策略惊人地相似。

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>
