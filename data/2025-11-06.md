<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.AI](#cs.AI) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation](https://arxiv.org/abs/2511.03001)
*Gyeom Hwangbo,Hyungjoo Chae,Minseok Kang,Hyeonjong Ju,Soohyun Oh,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 论文提出LEGO-Eval框架和LEGO-Bench基准，用于评估和生成更符合真实世界细节的3D场景。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型生成的3D场景在空间布局和对象属性上缺乏真实感，导致训练具身智能体时学习到的先验与现实世界不符，需要更精细的指令和评估方法。

Method: 作者提出LEGO-Eval框架，通过多种工具显式地场景组分，并创建LEGO-Bench基准测试详细指令的真实性。

Result: 实验显示，LEGO-Eval在评估场景与指令对齐时比现有方法高0.41 F1分数，且现有生成方法在完全对齐指令上的成功率不超过10%。

Conclusion: LEGO-Eval和LEGO-Bench显著提升了3D场景生成与评估的真实性和准确性，凸显当前方法的局限性。

Abstract: Despite recent progress in using Large Language Models (LLMs) for
automatically generating 3D scenes, generated scenes often lack realistic
spatial layouts and object attributes found in real-world environments. As this
problem stems from insufficiently detailed, coarse-grained instructions,
advancing 3D scene synthesis guided by more detailed, fine-grained instructions
that reflect real-world environments becomes crucial. Without such realistic
scenes, training embodied agents in unrealistic environments can lead them to
learn priors that diverge significantly from real-world physics and semantics,
degrading their performance when deployed. Thus, verifying the alignment
between the fine-grained instruction and the generated scene is essential for
effective learning. However, current evaluation methods, such as CLIPScore and
vision-language models (VLMs), often fail to reliably assess such alignment.
This shortcoming arises primarily from their shallow understanding of 3D
scenes, which often leads to improperly grounded scene components. To address
this, we introduce LEGO-Eval, an evaluation framework equipped with diverse
tools designed to explicitly ground scene components, enabling more accurate
alignment assessments. We also present LEGO-Bench, a benchmark of detailed
instructions that specify complex layouts and attributes of real-world
environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge
by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with
LEGO-Bench reveals significant limitations in current generation methods.
Across all evaluated approaches, success rates reached at most 10% in
generating scenes that fully align with fine-grained instructions.

</details>


### [2] [Targeted Error Correction in Knowledge Distillation: Small Language Models Surpass GPT](https://arxiv.org/abs/2511.03005)
*Hee-Jin Lee,Zhen Guo,Luchao Jin,Morteza Moazami Goudarzi*

Main category: cs.CL

TL;DR: ARF pipeline帮助小型开源语言模型在客户服务摘要任务中超越大型专有模型，通过分析、修订和微调三个步骤提升性能。


<details>
  <summary>Details</summary>
Motivation: 目的是降低成本、提高数据隐私，同时保持竞争力的准确性，推动开源LLM在不同下游应用中的发展。

Method: 分为三步：分析GPT-3.5的错误，用Llama 3.1 70B修订生成高质量数据，微调Llama 3.1 8B。

Result: ARF管道使小型LLM的摘要性能优于GPT-3.5，同时提升效率和隐私。

Conclusion: ARF框架具有可推广性，适用于增强开源LLM在多样化任务中的表现。

Abstract: We introduce an Analyze-Revise-Finetune (ARF) pipeline that enables smaller
open-source language models (LLMs) to surpass substantially larger proprietary
models in customer service summarization tasks. The pipeline first analyzes and
categorizes common errors in summaries produced by a teacher model (GPT-3.5),
then performs a targeted revision using a compact editor model (Llama 3.1 70B)
to generate high-quality, refined training data. Fine-tuning a smaller student
model (Llama 3.1 8B) on this refined data resulted in superior summarization
performance compared to GPT-3.5. The ARF pipeline improves cost efficiency and
data privacy while maintaining competitive accuracy, illustrating a
generalizable framework for enhancing open-source LLMs across diverse
downstream applications.

</details>


### [3] [Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034)
*Yan Cathy Hua,Paul Denny,Jörg Wicker,Katerina Taškova*

Main category: cs.CL

TL;DR: 该论文提出了针对资源匮乏领域的ABSA研究，包括新的评估方法FTS-OBP、小型生成模型的性能优化及教育领域的数据集发布。


<details>
  <summary>Details</summary>
Motivation: ABSA研究在教育和医疗等高需求但资源匮乏领域进展缓慢，传统评估方法的局限性也阻碍了研究进展。

Method: 提出FTS-OBP评估方法，研究小型生成语言模型，并探索无数据和轻数据微调方法。

Result: 小型生成模型优于大型专有模型，性能提升显著，且FTS-OBP与传统指标强相关。

Conclusion: 通过创新评估方法和模型优化，为资源匮乏领域ABSA研究提供了新方向和支持。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining
approach that identifies and classifies opinions associated with specific
entities (aspects) or their categories within a sentence. Despite its rapid
growth and broad potential, ABSA research and resources remain concentrated in
commercial domains, leaving analytical needs unmet in high-demand yet
low-resource areas such as education and healthcare. Domain adaptation
challenges and most existing methods' reliance on resource-intensive
in-training knowledge injection further hinder progress in these areas.
Moreover, traditional evaluation methods based on exact matches are overly
rigid for ABSA tasks, penalising any boundary variations which may misrepresent
the performance of generative models. This work addresses these gaps through
three contributions: 1) We propose a novel evaluation method, Flexible Text
Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates
realistic extraction boundary variations while maintaining strong correlation
with traditional metrics and offering fine-grained diagnostics. 2) We present
the first ABSA study of small decoder-only generative language models (SLMs;
<7B parameters), examining resource lower bounds via a case study in education
review ABSA. We systematically explore data-free (in-context learning and
weight merging) and data-light fine-tuning methods, and propose a multitask
fine-tuning strategy that significantly enhances SLM performance, enabling
1.5-3.8 B models to surpass proprietary large models and approach benchmark
results with only 200-1,000 examples on a single GPU. 3) We release the first
public set of education review ABSA resources to support future research in
low-resource domains.

</details>


### [4] [Reading Between the Lines: The One-Sided Conversation Problem](https://arxiv.org/abs/2511.03056)
*Victoria Ebert,Rishabh Singh,Tuochao Chen,Noah A. Smith,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 论文研究了单边对话问题（1SC），提出两种任务：实时重建缺失对话和从单边文本生成摘要，并通过实验验证了未来对话信息和提示方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中（如远程医疗、呼叫中心）只能记录单边对话，亟需解决从单边对话中推断和学习的问题。

Method: 研究了两种任务：重建缺失对话和生成单边文本摘要，采用了提示和微调模型，并进行了人类A/B测试和LLM评估。

Result: 实验表明，访问未来对话信息和提示方法能有效提升重建质量，且无需重建即可生成高质量摘要。

Conclusion: 1SC是一个新兴挑战，研究结果标志着隐私友好型对话AI的进步。

Abstract: Conversational AI is constrained in many real-world settings where only one
side of a dialogue can be recorded, such as telemedicine, call centers, and
smart glasses. We formalize this as the one-sided conversation problem (1SC):
inferring and learning from one side of a conversation. We study two tasks: (1)
reconstructing the missing speaker's turns for real-time use cases, and (2)
generating summaries from one-sided transcripts. Evaluating prompting and
finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B
testing and LLM-as-a-judge metrics, we find that access to one future turn and
information about utterance length improves reconstruction, placeholder
prompting helps to mitigate hallucination, and while large models generate
promising reconstructions with prompting, smaller models require finetuning.
Further, high-quality summaries can be generated without reconstructing missing
turns. We present 1SC as a novel challenge and report promising results that
mark a step toward privacy-aware conversational AI.

</details>


### [5] [PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech](https://arxiv.org/abs/2511.03080)
*Michel Wong,Ali Alshehri,Sophia Kao,Haotian He*

Main category: cs.CL

TL;DR: PolyNorm是一种基于提示的新方法，利用大语言模型进行文本规范化，减少手工规则依赖，支持多语言扩展。实验证明其在8种语言中降低词错误率。


<details>
  <summary>Details</summary>
Motivation: 传统文本规范化系统虽准确但工程量大，难以扩展且对低资源语言支持不足。PolyNorm旨在通过LLMs减少手工规则依赖，提高多语言适用性。

Method: 提出基于提示的PolyNorm方法，结合语言无关的自动数据生成与评估流程，支持多语言实验。

Result: 在8种语言中，PolyNorm相比生产级系统持续降低词错误率（WER）。

Conclusion: PolyNorm展示了利用LLMs实现高效、可扩展文本规范化的潜力，并发布了多语言数据集PolyNorm-Benchmark支持后续研究。

Abstract: Text Normalization (TN) is a key preprocessing step in Text-to-Speech (TTS)
systems, converting written forms into their canonical spoken equivalents.
Traditional TN systems can exhibit high accuracy, but involve substantial
engineering effort, are difficult to scale, and pose challenges to language
coverage, particularly in low-resource settings. We propose PolyNorm, a
prompt-based approach to TN using Large Language Models (LLMs), aiming to
reduce the reliance on manually crafted rules and enable broader linguistic
applicability with minimal human intervention. Additionally, we present a
language-agnostic pipeline for automatic data curation and evaluation, designed
to facilitate scalable experimentation across diverse languages. Experiments
across eight languages show consistent reductions in the word error rate (WER)
compared to a production-grade-based system. To support further research, we
release PolyNorm-Benchmark, a multilingual data set covering a diverse range of
text normalization phenomena.

</details>


### [6] [A Computational Approach to Analyzing Disrupted Language in Schizophrenia: Integrating Surprisal and Coherence Measures](https://arxiv.org/abs/2511.03089)
*Gowtham Premananth,Carol Espy-Wilson*

Main category: cs.CL

TL;DR: 该研究探讨了精神分裂症患者的语言障碍，通过计算语言学中的惊讶度和语义连贯性来分析其与健康对照组的差异，并研究了这些指标如何随症状严重程度变化。


<details>
  <summary>Details</summary>
Motivation: 语言障碍是精神分裂症的显著特征之一，表现为言语混乱和语篇连贯性受损。这些异常可能成为症状严重程度和诊断的客观标记。

Method: 采用计算语言学方法，测量惊讶度和语义连贯性，比较精神分裂症患者与健康对照组的差异，并分析其与症状严重程度的关系。

Result: 研究发现精神分裂症患者的语言表现在惊讶度和语义连贯性上与健康对照组存在显著差异，且这些差异与症状严重程度相关。

Conclusion: 惊讶度和语义连贯性可以作为精神分裂症语言障碍的潜在标志物，为症状评估和诊断提供新的视角。

Abstract: Language disruptions are one of the well-known effects of schizophrenia
symptoms. They are often manifested as disorganized speech and impaired
discourse coherence. These abnormalities in spontaneous language production
reflect underlying cognitive disturbances and have the potential to serve as
objective markers for symptom severity and diagnosis of schizophrenia. This
study focuses on how these language disruptions can be characterized in terms
of two computational linguistic measures: surprisal and semantic coherence. By
computing surprisal and semantic coherence of language using computational
models, this study investigates how they differ between subjects with
schizophrenia and healthy controls. Furthermore, this study provides further
insight into how language disruptions in terms of these linguistic measures
change with varying degrees of schizophrenia symptom severity.

</details>


### [7] [CARMA: Comprehensive Automatically-annotated Reddit Mental Health Dataset for Arabic](https://arxiv.org/abs/2511.03102)
*Saad Mankarious,Ayah Zirikly*

Main category: cs.CL

TL;DR: CARMA是首个自动标注的大规模阿拉伯语Reddit帖子数据集，涵盖六种心理健康状况，为资源有限的文化中早期心理健康检测提供支持。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语人群的心理健康检测面临资源有限和文化污名化的挑战，现有研究主要集中在英语，阿拉伯语研究不足，缺乏标注数据集。

Method: 通过创建CARMA数据集，进行定性和定量分析，研究用户间的词汇和语义差异，并使用多种模型进行分类实验。

Result: CARMA在规模和多样性上超越现有资源，分类实验结果表明其在推进阿拉伯语心理健康检测方面的潜力。

Conclusion: CARMA为阿拉伯语心理健康检测提供了重要资源，有望推动该领域的进一步研究。

Abstract: Mental health disorders affect millions worldwide, yet early detection
remains a major challenge, particularly for Arabic-speaking populations where
resources are limited and mental health discourse is often discouraged due to
cultural stigma. While substantial research has focused on English-language
mental health detection, Arabic remains significantly underexplored, partly due
to the scarcity of annotated datasets. We present CARMA, the first
automatically annotated large-scale dataset of Arabic Reddit posts. The dataset
encompasses six mental health conditions, such as Anxiety, Autism, and
Depression, and a control group. CARMA surpasses existing resources in both
scale and diversity. We conduct qualitative and quantitative analyses of
lexical and semantic differences between users, providing insights into the
linguistic markers of specific mental health conditions. To demonstrate the
dataset's potential for further mental health analysis, we perform
classification experiments using a range of models, from shallow classifiers to
large language models. Our results highlight the promise of advancing mental
health detection in underrepresented languages such as Arabic.

</details>


### [8] [Control Barrier Function for Aligning Large Language Models](https://arxiv.org/abs/2511.03121)
*Yuya Miyaoka,Masaki Inoue*

Main category: cs.CL

TL;DR: 提出了一种基于控制屏障函数（CBF）的框架，用于对齐大型语言模型（LLMs），确保生成用户期望的文本，无需微调基线模型。


<details>
  <summary>Details</summary>
Motivation: 为了确保大型语言模型生成的文本符合用户期望并具有安全性，提出了一种无需微调基线模型的对齐方法。

Method: 利用控制屏障函数（CBF）作为安全过滤器，对基线LLM生成的预测标记进行干预，确保文本生成符合要求。

Result: 系统采用开源语言模型实现，能够生成积极的文本，且安全过滤器设计灵活，可直接结合评估模型。

Conclusion: 该控制框架为LLMs的对齐提供了一种高效且灵活的方法，适用于多种场景。

Abstract: This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the CBF safety
filter to the predicted token generated from the baseline LLM, to intervene in
the generated text. The safety filter includes two significant advantages: this
safety filter is an add-on type, allowing it to be used for alignment purposes
without fine-tuning the baseline LLM, and if there is an evaluation model
regarding the desired alignment, it can be directly applied to the filter
design. The overall text-generation system is implemented with open-source
language models, aiming to generate positive text.

</details>


### [9] [MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity](https://arxiv.org/abs/2511.03146)
*Kaiyuan Zhang,Chenghao Yang,Zhoufutu Wen,Sihang Yuan,Qiuyue Wang,Chaoyi Huang,Guosheng Zhu,He Wang,Huawenyu Lu,Jianing Wen,Jianpeng Jiao,Lishu Luo,Longxiang Liu,Sijin Wu,Xiaolei Zhu,Xuanliang Zhang,Ge Zhang,Yi Lin,Guang Shi,Chaoyou Fu,Wenhao Huang*

Main category: cs.CL

TL;DR: 论文介绍了MME-CC多模态评估基准，用于系统性评估多模态大语言模型在视觉认知任务中的表现，发现闭源模型表现较好，但在空间和几何推理上仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准对文本推理过度强调或未能系统捕捉视觉认知行为，导致多模态大语言模型的认知能力评估不足。

Method: 提出MME-CC基准，将11个代表性推理任务分为空间、几何和知识推理三类，对16种模型进行实验分析。

Result: 闭源模型表现优于开源模型（如Gemini-2.5-Pro得分42.66），但空间和几何推理能力普遍较弱（≤30%），常见错误包括方向判断和跨视野身份持续性问题。

Conclusion: MME-CC揭示了多模态模型在视觉认知任务中的局限，建议未来研究和设计将认知能力作为核心评估指标。

Abstract: As reasoning models scale rapidly, the essential role of multimodality in
human cognition has come into sharp relief, driving a growing need to probe
vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either
overemphasize textual reasoning or fall short of systematically capturing
vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs
insufficiently assessed. To address this limitation, we introduce MME-CC
(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded
benchmark that organizes 11 representative reasoning tasks into three
fundamental categories of visual information: spatial, geometric, and
knowledge-based reasoning, and provides fine-grained analyses of MLLMs'
cognitive capacity across these dimensions. Based on MME-CC, we conduct
extensive experiments over 16 representative MLLMs. Our study reveals that
closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.
30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak
(less than or equal to 30%). We further identify common error patterns,
including orientation mistakes, fragile cross-view identity persistence, and
poor adherence to counterfactual instructions, and observe that
Chain-of-Thought typically follows a three-stage process (extract -> reason ->
verify) with heavy reliance on visual extraction. We hope this work catalyzes a
shift toward treating the cognitive capacity of MLLMs as central to both
evaluation and model design.

</details>


### [10] [Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment](https://arxiv.org/abs/2511.03152)
*Srishti Yadav,Jasmina Gajcin,Erik Miehling,Elizabeth Daly*

Main category: cs.CL

TL;DR: 论文提出了一个基于LLM的利益相关者风险评估框架，通过Risk Atlas Nexus和GloVE方法生成可解释的策略，展示不同利益相关者对风险的分歧，并在医疗AI、自动驾驶和欺诈检测三个领域验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了确保AI系统的负责任部署，理解不同利益相关者对风险的感知是关键。

Method: 使用LLM作为评估者，结合Risk Atlas Nexus和GloVE解释方法，生成利益相关者特定的可解释策略。

Result: 研究结果表明，利益相关者视角显著影响风险感知和冲突模式。

Conclusion: 利益相关者感知的解释对于提高LLM评估的透明度和可解释性至关重要，符合以人为中心的AI治理目标。

Abstract: Understanding how different stakeholders perceive risks in AI systems is
essential for their responsible deployment. This paper presents a framework for
stakeholder-grounded risk assessment by using LLMs, acting as judges to predict
and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our
framework generates stakeholder-specific, interpretable policies that shows how
different stakeholders agree or disagree about the same risks. We demonstrate
our method using three real-world AI use cases of medical AI, autonomous
vehicles, and fraud detection domain. We further propose an interactive
visualization that reveals how and why conflicts emerge across stakeholder
perspectives, enhancing transparency in conflict reasoning. Our results show
that stakeholder perspectives significantly influence risk perception and
conflict patterns. Our work emphasizes the importance of these
stakeholder-aware explanations needed to make LLM-based evaluations more
transparent, interpretable, and aligned with human-centered AI governance
goals.

</details>


### [11] [Measuring Aleatoric and Epistemic Uncertainty in LLMs: Empirical Evaluation on ID and OOD QA Tasks](https://arxiv.org/abs/2511.03166)
*Kevin Wang,Subre Abdoul Moktar,Jia Li,Kangshuo Li,Feng Chen*

Main category: cs.CL

TL;DR: 总结


<details>
  <summary>Details</summary>
Motivation: 验证不同不确定性估计方法在大语言模型（LLM）中的鲁棒性和有效性。

Method: 使用12种不确定性估计方法和4种生成质量指标（包括LLMScore），在问答任务中对分布内和分布外数据集进行评估。

Result: 信息基方法在分布内设置中表现优异；密度基方法和P(True)指标在分布外情境中效果更好；语义一致性方法在不同数据集和指标中表现可靠但非最优。

Conclusion: 不确定性估计方法的选择应根据具体任务和数据分布情况。

Abstract: Large Language Models (LLMs) have become increasingly pervasive, finding
applications across many industries and disciplines. Ensuring the
trustworthiness of LLM outputs is paramount, where Uncertainty Estimation (UE)
plays a key role. In this work, a comprehensive empirical study is conducted to
examine the robustness and effectiveness of diverse UE measures regarding
aleatoric and epistemic uncertainty in LLMs. It involves twelve different UE
methods and four generation quality metrics including LLMScore from LLM
criticizers to evaluate the uncertainty of LLM-generated answers in
Question-Answering (QA) tasks on both in-distribution (ID) and
out-of-distribution (OOD) datasets. Our analysis reveals that information-based
methods, which leverage token and sequence probabilities, perform exceptionally
well in ID settings due to their alignment with the model's understanding of
the data. Conversely, density-based methods and the P(True) metric exhibit
superior performance in OOD contexts, highlighting their effectiveness in
capturing the model's epistemic uncertainty. Semantic consistency methods,
which assess variability in generated answers, show reliable performance across
different datasets and generation metrics. These methods generally perform well
but may not be optimal for every situation.

</details>


### [12] [BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture](https://arxiv.org/abs/2511.03180)
*Shahriyar Zaman Ridoy,Azmine Toushik Wasi,Koushik Ahamed Tonmoy*

Main category: cs.CL

TL;DR: 论文介绍了BengaliMoralBench，首个针对孟加拉语和社会文化背景的大规模道德基准，用于评估多语言大模型在本地伦理规范上的对齐情况。


<details>
  <summary>Details</summary>
Motivation: 当前的多语言大模型在孟加拉语等非英语语言中的伦理评估缺乏文化敏感性，现有基准主要基于西方框架，无法满足本地化需求。

Method: 通过五个道德领域（日常活动、习惯、育儿、家庭关系和宗教活动）和50个文化相关子主题构建BengaliMoralBench，采用三种伦理视角（美德、常识和正义）对多语言大模型进行零样本评估。

Result: 评估结果显示模型准确率差异较大（50-91%），在文化基础、常识推理和道德公平性上存在明显弱点。

Conclusion: BengaliMoralBench为多语言AI的本地化提供了一种文化对齐的评估方法，支持在低资源多语言环境中部署伦理稳健的AI系统。

Abstract: As multilingual Large Language Models (LLMs) gain traction across South Asia,
their alignment with local ethical norms, particularly for Bengali, which is
spoken by over 285 million people and ranked 6th globally, remains
underexplored. Existing ethics benchmarks are largely English-centric and
shaped by Western frameworks, overlooking cultural nuances critical for
real-world deployment. To address this, we introduce BengaliMoralBench, the
first large-scale ethics benchmark for the Bengali language and socio-cultural
contexts. It covers five moral domains, Daily Activities, Habits, Parenting,
Family Relationships, and Religious Activities, subdivided into 50 culturally
relevant subtopics. Each scenario is annotated via native-speaker consensus
using three ethical lenses: Virtue, Commonsense, and Justice ethics. We conduct
systematic zero-shot evaluation of prominent multilingual LLMs, including
Llama, Gemma, Qwen, and DeepSeek, using a unified prompting protocol and
standard metrics. Performance varies widely (50-91% accuracy), with qualitative
analysis revealing consistent weaknesses in cultural grounding, commonsense
reasoning, and moral fairness. BengaliMoralBench provides a foundation for
responsible localization, enabling culturally aligned evaluation and supporting
the deployment of ethically robust AI in diverse, low-resource multilingual
settings such as Bangladesh.

</details>


### [13] [LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval](https://arxiv.org/abs/2511.03214)
*Wenchang Lei,Ping Zou,Yue Wang,Feng Sun,Lei Zhao*

Main category: cs.CL

TL;DR: 论文提出了语言图模型（LGM），通过提取自然语言中的元关系来增强概念清晰度，并结合反思机制验证这些关系，从而提升大语言模型的理解和响应能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在用户指令涉及模糊或概念不对齐的术语时表现不佳，因此需要一种方法增强其概念理解能力。

Method: LGM通过提取继承、别名和组合等元关系，并结合反思机制验证这些关系。它还利用概念迭代检索算法动态为大语言模型提供关系和描述。

Result: 实验表明，LGM在标准基准测试中持续优于现有的RAG基线方法，且无需截断文本。

Conclusion: LGM通过动态提供元关系和验证机制，显著提升了大语言模型的概念理解和响应能力。

Abstract: Large language models (LLMs) exhibit strong semantic understanding, yet
struggle when user instructions involve ambiguous or conceptually misaligned
terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity
by extracting meta-relations-inheritance, alias, and composition-from natural
language. The model further employs a reflection mechanism to validate these
meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these
relations and related descriptions are dynamically supplied to the LLM,
improving its ability to interpret concepts and generate accurate responses.
Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely
on extended context windows, our method enables large language models to
process texts of any length without the need for truncation. Experiments on
standard benchmarks demonstrate that the LGM consistently outperforms existing
RAG baselines.

</details>


### [14] [Beyond Ranked Lists: The SARAL Framework for Cross-Lingual Document Set Retrieval](https://arxiv.org/abs/2511.03228)
*Shantanu Agarwal,Joel Barry,Elizabeth Boschee,Scott Miller*

Main category: cs.CL

TL;DR: 本文介绍了SARAL团队在MATERIAL计划中的工作，提出了一种新颖的跨语言信息检索方法，强调检索查询相关的文档集而非仅是排名文档列表。


<details>
  <summary>Details</summary>
Motivation: MATERIAL计划旨在推进跨语言信息检索（CLIR）的技术水平，SARAL团队希望通过创新方法提升CLIR的效率和效果。

Method: SARAL团队开发了一种新颖的跨语言信息检索方法，专注于检索查询相关的文档集，而不仅仅是生成排名的文档列表。

Result: 在MATERIAL计划的第三阶段评估中，SARAL团队在三种不同语言（波斯语、哈萨克语和格鲁吉亚语）的六种评估条件下，其中有五种超越了其他团队的表现。

Conclusion: SARAL团队的方法在跨语言信息检索任务中表现出色，特别是在处理多样语言和复杂查询时具有显著优势。

Abstract: Machine Translation for English Retrieval of Information in Any Language
(MATERIAL) is an IARPA initiative targeted to advance the state of
cross-lingual information retrieval (CLIR). This report provides a detailed
description of Information Sciences Institute's (ISI's) Summarization and
domain-Adaptive Retrieval Across Language's (SARAL's) effort for MATERIAL.
Specifically, we outline our team's novel approach to handle CLIR with emphasis
in developing an approach amenable to retrieve a query-relevant document
\textit{set}, and not just a ranked document-list. In MATERIAL's Phase-3
evaluations, SARAL exceeded the performance of other teams in five out of six
evaluation conditions spanning three different languages (Farsi, Kazakh, and
Georgian).

</details>


### [15] [IndicSuperTokenizer: An Optimized Tokenizer for Indic Multilingual LLMs](https://arxiv.org/abs/2511.03237)
*Souvik Rana,Arul Menezes,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: IndicSuperTokenizer是一种针对印度多语言大语言模型（LLM）的标记化工具，结合子词和多词标记化，显著提升了性能指标。


<details>
  <summary>Details</summary>
Motivation: 由于多语言环境的复杂性和现有方法的局限性，设计高效的标记化工具对LLM的性能至关重要。

Method: 结合子词和多词标记化，辅以语言特定的预标记化策略。

Result: 平均生育分数比LLaMA4提高39.5%，推理吞吐量提升44%，在多语言和代码数据上表现优异。

Conclusion: IndicSuperTokenizer在多语言环境中展现了显著优势，其设计选择具有鲁棒性。

Abstract: Tokenizers play a crucial role in determining the performance, training
efficiency, and the inference cost of Large Language Models (LLMs). Designing
effective tokenizers for multilingual LLMs is particularly challenging due to
diverse scripts and rich morphological variation. While subword methods such as
Byte Pair Encoding (BPE) are widely adopted, their effectiveness in
multilingual settings remains underexplored. We present IndicSuperTokenizer, a
tokenizer for Indic multilingual LLMs, that combines both subword and
multi-word tokenization, along with language-specific pre-tokenization, leading
to more linguistically aligned tokens and achieving a new state-of-the-art in
fertility score. Evaluated across English, 22 Indian languages and code data,
our tokenizer improves the average fertility score by 39.5% over LLaMA4 and by
18% over Sutra (the current best). This translates to 44% improvement in
inference throughput over LLaMA4 while maintaining comparable performance on
English and Indic benchmarks. We also present detailed ablations across
tokenizer training data size, vocabulary size, merging techniques, and
pre-tokenization strategies, demonstrating the robustness of our design
choices.

</details>


### [16] [Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature](https://arxiv.org/abs/2511.03261)
*Ranul Dayarathne,Uvini Ranaweera,Upeksha Ganegoda*

Main category: cs.CL

TL;DR: 研究比较了四种开源LLM和GPT-3.5在问答任务中的表现，发现GPT-3.5与RAG结合效果最佳，而开源LLM中Mistral-7b-instruct表现最优。


<details>
  <summary>Details</summary>
Motivation: 比较不同LLM在问答任务中的性能，尤其是结合RAG技术时的表现。

Method: 使用四种开源LLM（Mistral-7b-instruct、LLaMa2-7b-chat、Falcon-7b-instruct、Orca-mini-v3-7b）和GPT-3.5，结合RAG进行问答任务，并评估准确率、精确度、专家排名、AI模型排名及余弦相似度。

Result: GPT-3.5表现最佳，开源LLM中Mistral-7b-instruct表现最突出；Orca-mini-v3-7b响应最快，LLaMa2-7b-chat最慢。

Conclusion: 开源LLM在优化基础设施后可媲美GPT-3.5等专有模型。

Abstract: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to
enhance the capabilities of Generative AI models by reducing hallucination.
Thus, the increasing prominence of RAG alongside Large Language Models (LLMs)
has sparked interest in comparing the performance of different LLMs in
question-answering (QA) in diverse domains. This study compares the performance
of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,
Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA
tasks within the computer science literature leveraging RAG support. Evaluation
metrics employed in the study include accuracy and precision for binary
questions and ranking by a human expert, ranking by Google's AI model Gemini,
alongside cosine similarity for long-answer questions. GPT-3.5, when paired
with RAG, effectively answers binary and long-answer questions, reaffirming its
status as an advanced LLM. Regarding open-source LLMs, Mistral AI's
Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary
and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b
reports the shortest average latency in generating responses, whereas
LLaMa2-7b-chat by Meta reports the highest average latency. This research
underscores the fact that open-source LLMs, too, can go hand in hand with
proprietary models like GPT-3.5 with better infrastructure.

</details>


### [17] [SCALE: Upscaled Continual Learning of Large Language Models](https://arxiv.org/abs/2511.03270)
*Jin-woo Lee,Junhwa Choi,Bongkyu Hwang,Jinho Choo,Bogun Kim,JeongSeon Yi,Joonseok Lee,DongYoung Jung,Jaeseon Park,Kyoungwon Park,Suk-hoon Jung*

Main category: cs.CL

TL;DR: SCALE是一种宽度扩展架构，通过在冻结预训练参数的同时插入轻量级扩展来提升大语言模型的持续预训练效果，专注于结构优化而非仅参数扩展。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型的持续预训练效果，减少遗忘问题，同时获得新知识。

Method: 提出SCALE架构，通过Persistent Preservation和Collaborative Adaptation原则，实现轻量级扩展和选择性训练。

Result: SCALE在合成数据集和韩语预训练中表现出较少遗忘和竞争性增益。

Conclusion: SCALE通过结构优化和选择性训练，在持续预训练中实现了更好的稳定性与可塑性权衡。

Abstract: We revisit continual pre-training for large language models and argue that
progress now depends more on scaling the right structure than on scaling
parameters alone. We introduce SCALE, a width upscaling architecture that
inserts lightweight expansion into linear modules while freezing all
pre-trained parameters. This preserves the residual and attention topologies
and increases capacity without perturbing the base model's original
functionality. SCALE is guided by two principles: Persistent Preservation,
which maintains the base model's behavior via preservation-oriented
initialization and freezing of the pre-trained weights, and Collaborative
Adaptation, which selectively trains a subset of expansion components to
acquire new knowledge with minimal interference. We instantiate these ideas as
SCALE-Preserve (preservation-first), SCALE-Adapt (adaptation-first), and
SCALE-Route, an optional routing extension that performs token-level routing
between preservation and adaptation heads. On a controlled synthetic biography
benchmark, SCALE mitigates the severe forgetting observed with depth expansion
while still acquiring new knowledge. In continual pre-training on a Korean
corpus, SCALE variants achieve less forgetting on English evaluations and
competitive gains on Korean benchmarks, with these variants offering the best
overall stability-plasticity trade-off. Accompanying analysis clarifies when
preservation provably holds and why the interplay between preservation and
adaptation stabilizes optimization compared to standard continual learning
setups.

</details>


### [18] [Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks](https://arxiv.org/abs/2511.03328)
*Jindong Hong,Tianjie Chen,Lingjie Luo,Chuanyang Zheng,Ting Xu,Haibao Yu,Jianing Qiu,Qianzhong Chen,Suning Huang,Yan Xu,Yong Gui,Yijun He,Jiankai Sun*

Main category: cs.CL

TL;DR: 研究了多模态大语言模型（MLLMs）的‘思考模式’对临床任务性能和可靠性的影响，发现其改进有限。


<details>
  <summary>Details</summary>
Motivation: 探讨增强推理能力的MLLMs在医疗任务中的表现和可靠性。

Method: 评估Seed1.5-VL和Gemini-2.5-Flash模型在四种视觉医疗任务上的表现，使用VQA-RAD和ROCOv2数据集。

Result: 思考模式对大多数任务的性能提升有限，复杂医疗任务表现仍不理想。

Conclusion: 需更多医疗领域数据和高级方法以提升知识整合能力。

Abstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is
the emergence of "reasoning MLLMs" that offer explicit control over their
internal thinking processes (normally referred as the "thinking mode")
alongside the standard "non-thinking mode". This capability allows these models
to engage in a step-by-step process of internal deliberation before generating
a final response. With the rapid transition to and adoption of these
"dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning
processes of these MLLMs impact model performance and reliability in clinical
tasks. This paper evaluates the active "thinking mode" capabilities of two
leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We
assessed their performance on four visual medical tasks using VQA-RAD and
ROCOv2 datasets. Our findings reveal that the improvement from activating the
thinking mode remains marginal compared to the standard non-thinking mode for
the majority of the tasks. Their performance on complex medical tasks such as
open-ended VQA and medical image interpretation remains suboptimal,
highlighting the need for domain-specific medical data and more advanced
methods for medical knowledge integration.

</details>


### [19] [Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances](https://arxiv.org/abs/2511.03354)
*Riasad Alvi,Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Md Rafi Ur Rashid,Md Rafiqul Islam,Yakub Sebastian,Sami Azam*

Main category: cs.CL

TL;DR: 该综述分析了生成式人工智能在生物信息学中的应用，提出了六个研究问题，评估了其在方法学进步、预测性能和专业化方面的表现，并指出了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 生物信息学领域的快速发展需要系统评估生成式人工智能（GenAI）的应用，以识别其在基因组学、蛋白质组学等领域的潜在优势和改进空间。

Method: 通过提出六个研究问题（RQs），基于系统综述和荟萃分析方法，评估GenAI在方法学、预测性能和专业化方面的表现，并分析了其在分子设计、数据整合等领域的应用。

Result: GenAI在多个生物信息学子领域中表现出优于传统方法的性能，尤其是定制化模型架构。然而，数据偏见和可扩展性问题仍限制其通用性。

Conclusion: 生成式人工智能在生物信息学中具有巨大潜力，但需进一步研究以解决可扩展性和数据偏见问题，同时需更多生物学基础建模。

Abstract: Generative artificial intelligence (GenAI) has become a transformative
approach in bioinformatics that often enables advancements in genomics,
proteomics, transcriptomics, structural biology, and drug discovery. To
systematically identify and evaluate these growing developments, this review
proposed six research questions (RQs), according to the preferred reporting
items for systematic reviews and meta-analysis methods. The objective is to
evaluate impactful GenAI strategies in methodological advancement, predictive
performance, and specialization, and to identify promising approaches for
advanced modeling, data-intensive discovery, and integrative biological
analysis. RQ1 highlights diverse applications across multiple bioinformatics
subfields (sequence analysis, molecular design, and integrative data modeling),
which demonstrate superior performance over traditional methods through pattern
recognition and output generation. RQ2 reveals that adapted specialized model
architectures outperformed general-purpose models, an advantage attributed to
targeted pretraining and context-aware strategies. RQ3 identifies significant
benefits in the bioinformatics domains, focusing on molecular analysis and data
integration, which improves accuracy and reduces errors in complex analysis.
RQ4 indicates improvements in structural modeling, functional prediction, and
synthetic data generation, validated by established benchmarks. RQ5 suggests
the main constraints, such as the lack of scalability and biases in data that
impact generalizability, and proposes future directions focused on robust
evaluation and biologically grounded modeling. RQ6 examines that molecular
datasets (such as UniProtKB and ProteinNet12), cellular datasets (such as
CELLxGENE and GTEx) and textual resources (such as PubMedQA and OMIM) broadly
support the training and generalization of GenAI models.

</details>


### [20] [Silenced Biases: The Dark Side LLMs Learned to Refuse](https://arxiv.org/abs/2511.03369)
*Rom Himelstein,Amit LeVi,Brit Youngmann,Yaniv Nemcovsky,Avi Mendelson*

Main category: cs.CL

TL;DR: 本文提出了一种名为Silenced Bias Benchmark（SBB）的新方法，用于揭示安全对齐大型语言模型（LLMs）中潜在的不公平偏好，解决了现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的公平性评估方法通常通过标准问答模式进行，容易忽略模型的拒绝回答背后的潜在偏见，导致对公平性的误判。

Method: 作者提出通过激活导向（activation steering）减少模型在问答中的拒绝行为，从而揭示被安全对齐掩盖的不公平偏好。

Result: 实验结果表明，模型的直接响应与其潜在的公平性问题之间存在显著差异，证明了SBB的有效性。

Conclusion: SBB提供了一个可扩展的公平性评估框架，有助于未来开发更公平的模型和工具。

Abstract: Safety-aligned large language models (LLMs) are becoming increasingly
widespread, especially in sensitive applications where fairness is essential
and biased outputs can cause significant harm. However, evaluating the fairness
of models is a complex challenge, and approaches that do so typically utilize
standard question-answer (QA) styled schemes. Such methods often overlook
deeper issues by interpreting the model's refusal responses as positive
fairness measurements, which creates a false sense of fairness. In this work,
we introduce the concept of silenced biases, which are unfair preferences
encoded within models' latent space and are effectively concealed by
safety-alignment. Previous approaches that considered similar indirect biases
often relied on prompt manipulation or handcrafted implicit queries, which
present limited scalability and risk contaminating the evaluation process with
additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to
uncover these biases by employing activation steering to reduce model refusals
during QA. SBB supports easy expansion to new demographic groups and subjects,
presenting a fairness evaluation framework that encourages the future
development of fair models and tools beyond the masking effects of alignment
training. We demonstrate our approach over multiple LLMs, where our findings
expose an alarming distinction between models' direct responses and their
underlying fairness issues.

</details>


### [21] [EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models for Edge-Deployable Credit Negotiation](https://arxiv.org/abs/2511.03370)
*Yunbo Long,Yuhan Liu,Alexandra Brintrup*

Main category: cs.CL

TL;DR: EQ-Negotiator框架通过结合游戏理论和HMM，让小型语言模型在信用谈判中表现优于大型模型，强调了情感智能而非模型规模的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动谈判中表现优异，但其计算成本高且对隐私保护不足，限制了其在隐私敏感场景的应用。小型语言模型（SLMs）虽实用，但性能较差，特别是在处理复杂情感角色时。

Method: 提出了EQ-Negotiator框架，结合游戏理论和HMM在线学习并跟踪债务人情感状态，无需预训练，赋予SLMs战略智能。

Result: 实验表明，7B参数的SLM搭配EQ-Negotiator在信用谈判中优于10倍规模的基线LLMs，债务回收和谈判效率更佳。

Conclusion: 情感智能是自动谈判成功的关键因素，EQ-Negotiator为隐私保护且高效的AI谈判者提供了解决方案。

Abstract: The deployment of large language models (LLMs) in automated negotiation has
set a high performance benchmark, but their computational cost and data privacy
requirements render them unsuitable for many privacy-sensitive, on-device
applications such as mobile assistants, embodied AI agents or private client
interactions. While small language models (SLMs) offer a practical alternative,
they suffer from a significant performance gap compared to LLMs in playing
emotionally charged complex personas, especially for credit negotiation. This
paper introduces EQ-Negotiator, a novel framework that bridges this capability
gap using emotional personas. Its core is a reasoning system that integrates
game theory with a Hidden Markov Model(HMM) to learn and track debtor emotional
states online, without pre-training. This allows EQ-Negotiator to equip SLMs
with the strategic intelligence to counter manipulation while de-escalating
conflict and upholding ethical standards. Through extensive agent-to-agent
simulations across diverse credit negotiation scenarios, including adversarial
debtor strategies like cheating, threatening, and playing the victim, we show
that a 7B parameter language model with EQ-Negotiator achieves better debt
recovery and negotiation efficiency than baseline LLMs more than 10 times its
size. This work advances persona modeling from descriptive character profiles
to dynamic emotional architectures that operate within privacy constraints.
Besides, this paper establishes that strategic emotional intelligence, not raw
model scale, is the critical factor for success in automated negotiation,
paving the way for effective, ethical, and privacy-preserving AI negotiators
that can operate on the edge.

</details>


### [22] [LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced Logical Reasoning](https://arxiv.org/abs/2511.03372)
*Shenghao Li*

Main category: cs.CL

TL;DR: 本文提出了一种符号逻辑控制的数据增强方法LFC-DA，将逻辑文本转化为命题表达式并通过搜索生成多样化且逻辑严谨的自然语言问题，显著提升了预训练模型的逻辑推理准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂逻辑数据增强中人类标注成本高以及直接使用大语言模型生成逻辑单一和难以理解的样本的问题。

Method: 提出了LFC-DA方法，通过将逻辑文本映射为命题表达式，编译规则库，进行有界状态空间搜索生成有效公式，再将其转换为自然语言问题。

Result: 在ReClor和LogiQA数据集上的实验表明，LFC-DA显著提高了预训练模型的逻辑推理准确性。

Conclusion: LFC-DA是一种有效的数据增强方法，能够在保证逻辑严谨性的同时提升多样性，适用于LLM引导的逻辑数据增强。

Abstract: For complex logical data augmentation, heavy reliance on human annotation is
costly, whereas direct generation with large language models yields
uninterpretable and logically homogeneous examples. To address this, we present
LFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to
propositional expressions, a compact rule library is compiled, and a bounded
state-space search systematically discovers valid formulas that are then
verbalized back into natural-language questions, ensuring both diversity and
logical rigor under propositional logic. Experiments on ReClor and LogiQA show
significant improvements in the logical-reasoning accuracy of pretrained
models, confirming the effectiveness of LFC-DA for LLM-guided logical data
augmentation.

</details>


### [23] [Segmentation Beyond Defaults: Asymmetrical Byte Pair Encoding for Optimal Machine Translation Performance](https://arxiv.org/abs/2511.03383)
*Saumitra Yadav,Manish Shrivastava*

Main category: cs.CL

TL;DR: 研究发现，不对称BPE在低资源机器翻译中优于对称BPE，尤其是在源语言高NMO和目标语言低NMO时效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有的机器翻译研究通常使用对称的BPE方法，但这并不保证在所有语言对和数据量下都表现最优，因此需要探索更有效的方法。

Method: 研究了不同数据量和语言对下的BPE分割方法，比较对称和不对称BPE对机器翻译性能的影响。

Result: 不对称BPE在低资源设置中显著提升了性能，尤其在10种系统中表现优于对称BPE。

Conclusion: 不对称BPE（源语言高NMO，目标语言低NMO）是低资源机器翻译的优化选择。

Abstract: Existing Machine Translation (MT) research often suggests a single, fixed set
of hyperparameters for word segmentation models, symmetric Byte Pair Encoding
(BPE), which applies the same number of merge operations (NMO) to train
tokenizers for both source and target languages. However, we demonstrate that
this uniform approach doesn't guarantee optimal MT performance across different
language pairs and data sizes. This work investigates BPE segmentation recipes
across various data volumes and language pairs to evaluate MT system
performance. We find that utilizing asymmetric BPE, where the source and target
languages have different NMOs, significantly improves results over the
symmetric approach, especially in low-resource settings (50K, 100K, and 500K
sentence pairs). Specifically, asymmetric BPE yield statistically significant
($p<0.05$) average gains of 5.32, 4.46, and 0.7 CHRF++ on English-Hindi in
low-resource setups. We validated this trend across six additional language
pairs (English and Telugu, Shona, Norwegian, Kyrgyz, Hausa, and Inuktitut),
observing statistically significant improvement in 10 out of 12 systems
compared to symmetric BPE. Our findings indicate a high NMO for the source (4K
to 32K) and a low NMO for the target (0.5K to 2K) provides optimal results,
particularly benefiting low-resource MT.

</details>


### [24] [Overcoming the Generalization Limits of SLM Finetuning for Shape-Based Extraction of Datatype and Object Properties](https://arxiv.org/abs/2511.03407)
*Célian Ringwald,Fabien Gandon,Catherine Faron,Franck Michel,Hanna Abi Akl*

Main category: cs.CL

TL;DR: 本研究探讨了小语言模型（SLMs）在处理数据类型和对象属性时如何实现完整的RDF图提取，发现稀有属性的长尾分布是关键瓶颈，并评估了多种策略。


<details>
  <summary>Details</summary>
Motivation: 研究SLMs在提取RDF图中的数据类型和对象属性时的表现，尤其关注稀有属性的长尾分布问题。

Method: 通过分层抽样、加权损失、数据集扩展和基于模板的合成数据增强等策略解决稀有属性的长尾分布问题。

Result: 最佳策略是为训练集中每个属性设置一个最小出现阈值，以确保在不平衡的目标属性上表现均匀。

Conclusion: 研究提供了训练形状感知SLMs的实用指南，并为未来语义关系提取的研究指明了方向。

Abstract: Small language models (SLMs) have shown promises for relation extraction (RE)
when extracting RDF triples guided by SHACL shapes focused on common datatype
properties. This paper investigates how SLMs handle both datatype and object
properties for a complete RDF graph extraction. We show that the key bottleneck
is related to long-tail distribution of rare properties. To solve this issue,
we evaluate several strategies: stratified sampling, weighted loss, dataset
scaling, and template-based synthetic data augmentation. We show that the best
strategy to perform equally well over unbalanced target properties is to build
a training set where the number of occurrences of each property exceeds a given
threshold. To enable reproducibility, we publicly released our datasets,
experimental results and code. Our findings offer practical guidance for
training shape-aware SLMs and highlight promising directions for future work in
semantic RE.

</details>


### [25] [Efficient Reasoning via Thought-Training and Thought-Free Inference](https://arxiv.org/abs/2511.03408)
*Canhui Wu,Qiong Cao,Chao Xue,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 3TF框架通过短到长的视角改进推理效率，训练混合模型实现内部结构化推理，同时保持外部输出的简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要压缩冗长的推理输出，而3TF通过短到长视角提升非推理输出质量，实现高效推理。

Method: 训练混合模型支持推理和非推理模式，通过CoT注释数据内化结构化推理，并在推理时使用非推理模式输出简洁内容。

Result: 3TF训练模型在无显式推理的基准测试中表现显著提升，证明高质量推理可隐式执行。

Conclusion: 3TF框架能在不生成显式步骤的情况下实现高质量隐式推理。

Abstract: Recent advances in large language models (LLMs) have leveraged explicit
Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most
existing methods primarily compress verbose reasoning outputs. These
Long-to-Short transformations aim to improve efficiency, but still rely on
explicit reasoning during inference. In this work, we introduce \textbf{3TF}
(\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree
inference), a framework for efficient reasoning that takes a Short-to-Long
perspective. We first train a hybrid model that can operate in both reasoning
and non-reasoning modes, and then further train it on CoT-annotated data to
internalize structured reasoning, while enforcing concise, thought-free outputs
at inference time using the no-reasoning mode. Unlike compression-based
approaches, 3TF improves the reasoning quality of non-reasoning outputs,
enabling models to perform rich internal reasoning implicitly while keeping
external outputs short. Empirically, 3TF-trained models obtain large
improvements on reasoning benchmarks under thought-free inference,
demonstrating that high quality reasoning can be learned and executed
implicitly without explicit step-by-step generation.

</details>


### [26] [CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field](https://arxiv.org/abs/2511.03441)
*Doria Bonzi,Alexandre Guiggi,Frédéric Béchet,Carlos Ramisch,Benoit Favre*

Main category: cs.CL

TL;DR: CareMedEval是一个用于评估大型语言模型（LLMs）在生物医学领域批判性评价和推理任务中的数据集，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域中批判性评价文献的能力至关重要，但现有LLMs在此任务中的可靠性有限。

Method: 研究者创建了CareMedEval数据集，包含534个基于37篇科学文章的问题，用于评估LLMs的批判性阅读和推理能力。

Result: 开放式和商业模型的准确率未超过0.5，生成中间推理标记能改善结果，但在研究局限和统计分析问题上仍存在挑战。

Conclusion: CareMedEval为基于科学文献的推理提供了挑战性基准，促进了未来自动化批判性评价支持的发展。

Abstract: Critical appraisal of scientific literature is an essential skill in the
biomedical field. While large language models (LLMs) can offer promising
support in this task, their reliability remains limited, particularly for
critical reasoning in specialized domains. We introduce CareMedEval, an
original dataset designed to evaluate LLMs on biomedical critical appraisal and
reasoning tasks. Derived from authentic exams taken by French medical students,
the dataset contains 534 questions based on 37 scientific articles. Unlike
existing benchmarks, CareMedEval explicitly evaluates critical reading and
reasoning grounded in scientific papers. Benchmarking state-of-the-art
generalist and biomedical-specialized LLMs under various context conditions
reveals the difficulty of the task: open and commercial models fail to exceed
an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens
considerably improves the results. Yet, models remain challenged especially on
questions about study limitations and statistical analysis. CareMedEval
provides a challenging benchmark for grounded reasoning, exposing current LLM
limitations and paving the way for future development of automated support for
critical appraisal.

</details>


### [27] [Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG](https://arxiv.org/abs/2511.03410)
*Longpeng Qiu,Ting Li,Shuai Mao,Nan Yang,Xiaohui Yan*

Main category: cs.CL

TL;DR: QuestionRAG通过外部知识增强和强化学习，解决了QA系统中的输入错误问题，提升了模型的理解和修正能力。


<details>
  <summary>Details</summary>
Motivation: QA系统中的输入错误常导致错误回答，LLMs在此任务中表现不佳，容易出现误解或过度修正的问题。

Method: QuestionRAG通过外部知识（如搜索结果）增强输入以减少误解，并通过强化学习对齐模型目标以避免过度修正。

Result: 知识增强对理解错误问题至关重要，强化学习比传统监督微调更有效，提升了模型的指令遵循和泛化能力。

Conclusion: 结合这两种策略，QuestionRAG充分发挥了LLMs在问题修正任务中的潜力。

Abstract: Input errors in question-answering (QA) systems often lead to incorrect
responses. Large language models (LLMs) struggle with this task, frequently
failing to interpret user intent (misinterpretation) or unnecessarily altering
the original question's structure (over-correction). We propose QuestionRAG, a
framework that tackles these problems. To address misinterpretation, it
enriches the input with external knowledge (e.g., search results, related
entities). To prevent over-correction, it uses reinforcement learning (RL) to
align the model's objective with precise correction, not just paraphrasing. Our
results demonstrate that knowledge augmentation is critical for understanding
faulty questions. Furthermore, RL-based alignment proves significantly more
effective than traditional supervised fine-tuning (SFT), boosting the model's
ability to follow instructions and generalize. By integrating these two
strategies, QuestionRAG unlocks the full potential of LLMs for the question
correction task.

</details>


### [28] [SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties](https://arxiv.org/abs/2511.03542)
*Roberta Di Marino,Giovanni Dioguardi,Antonio Romano,Giuseppe Riccio,Mariano Barone,Marco Postiglione,Flora Amato,Vincenzo Moscato*

Main category: cs.CL

TL;DR: SOLVE-Med 是一个多智能体架构，通过结合领域专用的小型语言模型解决复杂医学问题，相比单独的大模型表现更优且支持本地部署。


<details>
  <summary>Details</summary>
Motivation: 医学问答系统面临幻觉、偏见、计算需求高、隐私问题及多领域专业知识需求等挑战，需要更高效的解决方案。

Method: 采用 Router Agent 动态选择专家模型，10 个领域专用小型语言模型（每个 1B 参数），以及 Orchestrator Agent 综合回答。

Result: 在 10 个医学领域的意大利论坛数据上，SOLVE-Med 的 ROUGE-1 为 0.301，BERTScore F1 为 0.697，优于单独的大模型（最高 14B 参数）。

Conclusion: SOLVE-Med 通过多智能体架构解决了医学问答系统的核心问题，性能优越且支持本地化部署。

Abstract: Medical question answering systems face deployment challenges including
hallucinations, bias, computational demands, privacy concerns, and the need for
specialized expertise across diverse domains. Here, we present SOLVE-Med, a
multi-agent architecture combining domain-specialized small language models for
complex medical queries. The system employs a Router Agent for dynamic
specialist selection, ten specialized models (1B parameters each) fine-tuned on
specific medical domains, and an Orchestrator Agent that synthesizes responses.
Evaluated on Italian medical forum data across ten specialties, SOLVE-Med
achieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697,
outperforming standalone models up to 14B parameters while enabling local
deployment. Our code is publicly available on GitHub:
https://github.com/PRAISELab-PicusLab/SOLVE-Med.

</details>


### [29] [MultiZebraLogic: A Multilingual Logical Reasoning Benchmark](https://arxiv.org/abs/2511.03553)
*Sofie Helene Bruun,Dan Saattrup Smart*

Main category: cs.CL

TL;DR: 论文旨在创建多语言的大型高质量数据集（MultiZebraLogic），用于评估不同语言模型的逻辑推理能力，并通过增加难度（如使用干扰线索）来验证模型的性能。


<details>
  <summary>Details</summary>
Motivation: 为全面评估大型语言模型的逻辑推理能力，需要涵盖多任务和多语言的基准数据集，以适配不同能力的模型。

Method: 生成多种语言、主题、大小和线索类型的斑马谜题（Zebra puzzles），包括干扰线索（red herrings），并测试其在GPT-4o mini和o3-mini上的表现。

Result: 发现2x3和4x5大小的谜题分别对非推理模型和推理模型具有挑战性；干扰线索显著降低了模型性能，但语言和主题对模型表现无显著影响。

Conclusion: MultiZebraLogic数据集和代码的发布为未来的多语言和多主题研究提供了灵活性。

Abstract: Measuring the full abilities of large language models (LLMs) requires
benchmarks representing multiple tasks. We aim to create large, high-quality
datasets for comparison of logical reasoning skills across several languages
and of suitable difficulty for LLMs of various reasoning ability. We explore
multiple ways of increasing difficulty. We generate zebra puzzles in multiple
languages, themes, sizes and including 14 different clue types and 8 red
herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are
sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a
reasoning model), respectively. Including 5 red herrings decreases o3-mini
puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5
puzzles are not significantly affected by use of English vs. Danish or the
common houses theme vs. the country-specific smoerrebroed theme. We find no
correlation between difficulty and the selected clue types. Datasets of
128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic
languages for sizes 2x3 and 4x5. We publish code for puzzle generation,
designed for adaptablity into more languages and themes.

</details>


### [30] [Kastor: Fine-tuned Small Language Models for Shape-based Active Relation Extraction](https://arxiv.org/abs/2511.03466)
*Ringwald Celian,Gandon Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: Kastor框架通过RDF模式提取和小语言模型微调，优化知识库的完成与精炼，提升模型泛化和性能。


<details>
  <summary>Details</summary>
Motivation: 解决专业领域知识库的完成与精炼问题，通过改进传统验证任务提升模型表现。

Method: Kastor通过评估SHACL形状的所有可能属性组合并选择最优组合，结合迭代学习优化模型。

Result: 显著提升模型泛化和性能，并能通过迭代学习发现新的相关事实。

Conclusion: Kastor框架为专业领域知识库的优化提供了高效解决方案。

Abstract: RDF pattern-based extraction is a compelling approach for fine-tuning small
language models (SLMs) by focusing a relation extraction task on a specified
SHACL shape. This technique enables the development of efficient models trained
on limited text and RDF data. In this article, we introduce Kastor, a framework
that advances this approach to meet the demands for completing and refining
knowledge bases in specialized domains. Kastor reformulates the traditional
validation task, shifting from single SHACL shape validation to evaluating all
possible combinations of properties derived from the shape. By selecting the
optimal combination for each training example, the framework significantly
enhances model generalization and performance. Additionally, Kastor employs an
iterative learning process to refine noisy knowledge bases, enabling the
creation of robust models capable of uncovering new, relevant facts

</details>


### [31] [AILA--First Experiments with Localist Language Models](https://arxiv.org/abs/2511.03559)
*Joachim Diederich*

Main category: cs.CL

TL;DR: 论文提出了一种新型的Transformer语言模型框架，通过可调节的局部性参数动态控制表征的局部化程度，实验表明局部化配置能显著降低注意力熵并提高指针保真度。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型依赖分布式表征，缺乏可解释性。本文旨在通过可调的局部性参数，实现从可解释的局部编码到高效分布式表征的动态切换。

Method: 采用两层Transformer架构，在WikiText语料上实验，通过调节局部性参数λ（从1.0到0.0）观察效果。

Result: 局部化配置（λ=1.0）注意力熵显著降低（5.36比特），指针保真度更高。中间值λ=0.6在性能和可解释性间达到平衡（测试困惑度4.65，准确率84.7%）。

Conclusion: 局部化语言模型为需要透明性和能力的领域提供了实用框架，通过明确的数学控制平衡可解释性与性能。

Abstract: This paper presents the first empirical demonstration of controllable
locality in transformer language models, a novel architectural framework that
enables continuous control over the degree of representation localization
through a tunable locality dial parameter. Unlike traditional language models
that rely exclusively on distributed representations, our approach allows
dynamic interpolation between highly interpretable localist encodings and
efficient distributed representations without requiring model retraining. We
conducted experiments on the WikiText corpus using a two-layer transformer
architecture, systematically varying the locality parameter {\lambda} across
the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our
results demonstrate that localist configurations achieve dramatically lower
attention entropy, with {\lambda} = 1.0 yielding 5.36 bits compared to 7.18
bits at {\lambda} = 0.0, while maintaining substantially higher pointer
fidelity scores reflecting stronger alignment with rule-specified targets.
Prediction experiments reveal that intermediate locality values optimize the
tradeoff between interpretability and performance, with {\lambda} = 0.6
achieving test perplexity of 4.65 and accuracy of 84.7%. These findings
establish that localist language models provide a practical framework for
applications in regulated domains requiring both transparency and capability,
offering precise mathematical control over the interpretability-performance
spectrum through explicit penalty thresholds and information-theoretic design
principles.

</details>


### [32] [BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation](https://arxiv.org/abs/2511.03498)
*Kazi Reyazul Hasan,Mubasshira Musarrat,A. B. M. Alim Al Islam,Muhammad Abdullah Adnan*

Main category: cs.CL

TL;DR: 研究了针对孟加拉语和英语技术内容翻译的系统,提出了一种新的数据集BanglaSTEM和一个基于T5的翻译模型,显著提高了技术术语的翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理孟加拉语技术问题时的翻译准确性不足问题,特别是技术术语的误译。

Method: 构建了包含5000个高质量孟加拉语-英语技术句子对的BanglaSTEM数据集,训练了基于T5的翻译模型。

Result: 翻译模型在代码生成和数学问题解决任务中表现出显著的技术内容翻译准确性提升。

Conclusion: BanglaSTEM数据集和T5翻译模型为孟加拉语用户提供了更有效的技术问题解决方案,同时公开了资源。

Abstract: Large language models work well for technical problem solving in English but
perform poorly when the same questions are asked in Bangla. A simple solution
would be to translate Bangla questions into English first and then use these
models. However, existing Bangla-English translation systems struggle with
technical terms. They often mistranslate specialized vocabulary, which changes
the meaning of the problem and leads to wrong answers. We present BanglaSTEM, a
dataset of 5,000 carefully selected Bangla-English sentence pairs from STEM
fields including computer science, mathematics, physics, chemistry, and
biology. We generated over 12,000 translations using language models and then
used human evaluators to select the highest quality pairs that preserve
technical terminology correctly. We train a T5-based translation model on
BanglaSTEM and test it on two tasks: generating code and solving math problems.
Our results show significant improvements in translation accuracy for technical
content, making it easier for Bangla speakers to use English-focused language
models effectively. Both the BanglaSTEM dataset and the trained translation
model are publicly released at https://huggingface.co/reyazul/BanglaSTEM-T5.

</details>


### [33] [Step-Audio-EditX Technical Report](https://arxiv.org/abs/2511.03601)
*Chao Yan,Boyong Wu,Peng Yang,Pengfei Tan,Guoqiang Hu,Yuxin Zhang,Xiangyu,Zhang,Fei Tian,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.CL

TL;DR: Step-Audio-EditX是首个基于LLM的开源音频模型，擅长情感和语音风格的迭代编辑，并具备强大的零样本TTS能力，通过大边界合成数据实现创新。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖嵌入先验或辅助模块，限制了音频编辑的表达能力与迭代控制能力。

Method: 利用大边界合成数据，避免嵌入先验或辅助模块，专注于迭代控制和高表达能力。

Result: 在情感编辑和其他细粒度控制任务上，超越了MiniMax-2.6-hd和Doubao-Seed-TTS-2.0。

Conclusion: Step-Audio-EditX通过新颖的数据学习方法，实现了音频编辑的高表达性与迭代控制能力。

Abstract: We present Step-Audio-EditX, the first open-source LLM-based audio model
excelling at expressive and iterative audio editing encompassing emotion,
speaking style, and paralinguistics alongside robust zero-shot text-to-speech
(TTS) capabilities.Our core innovation lies in leveraging only large-margin
synthetic data, which circumvents the need for embedding-based priors or
auxiliary modules. This large-margin learning approach enables both iterative
control and high expressivity across voices, and represents a fundamental pivot
from the conventional focus on representation-level disentanglement. Evaluation
results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and
Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.

</details>


### [34] [HaluMem: Evaluating Hallucinations in Memory Systems of Agents](https://arxiv.org/abs/2511.03506)
*Ding Chen,Simin Niu,Kehang Li,Peng Liu,Xiangping Zheng,Bo Tang,Xinchi Li,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: HaluMem是首个针对记忆系统的操作级别幻觉评估基准，通过定义三个评估任务揭示不同操作阶段的幻觉行为，指出现有记忆系统在提取和更新阶段易产生幻觉，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 记忆系统在AI系统中至关重要，但存储和检索过程中常出现幻觉问题，现有评估方法难以定位问题阶段，因此需要新的评估工具。

Method: 提出HaluMem基准，包含记忆提取、更新和问答三个任务，构建用户中心的多轮交互数据集HaluMem-Medium和HaluMem-Long。

Result: 实验表明现有记忆系统在提取和更新阶段易产生幻觉，错误会传播到问答阶段。

Conclusion: 未来应开发可解释和受限的记忆操作机制，系统性抑制幻觉并提升记忆可靠性。

Abstract: Memory systems are key components that enable AI systems such as LLMs and AI
agents to achieve long-term learning and sustained interaction. However, during
memory storage and retrieval, these systems frequently exhibit memory
hallucinations, including fabrication, errors, conflicts, and omissions.
Existing evaluations of memory hallucinations are primarily end-to-end question
answering, which makes it difficult to localize the operational stage within
the memory system where hallucinations arise. To address this, we introduce the
Hallucination in Memory Benchmark (HaluMem), the first operation level
hallucination evaluation benchmark tailored to memory systems. HaluMem defines
three evaluation tasks (memory extraction, memory updating, and memory question
answering) to comprehensively reveal hallucination behaviors across different
operational stages of interaction. To support evaluation, we construct
user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and
HaluMem-Long. Both include about 15k memory points and 3.5k multi-type
questions. The average dialogue length per user reaches 1.5k and 2.6k turns,
with context lengths exceeding 1M tokens, enabling evaluation of hallucinations
across different context scales and task complexities. Empirical studies based
on HaluMem show that existing memory systems tend to generate and accumulate
hallucinations during the extraction and updating stages, which subsequently
propagate errors to the question answering stage. Future research should focus
on developing interpretable and constrained memory operation mechanisms that
systematically suppress hallucinations and improve memory reliability.

</details>


### [35] [ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation](https://arxiv.org/abs/2511.03656)
*Jing Gao,Shutiao Luo,Yumeng Liu,Yuanming Li,Hongji Zeng*

Main category: cs.CL

TL;DR: 介绍了专为中文多文档问答设计的ChiMDQA数据集，涵盖六个领域，包含6068个高质量QA对，适用于多种NLP任务。


<details>
  <summary>Details</summary>
Motivation: 满足高质量中文文档问答数据集的需求，以支持学术和商业场景的应用。

Method: 通过细致的文档筛选和系统化的问题设计方法创建数据集。

Result: ChiMDQA数据集保证了多样性和高质量，并提供了设计目标和评估系统的详细说明。

Conclusion: ChiMDQA为中文问答研究与实践提供了重要基础，数据和代码已公开。

Abstract: With the rapid advancement of natural language processing (NLP) technologies,
the demand for high-quality Chinese document question-answering datasets is
steadily growing. To address this issue, we present the Chinese Multi-Document
Question Answering Dataset(ChiMDQA), specifically designed for downstream
business scenarios across prevalent domains including academic, education,
finance, law, medical treatment, and news. ChiMDQA encompasses long-form
documents from six distinct fields, consisting of 6,068 rigorously curated,
high-quality question-answer (QA) pairs further classified into ten
fine-grained categories. Through meticulous document screening and a systematic
question-design methodology, the dataset guarantees both diversity and high
quality, rendering it applicable to various NLP tasks such as document
comprehension, knowledge extraction, and intelligent QA systems. Additionally,
this paper offers a comprehensive overview of the dataset's design objectives,
construction methodologies, and fine-grained evaluation system, supplying a
substantial foundation for future research and practical applications in
Chinese QA. The code and data are available at:
https://anonymous.4open.science/r/Foxit-CHiMDQA/.

</details>


### [36] [One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction Following with a Benchmark Evolving Framework](https://arxiv.org/abs/2511.03508)
*Qi Jia,Kaiwei Zhang,Xiujie Song,Ye Shen,Xiangyang Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: 论文提出了一个用于评估多轮指令跟随能力的可扩展框架EvolIF，并通过该框架构建了一个动态基准测试，结果显示GPT-5表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试通常限于固定轮次，容易饱和且无法反映用户的交互体验。

Method: 提出了一种三层机制的解耦框架，动态构建基准测试，模拟用户与大型语言模型的交互。

Result: GPT-5在平均18.54轮对话和70.31%的稳健性上表现最佳，显著优于其他模型。

Conclusion: 该框架为评估指令跟随能力提供了有效工具，且GPT-5在多轮对话中表现卓越。

Abstract: Understanding how well large language models can follow users' instructions
throughout a dialogue spanning multiple topics is of great importance for
data-intensive conversational applications. Existing benchmarks are often
limited to a fixed number of turns, making them susceptible to saturation and
failing to account for the user's interactive experience. In this work, we
propose an extensible framework for assessing multi-turn instruction-following
ability. At its core, our framework decouples linguistic surface forms from
user intent simulation through a three-layer mechanism that tracks constraints,
instructions, and topics. This framework mimics User-LLM interaction by
enabling the dynamic construction of benchmarks with state changes and
tracebacks, terminating a conversation only when the model exhausts a simulated
user's patience. We define a suite of metrics capturing the quality of the
interaction process. Using this framework, we construct EvolIF, an evolving
instruction-following benchmark incorporating nine distinct constraint types.
Our results indicate that GPT-5 exhibits superior instruction-following
performance. It sustains an average of 18.54 conversational turns and
demonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant
margin of 11.41%, while other models lag far behind. All of the data and code
will be made publicly available online.

</details>


### [37] [Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask](https://arxiv.org/abs/2511.03718)
*Nan Li,Albert Gatt,Massimo Poesio*

Main category: cs.CL

TL;DR: 研究了协作对话中由于视角差异导致的误解问题，提出了一种新的标注方法，并利用LLM标注流程分析了HCRC MapTask语料库中的13k参考表达，发现完全误解罕见但多义性差异常导致分歧。


<details>
  <summary>Details</summary>
Motivation: 探讨在不对称协作对话中，参与者可能因视角不同而误解对方所指实体的问题，旨在揭示误解的产生和修复过程。

Method: 引入了一种基于视角的标注方案，利用约束LLM标注流程对HCRC MapTask语料库中的参考表达进行标注，共标注13k条数据，并分析了理解状态。

Result: 结果表明，一旦统一词汇变体，完全误解很少见，但多义性差异会系统性导致分歧，揭示了表面一致背后可能存在的指代错位。

Conclusion: 研究成果提供了一个资源和分析框架，用于研究协作对话中的误解问题，并评估(V)LLM在建模视角依赖性共识方面的能力。

Abstract: Collaborative dialogue relies on participants incrementally establishing
common ground, yet in asymmetric settings they may believe they agree while
referring to different entities. We introduce a perspectivist annotation scheme
for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures
speaker and addressee grounded interpretations for each reference expression,
enabling us to trace how understanding emerges, diverges, and repairs over
time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k
annotated reference expressions with reliability estimates and analyze the
resulting understanding states. The results show that full misunderstandings
are rare once lexical variants are unified, but multiplicity discrepancies
systematically induce divergences, revealing how apparent grounding can mask
referential misalignment. Our framework provides both a resource and an
analytic lens for studying grounded misunderstanding and for evaluating
(V)LLMs' capacity to model perspective-dependent grounding in collaborative
dialogue.

</details>


### [38] [ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation](https://arxiv.org/abs/2511.03563)
*One Octadion,Bondan Sapta Prakoso,Nanang Yudi Setiawan,Novanto Yudistira*

Main category: cs.CL

TL;DR: 该研究通过微调大语言模型（LLMs）并结合检索增强生成（RAG）方法，以支持政策制定者理解、分析和制定法律规范。


<details>
  <summary>Details</summary>
Motivation: 提升LLMs对法律文本的理解能力，以辅助政策制定者更高效地完成法律研究和法规制定任务。

Method: 使用专门为法律领域定制的监督数据集进行微调，并集成RAG方法以引入外部最新的法律知识。

Result: 该方法显著提升了法律研究和法规制定的效率，为政策制定者提供了有力的工具支持。

Conclusion: 结合微调和RAG的方法在法律领域具有实际应用价值，能够满足政策制定者对快速、准确法律支持的迫切需求。

Abstract: In this study, we explore the fine-tuning of Large Language Models (LLMs) to
better support policymakers in their crucial work of understanding, analyzing,
and crafting legal regulations. To equip the model with a deep understanding of
legal texts, we curated a supervised dataset tailored to the specific needs of
the legal domain. Additionally, we integrated the Retrieval-Augmented
Generation (RAG) method, enabling the LLM to access and incorporate up-to-date
legal knowledge from external sources. This combination of fine-tuning and
RAG-based augmentation results in a tool that not only processes legal
information but actively assists policymakers in interpreting regulations and
drafting new ones that align with current needs. The results demonstrate that
this approach can significantly enhance the effectiveness of legal research and
regulation development, offering a valuable resource in the ever-evolving field
of law.

</details>


### [39] [A systematic review of relation extraction task since the emergence of Transformers](https://arxiv.org/abs/2511.03610)
*Ringwald Celian,Gandon,Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: 该论文系统综述了自Transformer模型兴起以来的关系抽取研究，分析了2019至2024年的34篇综述、64个数据集和104个模型，总结了方法进展、基准资源及语义网技术应用，指出了当前趋势与挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在系统梳理关系抽取领域的发展，为研究者和从业者提供全面的参考，帮助理解该领域的演变和未来方向。

Method: 采用自动化框架收集并标注文献，分析包括综述、数据集和模型在内的多维度数据。

Result: 总结了关系抽取的方法论进展、基准资源及语义网技术整合，明确了当前趋势和开放挑战。

Conclusion: 通过综合分析，为关系抽取领域的研究者和从业者提供了关于其发展和未来方向的全面视角。

Abstract: This article presents a systematic review of relation extraction (RE)
research since the advent of Transformer-based models. Using an automated
framework to collect and annotate publications, we analyze 34 surveys, 64
datasets, and 104 models published between 2019 and 2024. The review highlights
methodological advances, benchmark resources, and the integration of semantic
web technologies. By consolidating results across multiple dimensions, the
study identifies current trends, limitations, and open challenges, offering
researchers and practitioners a comprehensive reference for understanding the
evolution and future directions of RE.

</details>


### [40] [Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability](https://arxiv.org/abs/2511.03635)
*Apoorva Upadhyaya,Wolfgang Nejdl,Marco Fisichella*

Main category: cs.CL

TL;DR: IRIS是一种新颖的可解释零样本立场检测框架，通过隐性和显性理由理解立场，具有较高的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有零样本立场检测方法在泛化性和解释性上的不足，尤其是对未见目标的处理和对预测的可解释性。

Method: 采用信息检索排名任务的方式，结合隐性和显性理由（文本序列和语言学特征）来推断立场。

Result: 在多个基准数据集（VAST、EZ-STANCE、P-Stance、RFD）上表现出色，尤其在数据有限（10%-50%训练数据）时仍保持高泛化性。

Conclusion: IRIS通过创新架构和可解释设计，显著提升了零样本立场检测的性能和可解释性。

Abstract: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward
unseen targets. Existing research using contrastive, meta-learning, or data
augmentation suffers from generalizability issues or lack of coherence between
text and target. Recent works leveraging large language models (LLMs) for ZSSD
focus either on improving unseen target-specific knowledge or generating
explanations for stance analysis. However, most of these works are limited by
their over-reliance on explicit reasoning, provide coarse explanations that
lack nuance, and do not explicitly model the reasoning process, making it
difficult to interpret the model's predictions. To address these issues, in our
study, we develop a novel interpretable ZSSD framework, IRIS. We provide an
interpretable understanding of the attitude of the input towards the target
implicitly based on sequences within the text (implicit rationales) and
explicitly based on linguistic measures (explicit rationales). IRIS considers
stance detection as an information retrieval ranking task, understanding the
relevance of implicit rationales for different stances to guide the model
towards correct predictions without requiring the ground-truth of rationales,
thus providing inherent interpretability. In addition, explicit rationales
based on communicative features help decode the emotional and cognitive
dimensions of stance, offering an interpretable understanding of the author's
attitude towards the given target. Extensive experiments on the benchmark
datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%
training data prove the generalizability of our model, benefiting from the
proposed architecture and interpretable design.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [41] [PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework](https://arxiv.org/abs/2511.03023)
*Sina Montazeri,Yunhe Feng,Kewei Sha*

Main category: cs.AI

TL;DR: PublicAgent框架通过多智能体分解解决大语言模型在端到端分析中的局限性，提出五个设计原则，展示了专业化的价值。


<details>
  <summary>Details</summary>
Motivation: 开放数据仓库具有基于证据决策的潜力，但对非专家难以访问。大语言模型在单个任务中表现优异，但在端到端分析中存在局限性。

Method: 通过分解为意图澄清、数据集发现、分析和报告等专业智能体的多智能体框架（PublicAgent），并在每个阶段进行验证。

Result: 评估显示，专业化智能体提高了性能（97.5%的智能体胜率），并提出了五个设计原则，包括智能体分类和架构设计的重要性。

Conclusion: 多智能体架构能够解决复杂分析工作流的局限性，并通过自然语言接口使公共数据更易访问。

Abstract: Open data repositories hold potential for evidence-based decision-making, yet
are inaccessible to non-experts lacking expertise in dataset discovery, schema
mapping, and statistical analysis. Large language models show promise for
individual tasks, but end-to-end analytical workflows expose fundamental
limitations: attention dilutes across growing contexts, specialized reasoning
patterns interfere, and errors propagate undetected. We present PublicAgent, a
multi-agent framework that addresses these limitations through decomposition
into specialized agents for intent clarification, dataset discovery, analysis,
and reporting. This architecture maintains focused attention within agent
contexts and enables validation at each stage. Evaluation across five models
and 50 queries derives five design principles for multi-agent LLM systems.
First, specialization provides value independent of model strength--even the
strongest model shows 97.5% agent win rates, with benefits orthogonal to model
scale. Second, agents divide into universal (discovery, analysis) and
conditional (report, intent) categories. Universal agents show consistent
effectiveness (std dev 12.4%) while conditional agents vary by model (std dev
20.5%). Third, agents mitigate distinct failure modes--removing discovery or
analysis causes catastrophic failures (243-280 instances), while removing
report or intent causes quality degradation. Fourth, architectural benefits
persist across task complexity with stable win rates (86-92% analysis, 84-94%
discovery), indicating workflow management value rather than reasoning
enhancement. Fifth, wide variance in agent effectiveness across models (42-96%
for analysis) requires model-aware architecture design. These principles guide
when and why specialization is necessary for complex analytical workflows while
enabling broader access to public data through natural language interfaces.

</details>


### [42] [No-Human in the Loop: Agentic Evaluation at Scale for Recommendation](https://arxiv.org/abs/2511.03051)
*Tao Zhang,Kehui Yao,Luyi Ma,Jiao Chen,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: 该研究通过ScalingEval系统地比较了36种大型语言模型（LLM）的性能，采用多主体框架和多数投票机制评估模型表现，并提供了关键发现和实用性建议。


<details>
  <summary>Details</summary>
Motivation: 为了构建可扩展且可靠的大型语言模型评估流程，研究团队设计了ScalingEval，以解决现有评估方法的局限性。

Method: 研究采用多主体框架，结合模式审核和问题编码，通过多数投票生成标签，无需人工标注即可进行可复现的比较。

Result: 研究结果显示：Claude 3.5 Sonnet决策信心最高；Gemini 1.5 Pro综合性能最佳；GPT-4o在延迟-准确性-成本之间平衡最优；GPT-OSS 20B在开源模型中领先。结构化领域表现一致，但生活方式类存在分歧。

Conclusion: ScalingEval为LLM评估提供了可复现的基准和协议，并为模型的扩展性、可靠性和选择提供了实用指导。

Abstract: Evaluating large language models (LLMs) as judges is increasingly critical
for building scalable and trustworthy evaluation pipelines. We present
ScalingEval, a large-scale benchmarking study that systematically compares 36
LLMs, including GPT, Gemini, Claude, and Llama, across multiple product
categories using a consensus-driven evaluation protocol. Our multi-agent
framework aggregates pattern audits and issue codes into ground-truth labels
via scalable majority voting, enabling reproducible comparison of LLM
evaluators without human annotation. Applied to large-scale complementary-item
recommendation, the benchmark reports four key findings: (i) Anthropic Claude
3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers
the best overall performance across categories; (iii) GPT-4o provides the most
favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among
open-source models. Category-level analysis shows strong consensus in
structured domains (Electronics, Sports) but persistent disagreement in
lifestyle categories (Clothing, Food). These results establish ScalingEval as a
reproducible benchmark and evaluation protocol for LLMs as judges, with
actionable guidance on scaling, reliability, and model family tradeoffs.

</details>


### [43] [Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge](https://arxiv.org/abs/2511.03070)
*Drago Plecko,Patrik Okanovic,Torsten Hoefler,Elias Bareinboim*

Main category: cs.AI

TL;DR: 这篇论文旨在评估大型语言模型（LLMs）是否能够内化现实世界的概率分布知识，通过构建一个跨领域的基准测试。结果表明，LLMs在此方面表现不佳，未能有效学习观测分布，从而限制了其在干预和反事实知识上的能力。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs能否理解和内化现实世界的概率分布知识，以验证其作为通用分布学习器的潜力。

Method: 开发首个直接测试LLMs是否掌握现实世界分布的基准，覆盖经济、健康、教育和社会行为等多个领域。

Result: LLMs在基准测试中表现不佳，未能自然内化现实世界统计数据，且缺乏观测分布知识。

Conclusion: 研究表明LLMs在概率分布学习上存在局限，这限制了其在因果层级中的更高层次（干预和反事实）能力。

Abstract: Artificial intelligence (AI) systems hold great promise for advancing various
scientific disciplines, and are increasingly used in real-world applications.
Despite their remarkable progress, further capabilities are expected in order
to achieve more general types of intelligence. A critical distinction in this
context is between factual knowledge, which can be evaluated against true or
false answers (e.g., "what is the capital of England?"), and probabilistic
knowledge, reflecting probabilistic properties of the real world (e.g., "what
is the sex of a computer science graduate in the US?"). In this paper, our goal
is to build a benchmark for understanding the capabilities of LLMs in terms of
knowledge of probability distributions describing the real world. Given that
LLMs are trained on vast amounts of text, it may be plausible that they
internalize aspects of these distributions. Indeed, LLMs are touted as powerful
universal approximators of real-world distributions. At the same time,
classical results in statistics, known as curse of dimensionality, highlight
fundamental challenges in learning distributions in high dimensions,
challenging the notion of universal distributional learning. In this work, we
develop the first benchmark to directly test this hypothesis, evaluating
whether LLMs have access to empirical distributions describing real-world
populations across domains such as economics, health, education, and social
behavior. Our results demonstrate that LLMs perform poorly overall, and do not
seem to internalize real-world statistics naturally. When interpreted in the
context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that
language models do not contain knowledge on observational distributions (Layer
1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional
(Layer 2) and counterfactual (Layer 3) knowledge of these models is also
limited.

</details>


### [44] [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)
*Jonathan Li,Nasim Farahini,Evgenii Iuliugin,Magnus Vesterlund,Christian Haggstrom,Guangtao Wang,Shubhangi Upasani,Ayush Sachdeva,Rui Li,Faline Fu,Chen Wu,Ayesha Siddiqua,John Long,Tuowen Zhao,Matheen Musaddiq,Hakan Zeffer,Yun Du,Mingran Wang,Qinghua Li,Bo Li,Urmish Thakker,Raghu Prabhakar*

Main category: cs.AI

TL;DR: 该论文研究了在现代工业部署中如何有效控制KV缓存大小，同时保持模型准确性，并提出了SnapStream方法。


<details>
  <summary>Details</summary>
Motivation: 随着大模型（如100B+参数的LLMs）的普及，对片上内存的需求增加，但现有技术在工业部署中难以应用，且准确性影响不明确。

Method: 提出了SnapStream，一种可在静态图和连续批处理框架中部署的KV缓存压缩方法，并在Llama-3.1-8B-Instruct和DeepSeek-R1上测试其准确性影响。

Result: SnapStream在16路张量并行部署中显著改善了内存使用（4倍提升），同时准确性下降最小，适用于生产环境。

Conclusion: SnapStream是首个在静态图和连续批处理的生产推理系统中部署的稀疏KV注意力技术，具有实际应用价值。

Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.

</details>


### [45] [Large language models require a new form of oversight: capability-based monitoring](https://arxiv.org/abs/2511.03106)
*Katherine C. Kellogg,Bingyang Ye,Yifan Hu,Guergana K. Savova,Byron Wallace,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: 论文提出了一种新的监控原则——基于能力的监控，以应对大语言模型（LLM）在医疗领域的应用挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的任务型监控方法无法适用于通用的LLM模型，因为它们不是为特定任务设计的。

Method: 提出了基于能力的监控方法，围绕模型的共享能力（如总结、推理、翻译等）组织监控，而非独立评估每个任务。

Result: 这种方法能更有效地检测系统性弱点、长尾错误和涌现行为。

Conclusion: 基于能力的监控为LLM及未来通用人工智能模型的安全、自适应和协作监控提供了可扩展的基础。

Abstract: The rapid adoption of large language models (LLMs) in healthcare has been
accompanied by scrutiny of their oversight. Existing monitoring approaches,
inherited from traditional machine learning (ML), are task-based and founded on
assumed performance degradation arising from dataset drift. In contrast, with
LLMs, inevitable model degradation due to changes in populations compared to
the training dataset cannot be assumed, because LLMs were not trained for any
specific task in any given population. We therefore propose a new organizing
principle guiding generalist LLM monitoring that is scalable and grounded in
how these models are developed and used in practice: capability-based
monitoring. Capability-based monitoring is motivated by the fact that LLMs are
generalist systems whose overlapping internal capabilities are reused across
numerous downstream tasks. Instead of evaluating each downstream task
independently, this approach organizes monitoring around shared model
capabilities, such as summarization, reasoning, translation, or safety
guardrails, in order to enable cross-task detection of systemic weaknesses,
long-tail errors, and emergent behaviors that task-based monitoring may miss.
We describe considerations for developers, organizational leaders, and
professional societies for implementing a capability-based monitoring approach.
Ultimately, capability-based monitoring will provide a scalable foundation for
safe, adaptive, and collaborative monitoring of LLMs and future generalist
artificial intelligence models in healthcare.

</details>


### [46] [miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward](https://arxiv.org/abs/2511.03108)
*Azim Ospanov,Farzan Farnia,Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: 论文分析了miniF2F基准测试中形式与非形式化数学题目的差异，提出改进版本miniF2F-v2，显著提升了AI模型的证明准确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估AI在数学竞赛中理解和证明问题的能力，并揭示形式化与非形式化陈述之间的差异对模型性能的影响。

Method: 通过分析miniF2F中的问题，纠正形式与非形式化陈述中的错误，创建改进版miniF2F-v2，并重新评估模型的性能。

Result: 改进后的miniF2F-v2将模型准确率从40%提升至70%，但仍显示自动形式化与定理证明模型之间存在不一致性。

Conclusion: 高质量基准测试能更准确评估形式推理领域的进展，并帮助诊断自动形式化与定理证明模型的失败与成功模式。

Abstract: We perform a thorough analysis of the formal and informal statements in the
miniF2F benchmark from the perspective of an AI system that is tasked to
participate in a math Olympiad consisting of the problems in miniF2F. In such
setting, the model has to read and comprehend the problems in natural language,
formalize them in Lean language, then proceed with proving the problems, and it
will get credit for each problem if the formal proof corresponds to the
original informal statement presented to the model. Our evaluation results
reveal that the best accuracy of such pipeline can be about 36% using the SoTA
models in the literature, considerably lower than the individual SoTA
accuracies, 97% and 69% reported in the autoformalization and theorem proving
literature. Analyzing the failure modes, we trace back a considerable portion
of this drop to discrepancies between the formal and informal statements for
more than half of the problems in miniF2F. We proceed with correcting all the
errors, discrepancies and simplifications in formal and informal statements,
and present the miniF2F-v2 with fully verified formal and informal statements
and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to
the best accuracy of 70%, a significant improvement from the 40% on the
original miniF2F, yet indicating considerable misalignment between the
autoformalization models and theorem provers. Our deep analysis suggests that a
higher quality benchmark can help the community better evaluate progress in the
field of formal reasoning and also better diagnose the failure and success
modes of autoformalization and theorem proving models. Our dataset is available
at https://github.com/roozbeh-yz/miniF2F_v2.

</details>


### [47] [Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks](https://arxiv.org/abs/2511.03137)
*Shipeng Cen,Ying Tan*

Main category: cs.AI

TL;DR: 论文探讨了利用多模态大语言模型（MLLM）辅助设计烟花算法（FWA），以解决复杂高维优化问题，并在旅行商问题（TSP）和电子设计自动化问题（EDA）上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在处理非凸、高维、黑箱等问题时效率低下且准确性不足，而大语言模型（LLM）的发展为优化算法设计提供了新的思路。

Method: 将多模态大语言模型（MLLM）引入烟花算法（FWA），提出了关键部分（CP）的概念，以扩展FWA在复杂高维任务中的应用，并更高效利用优化过程中的信息。

Result: 新框架下生成的FWA在多个问题实例中达到或超越了当前最优（SOTA）结果，尤其是在TSP和EDA问题上表现突出。

Conclusion: 通过结合MLLM优化算法设计，显著提升了优化效率与效果，为复杂优化问题提供了新解决方案。

Abstract: As optimization problems grow increasingly complex and diverse, advancements
in optimization techniques and paradigm innovations hold significant
importance. The challenges posed by optimization problems are primarily
manifested in their non-convexity, high-dimensionality, black-box nature, and
other unfavorable characteristics. Traditional zero-order or first-order
methods, which are often characterized by low efficiency, inaccurate gradient
information, and insufficient utilization of optimization information, are
ill-equipped to address these challenges effectively. In recent years, the
rapid development of large language models (LLM) has led to substantial
improvements in their language understanding and code generation capabilities.
Consequently, the design of optimization algorithms leveraging large language
models has garnered increasing attention from researchers. In this study, we
choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel
approach to assist the design of the FWA by incorporating multi-modal large
language model(MLLM). To put it simply, we propose the concept of Critical
Part(CP), which extends FWA to complex high-dimensional tasks, and further
utilizes the information in the optimization process with the help of the
multi-modal characteristics of large language models. We focus on two specific
tasks: the \textit{traveling salesman problem }(TSP) and \textit{electronic
design automation problem} (EDA). The experimental results show that FWAs
generated under our new framework have achieved or surpassed SOTA results on
many problem instances.

</details>


### [48] [A Proprietary Model-Based Safety Response Framework for AI Agents](https://arxiv.org/abs/2511.03138)
*Qi Li,Jianjun Xu,Pingtao Wei,Jiu Li,Peiqiang Zhao,Jiwei Shi,Xuan Zhang,Yanhui Yang,Xiaodong Hui,Peng Xu,Wenqin Shao*

Main category: cs.AI

TL;DR: 提出了一种新型安全响应框架，通过输入输出两级的系统性保护机制，显著提升大型语言模型的安全性和可信赖性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，其安全问题日益突出，限制了在关键领域的可信部署。

Method: 输入级采用监督微调的安全分类模型，通过四层分类实现精确风险识别；输出级结合RAG和微调解释模型，确保结果基于可信知识库。

Result: 在公共安全评测基准上显著高于基线模型，专用高风险测试集中实现100%安全评分。

Conclusion: 研究为构建高安全性、高信赖的大型语言模型应用提供了有效工程路径。

Abstract: With the widespread application of Large Language Models (LLMs), their
associated security issues have become increasingly prominent, severely
constraining their trustworthy deployment in critical domains. This paper
proposes a novel safety response framework designed to systematically safeguard
LLMs at both the input and output levels. At the input level, the framework
employs a supervised fine-tuning-based safety classification model. Through a
fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused
Attention), it performs precise risk identification and differentiated handling
of user queries, significantly enhancing risk coverage and business scenario
adaptability, and achieving a risk recall rate of 99.3%. At the output level,
the framework integrates Retrieval-Augmented Generation (RAG) with a
specifically fine-tuned interpretation model, ensuring all responses are
grounded in a real-time, trustworthy knowledge base. This approach eliminates
information fabrication and enables result traceability. Experimental results
demonstrate that our proposed safety control model achieves a significantly
higher safety score on public safety evaluation benchmarks compared to the
baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk
test set, the framework's components attained a perfect 100% safety score,
validating their exceptional protective capabilities in complex risk scenarios.
This research provides an effective engineering pathway for building
high-security, high-trust LLM applications.

</details>


### [49] [Uncovering Bugs in Formal Explainers: A Case Study with PyXAI](https://arxiv.org/abs/2511.03169)
*Xuanxiang Huang,Yacine Izza,Alexey Ignatiev,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 论文提出了一种验证形式化可解释人工智能（XAI）的新方法，并通过评估公开工具PyXAI发现其存在错误解释问题。


<details>
  <summary>Details</summary>
Motivation: 形式化XAI虽然在理论上具有严谨性，但实际实现的验证研究较少，论文旨在填补这一空白。

Method: 开发了一种新方法用于验证形式化解释器，并以PyXAI为例进行了实验评估。

Result: 实验表明PyXAI在大多数数据集上存在错误解释，验证了新方法的必要性。

Conclusion: 形式化解释器的验证至关重要，新方法能够有效发现并纠正潜在问题。

Abstract: Formal explainable artificial intelligence (XAI) offers unique theoretical
guarantees of rigor when compared to other non-formal methods of
explainability. However, little attention has been given to the validation of
practical implementations of formal explainers. This paper develops a novel
methodology for validating formal explainers and reports on the assessment of
the publicly available formal explainer PyXAI. The paper documents the
existence of incorrect explanations computed by PyXAI on most of the datasets
analyzed in the experiments, thereby confirming the importance of the proposed
novel methodology for the validation of formal explainers.

</details>


### [50] [Adobe Summit Concierge Evaluation with Human in the Loop](https://arxiv.org/abs/2511.03186)
*Yiru Chen,Sally Fang,Sai Sree Harsha,Dan Luo,Vaishnavi Muppala,Fei Wu,Shun Jiang,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: Summit Concierge是一个为Adobe Summit开发的领域特定AI助手，通过结合提示工程、检索基础和人机验证，解决了数据稀疏和质量保证等现实约束。


<details>
  <summary>Details</summary>
Motivation: 研究旨在展示如何通过生成式AI助手提升企业生产力、简化信息访问并改善用户体验，尤其是在Adobe Summit这样的特定领域。

Method: 采用人机循环工作流程，结合提示工程、检索基础和轻量级人工验证，开发了Summit Concierge系统。

Result: 系统成功处理了广泛的会议相关查询，并在冷启动场景中实现了可扩展且可靠的AI助手。

Conclusion: 敏捷、反馈驱动的开发方法能够有效应对数据稀疏等挑战，并为现实部署提供可靠解决方案。

Abstract: Generative AI assistants offer significant potential to enhance productivity,
streamline information access, and improve user experience in enterprise
contexts. In this work, we present Summit Concierge, a domain-specific AI
assistant developed for Adobe Summit. The assistant handles a wide range of
event-related queries and operates under real-world constraints such as data
sparsity, quality assurance, and rapid deployment. To address these challenges,
we adopt a human-in-the-loop development workflow that combines prompt
engineering, retrieval grounding, and lightweight human validation. We describe
the system architecture, development process, and real-world deployment
outcomes. Our experience shows that agile, feedback-driven development enables
scalable and reliable AI assistants, even in cold-start scenarios.

</details>


### [51] [From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers](https://arxiv.org/abs/2511.03235)
*Yi-Fei Liu,Yi-Long Lu,Di He,Hang Zhang*

Main category: cs.AI

TL;DR: 研究发现大型语言模型（LLM）能够通过少量输入数据准确模拟人类心理特征的关联结构，表现优于基于语义相似性的预测，并接近直接训练的机器学习算法。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否能够通过简单的定量输入（如大五人格量表）模拟人类心理特征的关联结构，以验证其抽象和推理能力。

Method: 通过用816人的大五人格量表输入提示多种LLM，模拟其在其他九个心理量表上的回答，分析模型的两阶段推理过程。

Result: LLM生成的回答与人类数据的相关性极高（R²>0.89），表现显著优于基线模型，并通过压缩信息捕捉了高阶特征交互模式。

Conclusion: LLM能够通过抽象和推理从少量数据中精准预测心理特征，为心理模拟和模型推理能力研究提供了新工具与视角。

Abstract: Psychological constructs within individuals are widely believed to be
interconnected. We investigated whether and how Large Language Models (LLMs)
can model the correlational structure of human psychological traits from
minimal quantitative inputs. We prompted various LLMs with Big Five Personality
Scale responses from 816 human individuals to role-play their responses on nine
other psychological scales. LLMs demonstrated remarkable accuracy in capturing
human psychological structure, with the inter-scale correlation patterns from
LLM-generated responses strongly aligning with those from human data $(R^2 >
0.89)$. This zero-shot performance substantially exceeded predictions based on
semantic similarity and approached the accuracy of machine learning algorithms
trained directly on the dataset. Analysis of reasoning traces revealed that
LLMs use a systematic two-stage process: First, they transform raw Big Five
responses into natural language personality summaries through information
selection and compression, analogous to generating sufficient statistics.
Second, they generate target scale responses based on reasoning from these
summaries. For information selection, LLMs identify the same key personality
factors as trained algorithms, though they fail to differentiate item
importance within factors. The resulting compressed summaries are not merely
redundant representations but capture synergistic information--adding them to
original scores enhances prediction alignment, suggesting they encode emergent,
second-order patterns of trait interplay. Our findings demonstrate that LLMs
can precisely predict individual participants' psychological traits from
minimal data through a process of abstraction and reasoning, offering both a
powerful tool for psychological simulation and valuable insights into their
emergent reasoning capabilities.

</details>


### [52] [Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)](https://arxiv.org/abs/2511.03545)
*Sebastian Ordyniak,Giacomo Paesani,Mateusz Rychlicki,Stefan Szeider*

Main category: cs.AI

TL;DR: 本文通过深入研究各种机器学习模型中解释问题的参数化复杂性，填补了可解释人工智能领域的空白，强调透明性和问责制的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型常被视为黑箱，缺乏透明性。本文旨在研究具有透明内部机制的模型，探讨其解释问题的复杂性，以提升AI系统的透明度和问责性。

Method: 研究覆盖多种ML模型（如决策树、决策集、布尔电路等），分析两类解释问题（溯因和对比）的局部及全局变体，从理论上探究其参数化复杂性。

Result: 为可解释AI奠定了理论基础，揭示了生成模型解释的复杂性，并为后续研究提供了方向。

Conclusion: 研究强调了透明性在AI系统中的重要性，为提升AI的可解释性和问责性提供了关键见解。

Abstract: This paper presents a comprehensive theoretical investigation into the
parameterized complexity of explanation problems in various machine learning
(ML) models. Contrary to the prevalent black-box perception, our study focuses
on models with transparent internal mechanisms. We address two principal types
of explanation problems: abductive and contrastive, both in their local and
global variants. Our analysis encompasses diverse ML models, including Decision
Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof,
each offering unique explanatory challenges. This research fills a significant
gap in explainable AI (XAI) by providing a foundational understanding of the
complexities of generating explanations for these models. This work provides
insights vital for further research in the domain of XAI, contributing to the
broader discourse on the necessity of transparency and accountability in AI
systems.

</details>
